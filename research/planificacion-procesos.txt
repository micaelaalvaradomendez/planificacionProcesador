1. Concepto de planificación de CPU
La planificación de CPU se refiere a cómo el sistema operativo (SO) administra el acceso a la Unidad Central de Procesamiento (CPU) entre los diversos procesos que compiten por ella. Es una función fundamental del sistema operativo. En esencia, se trata de decidir qué proceso en la cola de listos se le asignará la CPU. El planificador de CPU, también conocido como planificador a corto plazo, es el módulo del sistema operativo que lleva a cabo este proceso de selección, tomando un proceso de aquellos que están en memoria y listos para ejecutar, y asignándole la CPU.
En un sistema uniprocesador, solo un proceso puede ejecutarse a la vez; los demás deben esperar hasta que la CPU esté libre y pueda ser reasignada. Sin embargo, en un sistema multiprocesador, la planificación se complica al decidir qué hilo (o proceso) ejecutar y en qué CPU.
Importancia en sistemas multitarea y multiprogramación
La planificación de CPU es la base de los sistemas operativos multiprogramados. Su importancia radica en que:
La multiprogramación aumenta la utilización de la CPU al organizar los trabajos (código y datos) de modo que la CPU siempre tenga algo que ejecutar. Esto evita que la CPU se quede inactiva cuando un proceso necesita esperar, por ejemplo, por una operación de E/S.
En sistemas de tiempo compartido, la planificación de CPU y la multiprogramación se utilizan para proporcionar a cada usuario una pequeña porción del tiempo de la computadora, dando la apariencia de ejecución simultánea y permitiendo la interacción con los programas en ejecución.
Es crucial en entornos donde múltiples procesos o hilos compiten por la CPU, ya que la conmutación de procesos es costosa y un buen planificador puede marcar una gran diferencia en el rendimiento percibido y la satisfacción del usuario.
Objetivos
Los algoritmos de planificación de CPU buscan satisfacer varios objetivos, que pueden ser orientados al usuario o al sistema:
Maximizar eficiencia (uso de CPU): El objetivo es mantener la CPU ocupada la mayor parte del tiempo. Se busca que siempre haya algún proceso en ejecución para aprovechar al máximo este recurso costoso.
Maximizar throughput (procesos completados por unidad de tiempo): Se refiere a la cantidad de trabajos que el sistema completa por hora. Una política de planificación efectiva debería intentar maximizar este número, lo que indica cuánto trabajo se está realizando en el sistema. Este es un objetivo importante en sistemas de procesamiento por lotes.
Minimizar tiempo de respuesta para el usuario: Este es un criterio orientado al usuario y es fundamental en sistemas interactivos y de tiempo compartido. El tiempo de respuesta es el tiempo transcurrido desde que se envía una solicitud hasta que la respuesta comienza a aparecer como salida. El objetivo es proporcionar un servicio rápido e interactivo, por ejemplo, maximizando el número de usuarios que experimentan un tiempo de respuesta promedio de dos segundos o menos. Sin embargo, tiempos de respuesta menores a menudo implican mayores costos.
Otros objetivos incluyen la equidad (tratar procesos comparables de manera similar), la aplicación de políticas (asegurar que se cumplan las políticas establecidas, como prioridades) y el balanceo (mantener ocupadas todas las partes del sistema, como CPU y dispositivos de E/S).
Tipos de planificadores
La actividad de planificación en muchos sistemas se divide en tres funciones separadas, sugiriendo escalas de tiempo relativas en que se realizan:
Planificador a Largo Plazo (LTS - Long-Term Scheduler):


También conocido como planificador de admisión o de trabajos, este planificador determina qué programas se admiten en el sistema para su procesamiento.
Su función principal es controlar el grado de multiprogramación (el número de procesos activos en el sistema).
Una vez admitido, un programa de usuario se convierte en un proceso y se añade a una cola para ser procesado. Puede considerar factores como la prioridad, el tiempo de ejecución esperado y los requisitos de E/S, para mantener una mezcla equilibrada de procesos limitados por CPU y limitados por E/S.
Se invoca cuando un trabajo termina o si la CPU permanece inactiva por encima de un umbral determinado.
Planificador a Medio Plazo (MTS - Medium-Term Scheduler):


Este planificador forma parte de la función de intercambio (swapping).
Decide qué procesos (o partes de ellos) se mueven entre la memoria principal y la memoria secundaria (disco), realizando operaciones de "swap-in" (carga a memoria) y "swap-out" (descarga a disco).
Su objetivo es gestionar la memoria y ajustar el grado de multiprogramación, especialmente en sistemas que no utilizan memoria virtual.
Planificador a Corto Plazo (STS - Short-Term Scheduler) / Planificador de CPU:


Es el planificador más frecuente y activo, encargado de decidir qué proceso de la cola de "listos para ejecutar" utilizará la CPU en el instante actual.
Se invoca cada vez que la CPU queda ociosa o cuando se cumplen ciertas condiciones (como interrupciones de reloj, interrupciones de E/S o llamadas al sistema).
Utiliza un despachador (dispatcher), un módulo que transfiere el control de la CPU al proceso seleccionado por el planificador a corto plazo. Esto implica cambiar el contexto, cambiar al modo de usuario y saltar a la ubicación correcta en el programa de usuario para reiniciarlo. La latencia de despacho es el tiempo que tarda el despachador en detener un proceso y comenzar otro, y debe ser lo más rápida posible.
Existen diversos algoritmos de planificación a corto plazo, incluyendo el turno rotatorio (round-robin), la planificación por prioridades, y el primero en llegar, primero en ser atendido (FCFS), entre otros. Estos algoritmos pueden ser expulsivos (preemptive), donde un proceso puede ser interrumpido para dar paso a otro de mayor prioridad o si excede su tiempo asignado, o no expulsivos (nonpreemptive), donde un proceso se ejecuta hasta que se bloquea o termina voluntariamente.
Estos tres tipos de planificadores trabajan en conjunto para asegurar el funcionamiento eficiente y ordenado de un sistema operativo, gestionando los procesos a lo largo de su ciclo de vida y optimizando el uso de los recursos del computador.




2. Tipos de planificación
La actividad de planificación en un sistema operativo se suele dividir en tres funciones separadas, cuyos nombres sugieren las escalas de tiempo relativas con las que se ejecutan: planificación a largo, medio y corto plazo.
🔹 Planificador de Largo Plazo (LTS - Long-Term Scheduler)
El planificador a largo plazo, también conocido como planificador de admisión o de trabajos, es el responsable de determinar qué programas se admiten en el sistema para su procesamiento. Su función principal es controlar el grado de multiprogramación, que es el número de procesos activos en la memoria principal.
Una vez admitido, un programa de usuario se convierte en un proceso y se añade a una cola, que puede ser la del planificador a corto plazo o, en algunos sistemas, la del planificador a medio plazo si el proceso se crea inicialmente en la zona de intercambio (swap). El planificador a largo plazo opera con una frecuencia relativamente baja (con poca frecuencia). Se puede invocar cada vez que un trabajo termina, o si la fracción de tiempo que el procesador está ocioso excede un determinado valor.
Para tomar la decisión de admisión, el LTS puede considerar varios criterios:
Carga del sistema (grado de multiprogramación): Limitar el número de procesos creados es crucial para no sobrecargar el sistema y proporcionar un servicio satisfactorio al conjunto actual de procesos.
Equilibrio entre procesos CPU-bound e I/O-bound: Puede intentar mantener una mezcla equilibrada de procesos que pasan más tiempo usando la CPU (CPU-bound) y procesos que pasan más tiempo esperando operaciones de E/S (I/O-bound). Una buena mezcla ayuda a mantener la CPU y los dispositivos de E/S ocupados, mejorando la eficiencia general. Un algoritmo de planificación que favorece a los programas limitados por E/S les permite emitir sus peticiones de disco rápidamente, manteniendo el disco ocupado y mejorando la utilización de la CPU cuando los procesos están limitados por E/S.
Priorización externa, tiempo estimado de ejecución, requisitos de E/S: Las decisiones de admisión pueden basarse en la prioridad del trabajo, el tiempo de ejecución esperado y los recursos de E/S que solicitará, con el objetivo de equilibrar el uso de los recursos del sistema.
🔹 Planificador de Mediano Plazo (MTS - Medium-Term Scheduler)
La planificación a medio plazo es una parte integral de la función de intercambio (swapping). Se enfoca en decisiones de intercambio (swapping), determinando qué procesos (o partes de ellos) se mueven entre la memoria principal y el almacenamiento secundario (disco).
Sus objetivos principales incluyen:
Liberar memoria: Cuando la memoria principal está ocupada y hay una gran cantidad de procesos en estado bloqueado, el sistema operativo puede "suspender" un proceso transfiriéndolo a disco para liberar espacio. Esto es especialmente relevante en sistemas que no utilizan memoria virtual.
Mejorar la performance general: Al ajustar el grado de multiprogramación y liberar memoria, busca optimizar el rendimiento del sistema. El "swapping" de procesos ayuda a asegurar que haya suficientes procesos listos en memoria principal para mantener ocupado al procesador, reduciendo el tiempo de inactividad. Sin embargo, debe ser inteligente para evitar el "trasiego" (thrashing), una condición donde el sistema gasta la mayor parte del tiempo transfiriendo partes de procesos entre memoria y disco en lugar de ejecutar instrucciones útiles.
El planificador a medio plazo se ejecuta más frecuentemente que el planificador a largo plazo para tomar estas decisiones de intercambio. La decisión de qué proceso intercambiar puede basarse en la necesidad de gestionar el grado de multiprogramación y los requisitos de memoria.
🔹 Planificador de Corto Plazo (STS - Short-Term Scheduler)
El planificador a corto plazo, también conocido como activador o despachador (dispatcher), es el planificador más frecuente y activo. Toma las decisiones de grano fino sobre qué proceso listo será ejecutado por el procesador a continuación. Cuando el STS selecciona un proceso, el despachador es el módulo que transfiere el control de la CPU a ese proceso. Esta acción implica guardar el estado del proceso actual y cargar el estado del nuevo proceso, incluyendo sus registros y, en algunos sistemas, su mapa de memoria.
El planificador a corto plazo se activa ante varios eventos que pueden conllevar el bloqueo del proceso actual o que pueden proporcionar la oportunidad de expulsar al proceso actualmente en ejecución:
Interrupciones (I/O, reloj, software):
Interrupciones de reloj: Ocurren cuando el proceso en ejecución ha excedido su rodaja de tiempo (time slice o quantum) máxima asignada. El proceso actual es expulsado y se selecciona otro.
Interrupciones de E/S: Se producen cuando una operación de entrada/salida ha finalizado, lo que podría hacer que uno o más procesos que estaban esperando por ella pasen al estado "listo".
Llamadas al sistema (software): Un proceso puede realizar una llamada al sistema que lo bloquea (por ejemplo, esperando E/S o un recurso), o que le permite abandonar voluntariamente el procesador (por ejemplo, sched_yield() en Linux). También puede ocurrir por señales.
Eventos que cambian el estado de procesos: Como la terminación de un proceso o la creación de uno nuevo. En sistemas con prioridades, si un proceso de mayor prioridad pasa al estado "listo" mientras uno de menor prioridad está ejecutando, el de mayor prioridad puede expulsar al actual. Este comportamiento se conoce como planificación expulsiva (preemptive scheduling). Si no hay un reloj disponible, solo es posible la planificación no expulsiva (nonpreemptive scheduling), donde un proceso se ejecuta hasta que se bloquea o termina voluntariamente.
La conmutación de procesos es una operación costosa debido a la sobrecarga de guardar y restaurar el contexto, así como la posible invalidación de la caché de la CPU. Por lo tanto, un buen algoritmo de planificación a corto plazo busca minimizar esta sobrecarga mientras optimiza los objetivos del sistema, como el tiempo de respuesta y la utilización de la CPU.


3. Criterios de planificación
Profundicemos en los Criterios de Planificación de la CPU, que son fundamentales para evaluar y diseñar algoritmos de planificación. Estos criterios nos permiten juzgar qué tan bien un planificador cumple con sus objetivos, ya sea desde la perspectiva del usuario o del sistema. A menudo, estos criterios son interdependientes y es imposible optimizar todos ellos de forma simultánea, lo que requiere compromisos en el diseño de políticas. La importancia relativa de cada criterio dependerá de la naturaleza y el uso previsto del sistema.
Los criterios comúnmente utilizados para evaluar las políticas de planificación se pueden clasificar en dos dimensiones: orientados al usuario y orientados al sistema.
🔸 Orientados al usuario:
Estos criterios se relacionan con el comportamiento del sistema tal como lo percibe el usuario individual o el proceso.
Tiempo de Retorno (Turnaround Time):


Este es el intervalo de tiempo transcurrido desde la entrega de un proceso hasta su finalización. Es una medida crucial para el usuario que representa el tiempo total que un proceso tarda en ejecutarse.
El tiempo de retorno incluye la suma de los periodos en que el proceso espera para entrar en memoria, espera en la cola de listos, se ejecuta en la CPU y realiza operaciones de E/S.
Es una medida apropiada para trabajos por lotes. Un planificador eficiente busca minimizar este tiempo para que los usuarios reciban sus resultados lo antes posible.
El algoritmo de planificación de CPU no afecta el tiempo de ejecución o de E/S, solo el tiempo que el proceso pasa esperando en la cola de listos.
Tiempo de Respuesta (Response Time):


Para un proceso interactivo, este es el tiempo que transcurre desde el envío de una petición hasta que la respuesta empieza a recibirse o aparecer como salida.
Es una medida orientada al usuario que es visible y de su interés. Es una mejor medida que el tiempo de retorno desde el punto de vista del usuario en sistemas interactivos, ya que un proceso puede empezar a producir alguna salida mientras sigue procesando la petición.
El objetivo del mecanismo de planificación es lograr bajos tiempos de respuesta y maximizar el número de usuarios interactivos que experimentan tiempos de respuesta aceptables (por ejemplo, 2 segundos o menos).
Existe un conflicto inherente entre la utilización de la CPU y el tiempo de respuesta [3a, 7, 209]. Un buen tiempo de respuesta puede requerir que el planificador cambie de procesos frecuentemente, lo que aumenta la sobrecarga del sistema y reduce el throughput.
Plazo de Ejecución (Deadlines):


Este criterio es especialmente relevante en sistemas de tiempo real, donde la corrección del sistema no solo depende del resultado lógico de la computación, sino también del momento en que se producen los resultados.
Cuando se puede especificar una fecha tope o plazo para un proceso, el planificador debe subordinar otros objetivos a maximizar el porcentaje de plazos cumplidos.
Las tareas pueden clasificarse como de tiempo real duro (hard real-time), que deben cumplir sus plazos estrictamente, ya que un fallo podría causar un daño inaceptable o un error fatal; o de tiempo real suave (soft real-time), donde es deseable cumplir el plazo, pero es tolerable fallar ocasionalmente.
Los plazos pueden ser de comienzo (momento en que la tarea debe empezar) o de conclusión (momento para el cual la tarea debe estar completada).
🔸 Orientados al sistema:
Estos criterios se centran en el uso efectivo y eficiente de los recursos del sistema en su totalidad, en lugar de la percepción individual del usuario.
Utilización de la CPU (CPU Utilization):


Es el porcentaje de tiempo que el procesador está ocupado.
En un sistema compartido costoso, maximizar la utilización de la CPU es un criterio significativo. La multiprogramación ayuda a mantener la CPU ocupada.
Sin embargo, en sistemas de un solo usuario o en sistemas de tiempo real, este criterio puede ser menos importante que otros (como el tiempo de respuesta o el cumplimiento de plazos).
Como se mencionó, puede entrar en conflicto con el tiempo de respuesta [3a, 7, 209].
Throughput (Rendimiento):


Es la tasa con la que los procesos se finalizan o el número de procesos completados por unidad de tiempo (por ejemplo, trabajos por hora).
El objetivo es maximizar la cantidad de trabajo que el sistema realiza.
Esta medida es valiosa desde la perspectiva del sistema y concierne más al administrador del sistema que a los usuarios individuales.
Justicia (Fairness):


Implica que, en ausencia de una política explícita de priorización, todos los procesos que compiten por un recurso deben ser tratados de la misma manera.
Es fundamental asegurar que ningún proceso sufra de inanición (starvation), una situación en la que un proceso listo para ejecutarse es postergado indefinidamente por el planificador.
Se puede aplicar la imposición de prioridades como una política, donde el planificador debería favorecer a los procesos con prioridades más altas.
Balance de Recursos (Resource Balancing):


La política del planificador debería intentar mantener ocupados todos los recursos del sistema, no solo la CPU.
Esto implica favorecer procesos que utilicen poco los recursos que en un momento determinado están sobreutilizados, y buscar una mezcla equilibrada de procesos limitados por CPU (CPU-bound) y procesos limitados por E/S (I/O-bound) para evitar que algunos recursos queden inactivos.
Predictibilidad:


Se desea que un trabajo dado se ejecute en aproximadamente el mismo tiempo y con el mismo coste, a pesar de la carga del sistema.
Grandes variaciones en el tiempo de respuesta o el tiempo de estancia son perjudiciales desde el punto de vista del usuario y pueden indicar inestabilidad o la necesidad de ajustar el sistema.
Es un criterio importante en algunos sistemas de tiempo real, especialmente aquellos que involucran multimedia, para evitar la degradación de la calidad.
En resumen, la planificación de la CPU es un arte de compromiso. Los diseñadores de sistemas operativos deben sopesar estos criterios conflictivos y decidir qué objetivos son más importantes para el uso particular del sistema. Por ejemplo, un sistema de procesamiento por lotes priorizará el throughput y la utilización de la CPU, mientras que un sistema interactivo se centrará en un bajo tiempo de respuesta. Los sistemas de tiempo real, por su parte, darán máxima importancia al cumplimiento de los plazos.


4. Políticas de planificación
Las políticas de planificación son las reglas y criterios que el sistema operativo utiliza para decidir qué proceso (o hilo) se ejecutará a continuación en la CPU. Estas políticas se definen a través de dos elementos clave: el modo de decisión y la función de selección.
🔹 Modos de Decisión:
El modo de decisión especifica los momentos exactos en que la función de selección se activa para elegir un nuevo proceso. Existen dos categorías principales:
Non-preemptive (Sin expulsión / No apropiativo):


En este modo, una vez que un proceso entra en el estado de "Ejecución", continúa ejecutándose ininterrumpidamente hasta que ocurre uno de dos eventos:
El proceso termina su ejecución.
El proceso se bloquea voluntariamente (por ejemplo, para esperar una operación de E/S o para solicitar algún servicio del sistema operativo).
Esto significa que, incluso si un proceso se ejecuta durante horas, no será suspendido de manera forzosa por el sistema operativo. Las decisiones de planificación no se toman durante las interrupciones de reloj periódicas en este modo. Si no hay un reloj de hardware disponible, la planificación no apropiativa es la única opción.
Este modo puede penalizar a los procesos cortos y a los procesos limitados por E/S. Por ejemplo, en un sistema uniprocesador, si un proceso "CPU-bound" (que usa mucho la CPU) se ejecuta durante mucho tiempo, los procesos "I/O-bound" (que requieren mucha E/S) pueden quedar esperando, lo que lleva a un uso ineficiente del procesador y los dispositivos de E/S.
Un ejemplo de algoritmo no apropiativo es "Primero en llegar, primero en servirse" (FCFS) y "Primero el proceso más corto" (SJF).
Preemptive (Con expulsión / Apropiativo):


En este modo, el sistema operativo tiene la capacidad de interrumpir y desalojar a un proceso que se está ejecutando en un momento dado, pasándolo al estado de "Listo".
La decisión de expulsar un proceso puede tomarse en varias situaciones:
Cuando llega un nuevo proceso al sistema, y este tiene una prioridad más alta o el sistema opera con rodajas de tiempo.
Cuando se produce una interrupción de E/S que hace que un proceso que estaba bloqueado pase al estado "Listo" con una prioridad más alta.
Periódicamente, basándose en interrupciones de reloj (también conocido como "time slicing" o "rodaja de tiempo"). Si un proceso ha excedido la cantidad máxima de tiempo asignada para su ejecución, es interrumpido y otro proceso puede ser seleccionado.
Cuando un proceso voluntariamente abandona el procesador sin bloquearse, por ejemplo, mediante una llamada al sistema sched_yield() en Linux, para permitir que otros procesos se ejecuten.
La expulsión es fundamental para los sistemas de tiempo compartido y de tiempo real, ya que permite mantener la interactividad y cumplir con plazos críticos. Permite que el sistema sea más equitativo y evita la inanición de procesos de baja prioridad, especialmente si se combina con mecanismos de envejecimiento o prioridad dinámica.
Algoritmos como "Turno rotatorio" (Round Robin) y "Menor tiempo restante" (Shortest Remaining Time, SRT) son ejemplos de planificación expulsiva.
🔹 Función de Selección:
La función de selección es la parte de la política de planificación que determina cuál proceso, de entre los que se encuentran en el estado "Listo", será el elegido para ejecutar en la CPU. El planificador a corto plazo (dispatcher) es el encargado de elegir uno de los procesos en la cola de listos.
Esta función puede basarse en diversos criterios:
Prioridades: Se elige el proceso con la prioridad más alta. En muchos sistemas, a cada proceso se le asigna una prioridad, y el planificador siempre elegirá un proceso de mayor prioridad sobre uno de menor prioridad.
Requisitos sobre los recursos: Decisiones basadas en los recursos que el proceso necesita o libera.
Características de ejecución del proceso: Esto implica medidas cuantitativas como el tiempo total de servicio requerido por el proceso (estimado por el usuario o por el sistema), el tiempo que lleva esperando o el tiempo que ya ha ejecutado.
Ejemplos de cómo la función de selección trabaja en conjunto con diferentes políticas de planificación:
En la política "Primero en llegar, primero en servirse" (FCFS), la función de selección elige el proceso que ha estado esperando servicio por más tiempo.
En el "Turno rotatorio" (Round Robin), una vez que un proceso es expulsado, el siguiente proceso en la cola de listos (siguiendo un orden FCFS) es seleccionado.
En "Primero el proceso más corto" (SPN), se selecciona el proceso con el menor tiempo de procesamiento esperado.
En "Menor tiempo restante" (SRT), una versión expulsiva de SPN, el planificador escoge el proceso con el menor tiempo de proceso restante esperado.
En "Primero el de mayor tasa de respuesta" (HRRN), la decisión se basa en una estimación del tiempo de estancia normalizado, buscando un compromiso entre favorecer procesos cortos y evitar la inanición de procesos largos.
Las políticas de "Retroalimentación" (Feedback) establecen un conjunto de colas de planificación (multinivel) y sitúan los procesos en las colas basándose en su historial de ejecución, favoreciendo a los procesos cortos y penalizando a los largos con prioridades decrecientes.
La elección específica de la función de selección y el modo de decisión dependerá de los criterios de planificación (orientados al usuario y al sistema) que el diseñador del sistema operativo desee optimizar, ya que estos criterios a menudo entran en conflicto y requieren compromisos.


5. Priorización
La priorización es un concepto central en la planificación de la CPU, ya que permite al sistema operativo tomar decisiones más sofisticadas sobre qué proceso ejecutar a continuación, más allá de un simple orden de llegada. A continuación, desarrollaremos este concepto.
La priorización es una técnica fundamental en la planificación de la CPU que permite al sistema operativo favorecer la ejecución de ciertos procesos sobre otros. Esto se logra asignando un nivel de importancia a cada proceso, lo que guía al planificador en su selección.
Uso de múltiples colas según prioridad
En muchos sistemas operativos, para implementar la planificación por prioridades, se utiliza un conjunto de colas de procesos listos, organizadas en orden descendente de prioridad. Por ejemplo, podría haber colas CL0, CL1, ..., CLn, donde los procesos en CL0 tienen la prioridad más alta, CL1 la siguiente más alta, y así sucesivamente.
Cuando el planificador necesita seleccionar un proceso para ejecutar, comienza examinando la cola de mayor prioridad (CL0). Si esta cola contiene uno o más procesos, se selecciona uno de ellos (a menudo utilizando una política de planificación como FCFS dentro de esa cola). Si CL0 está vacía, el planificador procede a examinar CL1, y así sucesivamente, hasta encontrar una cola no vacía.
Este esquema de múltiples colas también puede ser utilizado en planificación retroalimentada (feedback scheduling), donde los procesos migran entre colas de diferentes prioridades basándose en su comportamiento. Por ejemplo, un proceso puede comenzar en una cola de alta prioridad (CL0) y, después de ser expulsado varias veces o de consumir su rodaja de tiempo, puede ser movido a una cola de menor prioridad (CL1, CL2, etc.). Esto favorece a los procesos cortos y penaliza a los largos.
Algunos sistemas operativos, como UNIX SVR4, también utilizan esta estructura, asociando una cola de activación con cada nivel de prioridad y ejecutando los procesos dentro de cada cola de manera circular (round-robin). De forma similar, Linux agrupa hilos en clases de prioridad (tiempo real FIFO, tiempo real Round-Robin, y tiempo compartido), con prioridades más altas para las clases de tiempo real.
Prioridades estáticas: fijas
Las prioridades estáticas son aquellas que se asignan a un proceso en el momento de su creación y permanecen fijas o inalterables durante toda la vida útil del proceso. En un esquema de planificación con prioridades estáticas, el planificador siempre elegirá el proceso con la prioridad más alta de entre todos los procesos listos para ejecutar.
Ejemplos de sistemas o clases de procesos que utilizan prioridades estáticas incluyen:
Las tareas de tiempo real en Linux tienen prioridades estáticas, lo que significa que no hay cálculos de prioridad dinámica para ellas.
En Windows, los hilos en la clase de prioridad de tiempo real tienen una prioridad fija que nunca cambia.
El algoritmo de tasa monótona (Rate Monotonic Scheduling, RMS), un algoritmo de planificación de tiempo real, asigna prioridades fijas a las tareas basándose en su frecuencia de ocurrencia (periodo más breve = mayor prioridad).
Este enfoque es más sencillo de implementar y predecible, lo cual es vital en sistemas de tiempo real donde el cumplimiento de plazos es crítico.
Prioridades dinámicas: se ajustan durante la ejecución
Las prioridades dinámicas son aquellas que pueden cambiar (ajustarse) a lo largo del tiempo de ejecución de un proceso en función de diversos factores, como su comportamiento, el tiempo que ha esperado o el tiempo que ha utilizado la CPU.
Estos ajustes dinámicos buscan mejorar la equidad, la capacidad de respuesta o la eficiencia general del sistema. Algunas estrategias para modificar dinámicamente las prioridades incluyen:
Envejecimiento (Aging): Aumentar la prioridad de un proceso que ha esperado en la cola durante un período prolongado. Esto ayuda a evitar la inanición de procesos de baja prioridad.
Historial de ejecución:
Favorecer procesos limitados por E/S: Si un proceso gasta la mayor parte de su tiempo esperando por E/S (I/O-bound), se le puede dar una prioridad más alta para que pueda realizar sus operaciones de E/S rápidamente y mantener los dispositivos ocupados, mejorando la utilización del sistema. Por ejemplo, Linux favorece a las tareas limitadas por E/S sobre las limitadas por CPU dándoles mayor prioridad.
Penalizar procesos CPU-bound: Si un proceso consume mucho tiempo de CPU (CPU-bound) y agota su rodaja de tiempo, su prioridad puede ser reducida.
Windows baja la prioridad de un hilo si ha usado completamente su cuanto de tiempo y la sube si se interrumpe para esperar por un evento de E/S. También impulsa la prioridad de un hilo cuando se completa una operación de E/S o si está esperando un semáforo/mutex, especialmente para procesos en primer plano.
Planificación retroalimentada: Los procesos se mueven entre diferentes colas de prioridad (o clases de prioridad) en función de su tiempo de ejecución o de cuántas veces han sido expulsados. Por ejemplo, en UNIX SVR4, la prioridad de los procesos de tiempo compartido es variable y se ajusta según el uso del cuanto de tiempo y los eventos de bloqueo.
Peligro de inanición (starvation) para procesos con baja prioridad
La inanición (starvation) es una situación indeseable en la que un proceso que está listo para ejecutarse es denegado de forma continua el acceso al procesador porque otros procesos tienen siempre preferencia. Es decir, aunque el proceso es capaz de avanzar, nunca es seleccionado por el planificador.
Este problema es una amenaza inherente a los esquemas de planificación basados en prioridades fijas, especialmente si hay un flujo constante de procesos de mayor prioridad. Un proceso de baja prioridad podría quedar esperando indefinidamente si siempre hay procesos de mayor prioridad listos para ejecutar.
Además de la planificación de CPU, la inanición también puede ocurrir en otros contextos:
En algoritmos de exclusión mutua, si la selección de un proceso en espera es arbitraria, algunos procesos podrían ser denegados indefinidamente el acceso a una sección crítica.
En la planificación de disco, si se favorecen las peticiones más recientes (LIFO) o las más cercanas, las peticiones en pistas más lejanas pueden sufrir inanición.
Para mitigar el riesgo de inanición, se implementan varias estrategias, muchas de las cuales involucran la priorización dinámica:
Envejecimiento: Aumentar gradualmente la prioridad de un proceso a medida que pasa más tiempo esperando en la cola. Esto asegura que, eventualmente, incluso un proceso de baja prioridad alcanzará una prioridad lo suficientemente alta como para ser ejecutado.
Planificación retroalimentada con ajuste de prioridades: Promover procesos a colas de mayor prioridad después de un cierto tiempo de espera en su cola actual.
Algunos planificadores de E/S, como el planificador de E/S basado en plazos de Linux, están diseñados para superar el problema de la inanición mediante el uso de múltiples colas y tiempos de expiración para las peticiones.
En resumen, la priorización es una herramienta poderosa para optimizar el rendimiento del sistema y la respuesta al usuario, pero debe ser gestionada cuidadosamente para evitar que los procesos de baja prioridad queden en un estado de inanición prolongada.




6. Algoritmos de planificación
Estos algoritmos son el corazón de la gestión de procesos en los sistemas operativos y cada uno tiene sus propias características, ventajas y desventajas.
▶ FCFS (First-Come, First-Served - Primero en Llegar, Primero en Ser Atendido)
El algoritmo FCFS es, con diferencia, el algoritmo de planificación de CPU más sencillo. Funciona según el principio de que el proceso que solicita la CPU primero es el primero en ser asignado a la CPU.
No apropiativo: Una característica fundamental de FCFS es que es no apropiativo (nonpreemptive). Esto significa que, una vez que la CPU ha sido asignada a un proceso, ese proceso mantiene la CPU hasta que la libera voluntariamente, ya sea porque termina su ejecución o porque solicita una operación de E/S. No hay interrupciones forzadas por el planificador una vez que un proceso está en ejecución.
Selección por orden de llegada: La implementación de la política FCFS es muy sencilla y se gestiona fácilmente con una cola FIFO (First-In, First-Out). Cuando un proceso entra en la cola de listos, su Bloque de Control de Proceso (PCB) se enlaza a la cola. Cuando la CPU queda libre, se asigna al proceso que está al principio de la cola.
Problemas: A pesar de su simplicidad, FCFS presenta varias deficiencias:
Favorece procesos CPU-bound: FCFS tiende a favorecer a los procesos limitados por el procesador (CPU-bound) sobre los procesos limitados por E/S (I/O-bound). Esto puede llevar al llamado "efecto convoy" (convoy effect). Si un proceso CPU-bound obtiene y retiene la CPU, todos los demás procesos (especialmente los I/O-bound que realizan ráfagas cortas de CPU) terminan sus operaciones de E/S y se mueven a la cola de listos, esperando la CPU. Mientras esperan, los dispositivos de E/S quedan inactivos. Cuando el proceso CPU-bound finalmente termina su ráfaga de CPU y se mueve a un dispositivo de E/S, todos los procesos I/O-bound, que tienen ráfagas de CPU cortas, se ejecutan rápidamente y regresan a las colas de E/S. En este punto, la CPU puede quedar inactiva.
Baja eficiencia de I/O: El "efecto convoy" resulta en una menor utilización de la CPU y de los dispositivos de E/S. FCFS puede conducir a un uso ineficiente del procesador y de los dispositivos de E/S. Esto es particularmente problemático en sistemas de tiempo compartido, donde es crucial que cada usuario reciba una porción de la CPU a intervalos regulares. Permitir que un proceso retenga la CPU por un período extendido sería desastroso.
▶ Round Robin (RR - Turno Rotatorio)
El algoritmo Round Robin (RR) está diseñado especialmente para sistemas de tiempo compartido. Es una evolución de FCFS que incorpora la apropiación (preemption).
Apropiativo: RR es un algoritmo de planificación apropiativo (preemptive). Utiliza la preemption basada en un reloj para alternar entre procesos. Una interrupción de reloj se genera a intervalos periódicos. Cuando ocurre la interrupción, el proceso que se está ejecutando se coloca en la cola de listos, y el siguiente trabajo listo es seleccionado.
Quantum de tiempo para cada proceso: Una pequeña unidad de tiempo, llamada quantum de tiempo (time quantum o time slice), se define. Esta duración suele ser de entre 10 y 100 milisegundos. La cola de listos se trata como una cola circular. El planificador de CPU recorre la cola, asignando la CPU a cada proceso por un intervalo de tiempo de hasta un quantum. Si un proceso utiliza todo su quantum, la CPU es apropiada para dársela a otro proceso. Si el proceso se bloquea o termina antes de que el quantum haya transcurrido, la CPU se conmuta cuando el proceso se bloquea.
Sensibilidad al tamaño del quantum: El rendimiento del algoritmo RR depende en gran medida del tamaño del quantum de tiempo.
Si el quantum es muy grande, la política RR se vuelve idéntica a la política FCFS. Una regla general es que el 80% de las ráfagas de CPU deberían ser más cortas que el quantum.
Si el quantum es demasiado pequeño (por ejemplo, 1 milisegundo), RR se aproxima al "compartir procesador" (processor sharing), creando la ilusión de que cada uno de los n procesos tiene su propio procesador funcionando a 1/n de la velocidad del procesador real. Sin embargo, un quantum muy pequeño resulta en demasiadas conmutaciones de procesos, lo que aumenta la sobrecarga (overhead) de procesamiento debido al manejo de la interrupción de reloj y a las funciones de planificación y despacho. Esto reduce la eficiencia de la CPU.
Un quantum con un valor entre 20 y 50 milisegundos suele ser una solución razonable para equilibrar la eficiencia y la capacidad de respuesta.
Solución a los problemas: RR Virtual (prioriza I/O-bound): RR tiene la desventaja de tratar de forma desigual a los procesos limitados por CPU y a los limitados por E/S. Los procesos limitados por CPU tienden a usar el quantum completo y regresan inmediatamente a la cola, mientras que los limitados por E/S usan ráfagas cortas y luego se bloquean. Esto puede resultar en un mal rendimiento para los procesos I/O-bound y un uso ineficiente de los dispositivos de E/S. Para abordar esta injusticia, se ha sugerido una mejora llamada Virtual Round Robin (VRR). En VRR, cuando un proceso se bloquea por una operación de E/S, se mueve a una cola auxiliar FCFS. Cuando se toma una decisión de despacho, los procesos en la cola auxiliar reciben preferencia sobre los de la cola principal de listos. Cuando un proceso de la cola auxiliar es despachado, se le permite ejecutar por un tiempo igual al quantum básico menos el tiempo total que ya ha corrido desde la última vez que fue seleccionado de la cola principal. Esto mejora la equidad y el rendimiento para los procesos I/O-bound.
▶ SPN / SJF (Shortest Process Next / Shortest Job First - Primero el Proceso Más Corto)
El algoritmo SJF (Shortest-Job-First), o también conocido como SPN (Shortest Process Next), es otro enfoque para la planificación de la CPU. Se asocia a cada proceso la duración de su próxima ráfaga de CPU.
No apropiativo: SPN/SJF es una política de planificación no apropiativa. Una vez que un proceso es seleccionado, se ejecuta hasta que termina o se bloquea.
Selecciona el proceso con menor CPU burst estimado: Cuando la CPU está disponible, se asigna al proceso que tiene la ráfaga de CPU más pequeña (el menor tiempo de procesamiento esperado). Si las ráfagas de CPU de dos procesos son iguales, se utiliza FCFS para desempatar. Este algoritmo es óptimo para proporcionar el tiempo de espera promedio más corto, especialmente cuando todos los trabajos están disponibles al mismo tiempo.
Necesita predicción del tiempo de ráfaga (media o promedio exponencial): El principal problema de SPN/SJF es la necesidad de conocer, o al menos estimar, el tiempo de procesamiento requerido por cada proceso. Para trabajos por lotes, el sistema podría exigir que el programador proporcione esta estimación. Para procesos interactivos, el sistema operativo puede mantener un promedio del tiempo de ejecución de cada "ráfaga" de CPU. Una forma común de predecir la longitud de la siguiente ráfaga de CPU es mediante la media exponencial (exponential average). Esta técnica asigna un peso decreciente a las predicciones más antiguas, reaccionando más rápidamente a los cambios en el comportamiento del proceso.
Posible inanición de procesos largos: Un riesgo importante con SPN/SJF es la inanición (starvation) para los procesos más largos, especialmente si hay un flujo constante de procesos más cortos. Un proceso largo podría no tener la oportunidad de ejecutarse si siempre hay procesos más cortos listos que llegan constantemente. Aunque reduce la predilección por trabajos largos, no es ideal para entornos de tiempo compartido debido a la falta de apropiación.
▶ SRT (Shortest Remaining Time - Menor Tiempo Restante)
El algoritmo SRT es una versión apropiativa de SPN/SJF.
Apropiativo. Variante de SJF: En SRT, el planificador siempre elige el proceso que tiene el menor tiempo de procesamiento restante esperado.
Reemplaza al proceso si otro con menor ráfaga restante aparece: Cuando un nuevo proceso se une a la cola de listos, su tiempo restante de ejecución se compara con el tiempo restante del proceso que se está ejecutando actualmente. Si el nuevo proceso necesita menos tiempo para terminar, el proceso actual es expulsado (preempted) y el nuevo proceso se inicia. Esto permite que los trabajos cortos nuevos obtengan un buen servicio. Al igual que SJF, SRT también necesita una estimación del tiempo de procesamiento y tiene el riesgo de inanición para procesos largos.
Menor overhead que RR: SRT no tiene el sesgo a favor de los procesos largos que se encuentra en FCFS. A diferencia de Round Robin, no genera interrupciones adicionales periódicas para la apropiación, lo que reduce la sobrecarga. Sin embargo, requiere almacenar los tiempos de servicio transcurridos, lo que sí genera cierta sobrecarga. Se espera que mejore los tiempos de estancia de SPN, ya que un trabajo corto obtiene preferencia sobre un trabajo más largo en ejecución.
▶ Retroalimentación Multinivel (Multilevel Feedback Scheduling - MFS)
Los algoritmos de planificación de colas multinivel se utilizan en situaciones donde los procesos pueden clasificarse en diferentes grupos, como procesos interactivos (primer plano) y procesos por lotes (segundo plano). El algoritmo de colas de retroalimentación multinivel es una versión más compleja que permite a los procesos moverse entre colas.
Varias colas de prioridades: La idea es separar los procesos según las características de sus ráfagas de CPU. Se utiliza un conjunto de múltiples colas de prioridades (por ejemplo, CL0, CL1, CLn). Cada cola puede tener su propio algoritmo de planificación (por ejemplo, Round Robin para colas de alta prioridad y FCFS para la cola de menor prioridad). El planificador primero ejecuta todos los procesos en la cola de mayor prioridad. Solo cuando esa cola está vacía, pasa a la siguiente.
Proceso baja de nivel si usa todo el quantum: Si un proceso utiliza demasiado tiempo de CPU (es decir, agota su quantum), será movido a una cola de menor prioridad. Por ejemplo, un proceso puede comenzar en la cola 0, recibir un quantum y, si no termina, pasar a la cola 1. Si agota otro quantum, pasa a la cola 2, y así sucesivamente. Esto se conoce como democión.
Favorece procesos I/O-bound: Este esquema tiende a dejar los procesos I/O-bound e interactivos en las colas de mayor prioridad. Un proceso I/O-bound, al tener ráfagas de CPU cortas, es probable que se bloquee por E/S antes de agotar su quantum, lo que le permite permanecer en una cola de alta prioridad. Los procesos CPU-bound, que agotan sus quanta, bajan automáticamente de prioridad.
Algoritmo adaptable y justo si se implementa bien: Los algoritmos de retroalimentación multinivel son flexibles y pueden ser justos si se implementan correctamente, ya que se ajustan al comportamiento de los procesos durante la ejecución.
Variantes: Hay varias variaciones de este esquema:
Tamaño de quantum variable (ej: Q = 2^(i-1)): Para evitar que los procesos largos tarden demasiado en completarse, se pueden variar los tiempos de apropiación según la cola. Por ejemplo, a un proceso en la cola CLi se le permite ejecutar por 2^i unidades de tiempo antes de ser expulsado. Esto se ilustra en el ejemplo de los fuentes con q = 2^i.
Promoción después de cierto tiempo para evitar inanición: El problema del esquema simple es que los procesos más largos pueden sufrir de inanición (starvation) si llegan continuamente nuevos trabajos. Para evitar esto, se puede aumentar la prioridad de un proceso (promoverlo) si ha esperado demasiado tiempo en una cola de baja prioridad. Esta forma de envejecimiento (aging) previene la inanición.
Estos algoritmos de planificación representan las bases sobre las cuales se construyen planificadores más complejos en sistemas operativos modernos, a menudo combinando elementos de varios de ellos para lograr un equilibrio entre los distintos criterios de rendimiento.




7. Estimación de ráfagas de CPU
Para algoritmos de planificación como "Primero el Proceso Más Corto" (SPN/SJF) y "Menor Tiempo Restante" (SRT), es esencial conocer o estimar la duración de la próxima ráfaga de CPU de un proceso. Dado que es imposible saber con exactitud el futuro tiempo de procesamiento de un proceso interactivo, el sistema operativo recurre a técnicas de predicción basadas en el comportamiento pasado. Dos métodos comunes para realizar esta estimación son el promedio simple y el promedio ponderado (también conocido como promedio exponencial o envejecimiento).
Promedio simple: S[n+1] = (1/n) ∑ T[i]
El promedio simple es una manera básica de estimar la duración de la próxima ráfaga de CPU.
Fórmula: La fórmula para el promedio simple es: $S_{n+1} = \frac{1}{n} \sum_{i=1}^{n} T_i$ Donde:
$T_i$ es el tiempo de ejecución del procesador para la $i$-ésima instancia de este proceso (o el tiempo de ráfaga del procesador para trabajos interactivos).
$S_{n+1}$ es el valor predicho para la siguiente instancia.
$n$ es el número de instancias anteriores medidas.
Funcionamiento: Esta fórmula suma los tiempos de todas las ráfagas de CPU anteriores ($T_i$) y los divide por el número total de ráfagas (n) para obtener un promedio.
Característica clave: En este método, cada término de la suma tiene el mismo peso; es decir, cada ráfaga de CPU pasada contribuye de igual manera a la predicción de la siguiente ráfaga.
Limitación: El problema con el promedio simple es que trata todas las instancias pasadas con igual importancia. Sin embargo, a menudo se desea que las instancias más recientes tengan un mayor peso, ya que es más probable que reflejen el comportamiento futuro del proceso.
Para evitar recalcular la suma completa cada vez, la Ecuación (9.1) puede reescribirse como: $S_{n+1} = \frac{1}{n} T_n + \frac{n-1}{n} S_n$.
Promedio ponderado (o promedio exponencial): S[n+1] = αT[n] + (1-α)S[n]
El promedio ponderado, o promedio exponencial (exponential average), es una técnica común y más sofisticada para predecir un valor futuro basándose en una serie temporal de valores pasados. Esta técnica asigna un peso decreciente a las observaciones más antiguas, dando más importancia a las ráfagas recientes.
Fórmula: La fórmula para el promedio exponencial es: $S_{n+1} = \alpha T_n + (1 - \alpha) S_n$ Donde:
$T_n$ es la duración de la $n$-ésima (más reciente) ráfaga de CPU.
$S_n$ (o $\tau_n$ en algunas notaciones) es el valor predicho para la $n$-ésima ráfaga de CPU (es decir, la estimación anterior para la ráfaga que acaba de ocurrir).
$S_{n+1}$ (o $\tau_{n+1}$) es el valor predicho para la siguiente ráfaga de CPU.
$\alpha$ es un factor de ponderación constante (constant weighting factor), con un valor que se encuentra entre 0 y 1 (es decir, $0 \le \alpha \le 1$, o $0 < \alpha < 1$ en algunas fuentes).
Da más peso a ráfagas recientes: El parámetro $\alpha$ controla el peso relativo de la historia reciente ($T_n$) y la historia pasada ($S_n$) en la predicción.
Si $\alpha = 0$, entonces $S_{n+1} = S_n$. En este caso, la historia reciente no tiene ningún efecto, y se asume que las condiciones actuales son transitorias.
Si $\alpha = 1$, entonces $S_{n+1} = T_n$. Aquí, solo importa la ráfaga de CPU más reciente, y se asume que la historia pasada es irrelevante.
Un valor más común es $\alpha = 1/2$, lo que significa que la historia reciente y la historia pasada tienen un peso equitativo en la predicción. En este caso, la técnica es muy sencilla de implementar: se suma el nuevo valor a la estimación actual y se divide entre 2 (lo que puede hacerse con un simple desplazamiento de bits a la derecha).
Si $\alpha > 0.5$, se le da más peso a las observaciones más recientes. Por ejemplo, para $\alpha = 0.8$, prácticamente todo el peso se da a las cuatro observaciones más recientes, mientras que para $\alpha = 0.2$, el promedio se extiende sobre las ocho observaciones más recientes.
Funcionamiento a largo plazo: El promedio exponencial considera todos los valores pasados, pero los menos recientes tienen cada vez menos peso. Esto se puede ver al expandir la fórmula: $S_{n+1} = \alpha T_n + (1 - \alpha)\alpha T_{n-1} + \dots + (1 - \alpha)^i \alpha T_{n-i} + \dots + (1 - \alpha)^n S_1$ Dado que $\alpha$ y $(1-\alpha)$ son menores que 1, cada término sucesivo en la ecuación anterior es más pequeño, lo que significa que las observaciones más antiguas tienen un impacto decreciente en la predicción.
Ventajas y desventajas:
La ventaja de usar un valor de $\alpha$ cercano a 1 es que el promedio reflejará rápidamente un cambio rápido en la cantidad observada.
La desventaja es que, si hay un aumento repentino y breve en el valor observado que luego vuelve a un valor promedio, el uso de un $\alpha$ alto puede provocar cambios bruscos o "jerky changes" en el promedio.
Inanición (Starvation): Esta técnica de estimación es tan relevante que a veces se le conoce como "envejecimiento" (aging), ya que se aplica en muchas situaciones donde se deben hacer predicciones basadas en valores anteriores, como en la planificación.
Valor inicial: El valor inicial $S_0$ (o $\tau_0$) se puede definir como una constante o como un promedio general del sistema.
La correcta estimación de las ráfagas de CPU permite a los algoritmos de planificación tomar decisiones más informadas, lo que puede resultar en una mejor utilización de la CPU y tiempos de respuesta más bajos para los usuarios.


8. Comparación y elección de algoritmos
La comparación y elección de algoritmos de planificación es una de las decisiones de diseño más críticas y complejas en el desarrollo de un sistema operativo. No existe un algoritmo de planificación único que sea "el mejor" en todas las situaciones. La eficacia de cualquier algoritmo dado depende de múltiples factores y requiere compromisos entre objetivos a menudo conflictivos.
La selección de un algoritmo de planificación de CPU es un proceso de ingeniería que implica sopesar ventajas y desventajas en función de las características y los objetivos específicos del sistema. Esta decisión se ve influenciada principalmente por:
Depende de la Carga del Sistema
La naturaleza de la carga de trabajo que el sistema operativo debe manejar es un factor determinante en la elección del algoritmo. Los procesos pueden clasificarse, por ejemplo, como CPU-bound (limitados por el procesador, es decir, que pasan la mayor parte del tiempo realizando cálculos) o I/O-bound (limitados por E/S, que pasan la mayor parte del tiempo esperando operaciones de entrada/salida). Un algoritmo que funciona bien para un tipo de proceso puede ser ineficiente para otro.
Sistemas de procesamiento por lotes: En estos sistemas, no hay usuarios esperando una respuesta inmediata. Los objetivos principales son maximizar el rendimiento (throughput) (cantidad de trabajos completados por unidad de tiempo) y la utilización de la CPU. Algoritmos no expulsivos o expulsivos con cuantums largos, como FCFS o SJF (si se pueden estimar los tiempos de ráfaga), pueden ser aceptables, ya que reducen la sobrecarga de conmutación de procesos. Sin embargo, FCFS puede generar el "efecto convoy" y desfavorecer a los procesos I/O-bound, reduciendo la eficiencia general [FCFS section in previous turns].
Sistemas interactivos (tiempo compartido): Aquí, la capacidad de respuesta (response time) es el requisito crítico. Los usuarios esperan una interacción fluida y rápida. Algoritmos expulsivos con cuantums de tiempo adecuados, como Round Robin, son fundamentales para proporcionar a cada usuario una pequeña porción del tiempo de la CPU y dar la apariencia de ejecución simultánea. Los algoritmos que favorecen a los procesos I/O-bound (como algunas variantes de Round Robin o prioridades dinámicas) mejoran la interactividad.
Sistemas de tiempo real: En estos sistemas, el cumplimiento de los plazos de ejecución (deadlines) es la prioridad máxima, y a menudo, la predictibilidad. Fallar un plazo puede tener consecuencias graves. Se utilizan algoritmos estáticos o dinámicos de planificación de tiempo real, como Rate Monotonic Scheduling (RMS) o Earliest Deadline First (EDF). Estos algoritmos garantizan que las tareas críticas se ejecuten a tiempo, incluso si esto significa expulsar procesos de menor prioridad. La planificación de tiempo real es un área activa de investigación en informática.
Sistemas multiprocesador: Con más de un procesador, la planificación se complica. La asignación de procesos a procesadores y el uso de multiprogramación en cada procesador son aspectos clave. En multiprocesadores con pocos procesadores, la disciplina de planificación específica es menos importante, y FCFS o FCFS con prioridades estáticas pueden ser suficientes. Sin embargo, en sistemas con muchos procesadores, la eficiencia individual de cada procesador cede ante el rendimiento general de las aplicaciones. Para la planificación de hilos en multiprocesadores, se consideran enfoques como la compartición de carga, la asignación de procesadores dedicados o la planificación por pandilla.
Soporte de Hardware
Los algoritmos de planificación y, más ampliamente, las técnicas de gestión de recursos, requieren un soporte de hardware específico. La eficacia de un algoritmo depende de la infraestructura de hardware del sistema.
Multiprogramación: El hardware que soporta interrupciones de E/S y Acceso Directo a Memoria (DMA) es crucial para la multiprogramación, ya que permite al procesador continuar con otro trabajo mientras un dispositivo de E/S opera, siendo interrumpido solo al finalizar la operación de E/S.
Gestión de memoria virtual: Técnicas como la paginación o la segmentación (y la memoria virtual en general) son inabordables sin hardware de traducción de direcciones (como la MMU) y otras funciones básicas. El tamaño de página es una decisión de diseño que interacúa con el hardware.
CPUs avanzadas: Las CPUs modernas con canalizaciones (pipelines) o diseños superescalares ejecutan instrucciones en paralelo o desordenadamente, lo que impone una complejidad considerable al sistema operativo y afecta la planificación.
Memoria caché: Las jerarquías de memoria, incluyendo las caches, son fundamentales para el rendimiento. Un cambio de contexto de proceso puede invalidar la caché, lo que hace que la conmutación sea costosa. Los detalles del hardware relacionados con la caché y la localidad de la memoria influyen en el diseño de los administradores de memoria virtual.
Relojes (Temporizadores): Son esenciales para la operación de cualquier sistema multiprogramado, manteniendo la hora del día y evitando que un proceso monopolice la CPU, entre otras cosas. Las interrupciones del reloj son clave para la planificación expulsiva.
Hardware Abstraction Layer (HAL): En sistemas como Windows, el HAL aísla el sistema operativo de las diferencias específicas del hardware, facilitando la portabilidad del kernel y los controladores. Esto significa que, aunque el hardware subyacente puede variar, el sistema operativo puede usar una interfaz consistente.
Peso Relativo de los Criterios (tiempo de respuesta, eficiencia, etc.)
Los criterios de planificación son los objetivos que el sistema operativo intenta optimizar. Como se ha mencionado, estos criterios suelen ser conflictivos y es imposible optimizar todos ellos simultáneamente. La elección del algoritmo implica establecer compromisos y asignar pesos relativos a los distintos requisitos según el tipo y uso del sistema.
Equidad (Fairness): Asegurar que todos los procesos comparables reciban un servicio comparable y evitar la inanición (starvation), donde un proceso es postergado indefinidamente.
Tiempo de respuesta (Response Time): Crucial para la interactividad. Minimizar el tiempo que un usuario espera una respuesta a una petición.
Rendimiento (Throughput): La tasa de finalización de procesos por unidad de tiempo.
Utilización de la CPU (CPU Utilization): El porcentaje de tiempo que la CPU está ocupada realizando trabajo útil.
Plazo de ejecución (Deadlines): El cumplimiento de tiempos límite para la finalización de tareas, crítico en sistemas de tiempo real.
Balanceo de recursos: Mantener todos los recursos del sistema (CPU, E/S) ocupados de manera eficiente.
Predictibilidad: Un trabajo dado debe ejecutarse en un tiempo y coste aproximadamente iguales, independientemente de la carga del sistema, importante en sistemas de tiempo real.
Un buen tiempo de respuesta puede requerir cambios frecuentes de proceso, lo que aumenta la sobrecarga del sistema y reduce el rendimiento. Por el contrario, un alto throughput puede lograrse con menos cambios de contexto, pero a costa de tiempos de respuesta más largos para procesos individuales.
Algoritmo puede cambiar dinámicamente (como en mainframes)
En sistemas complejos, la elección de un algoritmo no siempre es estática; el sistema operativo puede ajustar dinámicamente las políticas de planificación para adaptarse a las condiciones cambiantes del sistema y a los objetivos de rendimiento.
Mainframes: Los grandes sistemas como el IBM OS/390 tienen un Gestor de Recursos del Sistema (SRM) que es capaz de modificar dinámicamente las características de rendimiento de los trabajos basándose en la monitorización de la utilización del sistema y los objetivos de rendimiento establecidos por la instalación.
Prioridades dinámicas: Muchos algoritmos utilizan prioridades que se ajustan durante la ejecución de los procesos [Priorización section in previous turns]. Por ejemplo, en Windows, la prioridad dinámica de un hilo puede fluctuar: baja si usa mucho el quantum de tiempo y sube si se interrumpe para esperar E/S, favoreciendo a los hilos limitados por E/S e interactivos. También se puede promover la prioridad de procesos que han esperado mucho tiempo (envejecimiento) para evitar la inanición.
Planificación multinivel con retroalimentación: Estos algoritmos están diseñados para adaptar la prioridad de los procesos en función de su comportamiento de ejecución. Los procesos que agotan su quantum son movidos a colas de menor prioridad, mientras que los que se bloquean por E/S permanecen en colas de mayor prioridad [Multilevel Feedback section in previous turns].
Balanceo de carga dinámico: En sistemas multiprocesador o multicomputadora, los hilos pueden moverse de la cola de un procesador a la cola de otro para balancear la carga de forma dinámica. Algunos enfoques incluso permiten que el número de hilos de un proceso cambie dinámicamente durante su ejecución, con el sistema operativo y la aplicación colaborando en las decisiones de planificación.
En conclusión, la elección y, a menudo, la adaptación de los algoritmos de planificación de la CPU es un reflejo de la diversidad de los sistemas computacionales, sus cargas de trabajo y sus objetivos de rendimiento. Los diseñadores de sistemas operativos deben considerar cuidadosamente estos factores para lograr un equilibrio óptimo entre la eficiencia del sistema y la satisfacción del usuario.



9. Planificación Compartida Justa (Fair Share Scheduling – FSS)
Este algoritmo es una evolución interesante de la planificación tradicional, diseñada para sistemas multiusuario y entornos complejos.
La Planificación Compartida Justa (FSS) es una clase de algoritmos de planificación que va más allá de tratar a la colección de procesos listos como un simple conjunto homogéneo. En lugar de eso, reconoce y gestiona la estructura de los procesos organizados en aplicaciones o grupos de usuarios. La filosofía detrás de este tipo de planificador es asignar una "participación justa" de los recursos del sistema a cada usuario o grupo, en lugar de centrarse únicamente en el rendimiento individual de un proceso.
Orientada a multiusuario / multigrupo
A diferencia de los planificadores tradicionales que operan sobre una única "bolsa" de procesos listos, FSS está especialmente diseñada para sistemas multiusuario o multigrupo. En un sistema donde las aplicaciones o trabajos de un usuario pueden consistir en múltiples procesos o hilos, la preocupación del usuario no es cómo se desempeña un proceso individual, sino cómo se desempeña el conjunto de sus procesos que constituyen una aplicación. Este enfoque se extiende incluso a grupos de usuarios, por ejemplo, todos los usuarios de un departamento pueden ser considerados miembros del mismo grupo. Así, las decisiones de planificación buscan dar un servicio similar a cada grupo.
Asigna cuotas de CPU por grupo de usuarios
El sistema FSS divide a la comunidad de usuarios en un conjunto de grupos y asigna una fracción del recurso de procesador a cada grupo. Esto significa que, si hay, por ejemplo, cuatro grupos, a cada uno se le podría asignar el 25% del uso del procesador. En efecto, cada grupo de contribución justa recibe un sistema virtual que funciona proporcionalmente más lento que un sistema completo. El objetivo es que si un usuario A tiene el doble de "peso" o "prima" que un usuario B, a largo plazo, el usuario A debería poder realizar el doble de trabajo que el usuario B. El planificador monitorea el uso para dar menos recursos a quienes se han excedido de su cuota y más a quienes han recibido menos.
Fórmula de prioridad
La planificación en FSS se realiza en base a una prioridad que tiene en cuenta varios factores: la prioridad base del proceso, su uso reciente del procesador y el uso reciente del procesador por parte del grupo al que pertenece el proceso. En este esquema, un valor numérico de prioridad más alto representa una prioridad real más baja.
La fórmula de prioridad para un proceso $j$ en el grupo $k$ es la siguiente:
$P_j(i) = Base_j + CPU_j(i)/2 + GCPU_k(i)/(4 \times W_k)$
Donde:
$P_j(i)$: Prioridad del proceso $j$ al comienzo del intervalo de tiempo $i$. Valores más pequeños equivalen a prioridades más altas.
$Base_j$: Prioridad base del proceso $j$. Esta prioridad inicial ayuda a dividir los procesos en bandas fijas de niveles de prioridad.
$CPU_j(i)$: Medida de la utilización del procesador por el proceso $j$ a lo largo del intervalo $i$.
$GCPU_k(i)$: Medida de la utilización del procesador por el grupo $k$ a lo largo del intervalo $i$.
$W_k$: Ponderación (peso o prima) asignada al grupo $k$, con la restricción de que $0 \le W_k \le 1$ y la suma de todas las $W_k$ para todos los grupos debe ser 1. Un mayor peso asignado al grupo significa que su utilización afectará menos a la prioridad de sus procesos.
Promedios de uso se calculan exponencialmente
Las medidas de utilización del procesador para procesos individuales ($CPU_j(i)$) y para grupos ($GCPU_k(i)$) se actualizan en cada intervalo de planificación. La fuente menciona específicamente que estas medidas se calculan con un factor de decaimiento:
$CPU_j(i) = CPU_j(i-1)/2$ $GCPU_k(i) = GCPU_k(i-1)/2$
Estas fórmulas indican que la utilización histórica (previa) decae exponencialmente con un factor de 0.5 en cada intervalo. Es decir, la mitad de la utilización previa se "olvida" en cada paso. Este tipo de cálculo es coherente con el concepto de promedio exponencial (o envejecimiento) que da más peso a las ráfagas recientes, lo que se busca en las estimaciones de ráfagas de CPU, donde $S_{n+1} = \alpha T_n + (1 - \alpha)S_n$. En el contexto de FSS, este decaimiento asegura que el uso pasado no penalice indefinidamente la prioridad futura de un proceso o grupo. La fórmula de tu consulta CPUj[i] = 0.5 * Uj[i-1] + 0.5 * CPUj[i-1] es una forma común de promedio exponencial donde $\alpha = 0.5$ y $Uj[i-1]$ sería el uso actual medido, y $CPUj[i-1]$ la estimación anterior. La fórmula del sistema FSS mostrada en la fuente es específicamente cómo la utilización acumulada previa se reduce antes de que se considere el nuevo uso para la prioridad.
A mayor uso del grupo o proceso → menor prioridad futura
Observando la fórmula de prioridad, podemos ver que tanto el término $CPU_j(i)/2$ (uso del proceso) como el término $GCPU_k(i)/(4 \times W_k)$ (uso del grupo) se suman a la prioridad base. Dado que un valor numérico más alto en $P_j(i)$ representa una prioridad real más baja, esto significa que:
Cuanto más tiempo ha utilizado un proceso el procesador, su $CPU_j(i)$ aumentará (o se mantendrá alto, si el uso es continuo), lo que disminuirá su prioridad efectiva futura.
Cuanto más tiempo ha utilizado el procesador el grupo al que pertenece un proceso, su $GCPU_k(i)$ aumentará, lo que también disminuirá la prioridad efectiva futura de los procesos dentro de ese grupo.
Este mecanismo tiene como objetivo asegurar la justicia: si un proceso o un grupo ha consumido su "contribución justa" de tiempo de CPU, su prioridad se reducirá temporalmente para permitir que otros procesos o grupos que han recibido menos tiempo de CPU tengan una oportunidad de ejecutarse.
Equilibra carga entre usuarios, no solo procesos
El FSS fue diseñado para abordar la limitación de los planificadores tradicionales que tratan los procesos como entidades individuales. Con la planificación FSS, el sistema operativo equilibra la carga de trabajo y el tiempo de CPU entre los diferentes usuarios o grupos de usuarios, en lugar de solo entre procesos individuales. Por ejemplo, si un usuario tiene muchos procesos activos (y por lo tanto consume más tiempo de CPU en total) mientras otro usuario tiene solo un proceso, el FSS intentaría asegurarse de que ambos usuarios reciban su cuota prometida del procesador. Si muchos usuarios de un departamento inician sesión en el sistema, la degradación del tiempo de respuesta afectaría principalmente a los miembros de ese departamento, en lugar de afectar a todos los usuarios del sistema. Un ejemplo ilustra esto: si el proceso A está en un grupo y los procesos B y C están en otro, y cada grupo tiene una ponderación de 0.5, el planificador se asegurará de que el 50% del procesador se asigne al grupo de A y el otro 50% al grupo de B y C, incluso si eso significa alternar la ejecución de A, B y C para cumplir esa proporción.
En resumen, la Planificación Compartida Justa es un algoritmo sofisticado que busca la equidad en sistemas multiusuario al asignar recursos a nivel de grupo o usuario, adaptando dinámicamente las prioridades en función del uso histórico y la ponderación asignada a cada grupo.

📚 Resumen Visual (sintetizado)
Algoritmo
Preemptivo
Favorece
Ráfaga necesaria
Inanición posible
FCFS
No
CPU-bound
No
Sí
RR
Sí
I/O-bound
No
Menor riesgo
SPN/SJF
No
I/O-bound
Sí
Sí
SRT
Sí
I/O-bound
Sí
Sí
MFS
Sí
I/O-bound
No
Sí (sin promoción)
FSS
Sí
Equidad por usuario
Sí
Menor si bien configurado




