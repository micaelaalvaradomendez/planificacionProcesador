1. Concepto de planificaci√≥n de CPU
La planificaci√≥n de CPU se refiere a c√≥mo el sistema operativo (SO) administra el acceso a la Unidad Central de Procesamiento (CPU) entre los diversos procesos que compiten por ella. Es una funci√≥n fundamental del sistema operativo. En esencia, se trata de decidir qu√© proceso en la cola de listos se le asignar√° la CPU. El planificador de CPU, tambi√©n conocido como planificador a corto plazo, es el m√≥dulo del sistema operativo que lleva a cabo este proceso de selecci√≥n, tomando un proceso de aquellos que est√°n en memoria y listos para ejecutar, y asign√°ndole la CPU.
En un sistema uniprocesador, solo un proceso puede ejecutarse a la vez; los dem√°s deben esperar hasta que la CPU est√© libre y pueda ser reasignada. Sin embargo, en un sistema multiprocesador, la planificaci√≥n se complica al decidir qu√© hilo (o proceso) ejecutar y en qu√© CPU.
Importancia en sistemas multitarea y multiprogramaci√≥n
La planificaci√≥n de CPU es la base de los sistemas operativos multiprogramados. Su importancia radica en que:
La multiprogramaci√≥n aumenta la utilizaci√≥n de la CPU al organizar los trabajos (c√≥digo y datos) de modo que la CPU siempre tenga algo que ejecutar. Esto evita que la CPU se quede inactiva cuando un proceso necesita esperar, por ejemplo, por una operaci√≥n de E/S.
En sistemas de tiempo compartido, la planificaci√≥n de CPU y la multiprogramaci√≥n se utilizan para proporcionar a cada usuario una peque√±a porci√≥n del tiempo de la computadora, dando la apariencia de ejecuci√≥n simult√°nea y permitiendo la interacci√≥n con los programas en ejecuci√≥n.
Es crucial en entornos donde m√∫ltiples procesos o hilos compiten por la CPU, ya que la conmutaci√≥n de procesos es costosa y un buen planificador puede marcar una gran diferencia en el rendimiento percibido y la satisfacci√≥n del usuario.
Objetivos
Los algoritmos de planificaci√≥n de CPU buscan satisfacer varios objetivos, que pueden ser orientados al usuario o al sistema:
Maximizar eficiencia (uso de CPU): El objetivo es mantener la CPU ocupada la mayor parte del tiempo. Se busca que siempre haya alg√∫n proceso en ejecuci√≥n para aprovechar al m√°ximo este recurso costoso.
Maximizar throughput (procesos completados por unidad de tiempo): Se refiere a la cantidad de trabajos que el sistema completa por hora. Una pol√≠tica de planificaci√≥n efectiva deber√≠a intentar maximizar este n√∫mero, lo que indica cu√°nto trabajo se est√° realizando en el sistema. Este es un objetivo importante en sistemas de procesamiento por lotes.
Minimizar tiempo de respuesta para el usuario: Este es un criterio orientado al usuario y es fundamental en sistemas interactivos y de tiempo compartido. El tiempo de respuesta es el tiempo transcurrido desde que se env√≠a una solicitud hasta que la respuesta comienza a aparecer como salida. El objetivo es proporcionar un servicio r√°pido e interactivo, por ejemplo, maximizando el n√∫mero de usuarios que experimentan un tiempo de respuesta promedio de dos segundos o menos. Sin embargo, tiempos de respuesta menores a menudo implican mayores costos.
Otros objetivos incluyen la equidad (tratar procesos comparables de manera similar), la aplicaci√≥n de pol√≠ticas (asegurar que se cumplan las pol√≠ticas establecidas, como prioridades) y el balanceo (mantener ocupadas todas las partes del sistema, como CPU y dispositivos de E/S).
Tipos de planificadores
La actividad de planificaci√≥n en muchos sistemas se divide en tres funciones separadas, sugiriendo escalas de tiempo relativas en que se realizan:
Planificador a Largo Plazo (LTS - Long-Term Scheduler):


Tambi√©n conocido como planificador de admisi√≥n o de trabajos, este planificador determina qu√© programas se admiten en el sistema para su procesamiento.
Su funci√≥n principal es controlar el grado de multiprogramaci√≥n (el n√∫mero de procesos activos en el sistema).
Una vez admitido, un programa de usuario se convierte en un proceso y se a√±ade a una cola para ser procesado. Puede considerar factores como la prioridad, el tiempo de ejecuci√≥n esperado y los requisitos de E/S, para mantener una mezcla equilibrada de procesos limitados por CPU y limitados por E/S.
Se invoca cuando un trabajo termina o si la CPU permanece inactiva por encima de un umbral determinado.
Planificador a Medio Plazo (MTS - Medium-Term Scheduler):


Este planificador forma parte de la funci√≥n de intercambio (swapping).
Decide qu√© procesos (o partes de ellos) se mueven entre la memoria principal y la memoria secundaria (disco), realizando operaciones de "swap-in" (carga a memoria) y "swap-out" (descarga a disco).
Su objetivo es gestionar la memoria y ajustar el grado de multiprogramaci√≥n, especialmente en sistemas que no utilizan memoria virtual.
Planificador a Corto Plazo (STS - Short-Term Scheduler) / Planificador de CPU:


Es el planificador m√°s frecuente y activo, encargado de decidir qu√© proceso de la cola de "listos para ejecutar" utilizar√° la CPU en el instante actual.
Se invoca cada vez que la CPU queda ociosa o cuando se cumplen ciertas condiciones (como interrupciones de reloj, interrupciones de E/S o llamadas al sistema).
Utiliza un despachador (dispatcher), un m√≥dulo que transfiere el control de la CPU al proceso seleccionado por el planificador a corto plazo. Esto implica cambiar el contexto, cambiar al modo de usuario y saltar a la ubicaci√≥n correcta en el programa de usuario para reiniciarlo. La latencia de despacho es el tiempo que tarda el despachador en detener un proceso y comenzar otro, y debe ser lo m√°s r√°pida posible.
Existen diversos algoritmos de planificaci√≥n a corto plazo, incluyendo el turno rotatorio (round-robin), la planificaci√≥n por prioridades, y el primero en llegar, primero en ser atendido (FCFS), entre otros. Estos algoritmos pueden ser expulsivos (preemptive), donde un proceso puede ser interrumpido para dar paso a otro de mayor prioridad o si excede su tiempo asignado, o no expulsivos (nonpreemptive), donde un proceso se ejecuta hasta que se bloquea o termina voluntariamente.
Estos tres tipos de planificadores trabajan en conjunto para asegurar el funcionamiento eficiente y ordenado de un sistema operativo, gestionando los procesos a lo largo de su ciclo de vida y optimizando el uso de los recursos del computador.




2. Tipos de planificaci√≥n
La actividad de planificaci√≥n en un sistema operativo se suele dividir en tres funciones separadas, cuyos nombres sugieren las escalas de tiempo relativas con las que se ejecutan: planificaci√≥n a largo, medio y corto plazo.
üîπ Planificador de Largo Plazo (LTS - Long-Term Scheduler)
El planificador a largo plazo, tambi√©n conocido como planificador de admisi√≥n o de trabajos, es el responsable de determinar qu√© programas se admiten en el sistema para su procesamiento. Su funci√≥n principal es controlar el grado de multiprogramaci√≥n, que es el n√∫mero de procesos activos en la memoria principal.
Una vez admitido, un programa de usuario se convierte en un proceso y se a√±ade a una cola, que puede ser la del planificador a corto plazo o, en algunos sistemas, la del planificador a medio plazo si el proceso se crea inicialmente en la zona de intercambio (swap). El planificador a largo plazo opera con una frecuencia relativamente baja (con poca frecuencia). Se puede invocar cada vez que un trabajo termina, o si la fracci√≥n de tiempo que el procesador est√° ocioso excede un determinado valor.
Para tomar la decisi√≥n de admisi√≥n, el LTS puede considerar varios criterios:
Carga del sistema (grado de multiprogramaci√≥n): Limitar el n√∫mero de procesos creados es crucial para no sobrecargar el sistema y proporcionar un servicio satisfactorio al conjunto actual de procesos.
Equilibrio entre procesos CPU-bound e I/O-bound: Puede intentar mantener una mezcla equilibrada de procesos que pasan m√°s tiempo usando la CPU (CPU-bound) y procesos que pasan m√°s tiempo esperando operaciones de E/S (I/O-bound). Una buena mezcla ayuda a mantener la CPU y los dispositivos de E/S ocupados, mejorando la eficiencia general. Un algoritmo de planificaci√≥n que favorece a los programas limitados por E/S les permite emitir sus peticiones de disco r√°pidamente, manteniendo el disco ocupado y mejorando la utilizaci√≥n de la CPU cuando los procesos est√°n limitados por E/S.
Priorizaci√≥n externa, tiempo estimado de ejecuci√≥n, requisitos de E/S: Las decisiones de admisi√≥n pueden basarse en la prioridad del trabajo, el tiempo de ejecuci√≥n esperado y los recursos de E/S que solicitar√°, con el objetivo de equilibrar el uso de los recursos del sistema.
üîπ Planificador de Mediano Plazo (MTS - Medium-Term Scheduler)
La planificaci√≥n a medio plazo es una parte integral de la funci√≥n de intercambio (swapping). Se enfoca en decisiones de intercambio (swapping), determinando qu√© procesos (o partes de ellos) se mueven entre la memoria principal y el almacenamiento secundario (disco).
Sus objetivos principales incluyen:
Liberar memoria: Cuando la memoria principal est√° ocupada y hay una gran cantidad de procesos en estado bloqueado, el sistema operativo puede "suspender" un proceso transfiri√©ndolo a disco para liberar espacio. Esto es especialmente relevante en sistemas que no utilizan memoria virtual.
Mejorar la performance general: Al ajustar el grado de multiprogramaci√≥n y liberar memoria, busca optimizar el rendimiento del sistema. El "swapping" de procesos ayuda a asegurar que haya suficientes procesos listos en memoria principal para mantener ocupado al procesador, reduciendo el tiempo de inactividad. Sin embargo, debe ser inteligente para evitar el "trasiego" (thrashing), una condici√≥n donde el sistema gasta la mayor parte del tiempo transfiriendo partes de procesos entre memoria y disco en lugar de ejecutar instrucciones √∫tiles.
El planificador a medio plazo se ejecuta m√°s frecuentemente que el planificador a largo plazo para tomar estas decisiones de intercambio. La decisi√≥n de qu√© proceso intercambiar puede basarse en la necesidad de gestionar el grado de multiprogramaci√≥n y los requisitos de memoria.
üîπ Planificador de Corto Plazo (STS - Short-Term Scheduler)
El planificador a corto plazo, tambi√©n conocido como activador o despachador (dispatcher), es el planificador m√°s frecuente y activo. Toma las decisiones de grano fino sobre qu√© proceso listo ser√° ejecutado por el procesador a continuaci√≥n. Cuando el STS selecciona un proceso, el despachador es el m√≥dulo que transfiere el control de la CPU a ese proceso. Esta acci√≥n implica guardar el estado del proceso actual y cargar el estado del nuevo proceso, incluyendo sus registros y, en algunos sistemas, su mapa de memoria.
El planificador a corto plazo se activa ante varios eventos que pueden conllevar el bloqueo del proceso actual o que pueden proporcionar la oportunidad de expulsar al proceso actualmente en ejecuci√≥n:
Interrupciones (I/O, reloj, software):
Interrupciones de reloj: Ocurren cuando el proceso en ejecuci√≥n ha excedido su rodaja de tiempo (time slice o quantum) m√°xima asignada. El proceso actual es expulsado y se selecciona otro.
Interrupciones de E/S: Se producen cuando una operaci√≥n de entrada/salida ha finalizado, lo que podr√≠a hacer que uno o m√°s procesos que estaban esperando por ella pasen al estado "listo".
Llamadas al sistema (software): Un proceso puede realizar una llamada al sistema que lo bloquea (por ejemplo, esperando E/S o un recurso), o que le permite abandonar voluntariamente el procesador (por ejemplo, sched_yield() en Linux). Tambi√©n puede ocurrir por se√±ales.
Eventos que cambian el estado de procesos: Como la terminaci√≥n de un proceso o la creaci√≥n de uno nuevo. En sistemas con prioridades, si un proceso de mayor prioridad pasa al estado "listo" mientras uno de menor prioridad est√° ejecutando, el de mayor prioridad puede expulsar al actual. Este comportamiento se conoce como planificaci√≥n expulsiva (preemptive scheduling). Si no hay un reloj disponible, solo es posible la planificaci√≥n no expulsiva (nonpreemptive scheduling), donde un proceso se ejecuta hasta que se bloquea o termina voluntariamente.
La conmutaci√≥n de procesos es una operaci√≥n costosa debido a la sobrecarga de guardar y restaurar el contexto, as√≠ como la posible invalidaci√≥n de la cach√© de la CPU. Por lo tanto, un buen algoritmo de planificaci√≥n a corto plazo busca minimizar esta sobrecarga mientras optimiza los objetivos del sistema, como el tiempo de respuesta y la utilizaci√≥n de la CPU.


3. Criterios de planificaci√≥n
Profundicemos en los Criterios de Planificaci√≥n de la CPU, que son fundamentales para evaluar y dise√±ar algoritmos de planificaci√≥n. Estos criterios nos permiten juzgar qu√© tan bien un planificador cumple con sus objetivos, ya sea desde la perspectiva del usuario o del sistema. A menudo, estos criterios son interdependientes y es imposible optimizar todos ellos de forma simult√°nea, lo que requiere compromisos en el dise√±o de pol√≠ticas. La importancia relativa de cada criterio depender√° de la naturaleza y el uso previsto del sistema.
Los criterios com√∫nmente utilizados para evaluar las pol√≠ticas de planificaci√≥n se pueden clasificar en dos dimensiones: orientados al usuario y orientados al sistema.
üî∏ Orientados al usuario:
Estos criterios se relacionan con el comportamiento del sistema tal como lo percibe el usuario individual o el proceso.
Tiempo de Retorno (Turnaround Time):


Este es el intervalo de tiempo transcurrido desde la entrega de un proceso hasta su finalizaci√≥n. Es una medida crucial para el usuario que representa el tiempo total que un proceso tarda en ejecutarse.
El tiempo de retorno incluye la suma de los periodos en que el proceso espera para entrar en memoria, espera en la cola de listos, se ejecuta en la CPU y realiza operaciones de E/S.
Es una medida apropiada para trabajos por lotes. Un planificador eficiente busca minimizar este tiempo para que los usuarios reciban sus resultados lo antes posible.
El algoritmo de planificaci√≥n de CPU no afecta el tiempo de ejecuci√≥n o de E/S, solo el tiempo que el proceso pasa esperando en la cola de listos.
Tiempo de Respuesta (Response Time):


Para un proceso interactivo, este es el tiempo que transcurre desde el env√≠o de una petici√≥n hasta que la respuesta empieza a recibirse o aparecer como salida.
Es una medida orientada al usuario que es visible y de su inter√©s. Es una mejor medida que el tiempo de retorno desde el punto de vista del usuario en sistemas interactivos, ya que un proceso puede empezar a producir alguna salida mientras sigue procesando la petici√≥n.
El objetivo del mecanismo de planificaci√≥n es lograr bajos tiempos de respuesta y maximizar el n√∫mero de usuarios interactivos que experimentan tiempos de respuesta aceptables (por ejemplo, 2 segundos o menos).
Existe un conflicto inherente entre la utilizaci√≥n de la CPU y el tiempo de respuesta [3a, 7, 209]. Un buen tiempo de respuesta puede requerir que el planificador cambie de procesos frecuentemente, lo que aumenta la sobrecarga del sistema y reduce el throughput.
Plazo de Ejecuci√≥n (Deadlines):


Este criterio es especialmente relevante en sistemas de tiempo real, donde la correcci√≥n del sistema no solo depende del resultado l√≥gico de la computaci√≥n, sino tambi√©n del momento en que se producen los resultados.
Cuando se puede especificar una fecha tope o plazo para un proceso, el planificador debe subordinar otros objetivos a maximizar el porcentaje de plazos cumplidos.
Las tareas pueden clasificarse como de tiempo real duro (hard real-time), que deben cumplir sus plazos estrictamente, ya que un fallo podr√≠a causar un da√±o inaceptable o un error fatal; o de tiempo real suave (soft real-time), donde es deseable cumplir el plazo, pero es tolerable fallar ocasionalmente.
Los plazos pueden ser de comienzo (momento en que la tarea debe empezar) o de conclusi√≥n (momento para el cual la tarea debe estar completada).
üî∏ Orientados al sistema:
Estos criterios se centran en el uso efectivo y eficiente de los recursos del sistema en su totalidad, en lugar de la percepci√≥n individual del usuario.
Utilizaci√≥n de la CPU (CPU Utilization):


Es el porcentaje de tiempo que el procesador est√° ocupado.
En un sistema compartido costoso, maximizar la utilizaci√≥n de la CPU es un criterio significativo. La multiprogramaci√≥n ayuda a mantener la CPU ocupada.
Sin embargo, en sistemas de un solo usuario o en sistemas de tiempo real, este criterio puede ser menos importante que otros (como el tiempo de respuesta o el cumplimiento de plazos).
Como se mencion√≥, puede entrar en conflicto con el tiempo de respuesta [3a, 7, 209].
Throughput (Rendimiento):


Es la tasa con la que los procesos se finalizan o el n√∫mero de procesos completados por unidad de tiempo (por ejemplo, trabajos por hora).
El objetivo es maximizar la cantidad de trabajo que el sistema realiza.
Esta medida es valiosa desde la perspectiva del sistema y concierne m√°s al administrador del sistema que a los usuarios individuales.
Justicia (Fairness):


Implica que, en ausencia de una pol√≠tica expl√≠cita de priorizaci√≥n, todos los procesos que compiten por un recurso deben ser tratados de la misma manera.
Es fundamental asegurar que ning√∫n proceso sufra de inanici√≥n (starvation), una situaci√≥n en la que un proceso listo para ejecutarse es postergado indefinidamente por el planificador.
Se puede aplicar la imposici√≥n de prioridades como una pol√≠tica, donde el planificador deber√≠a favorecer a los procesos con prioridades m√°s altas.
Balance de Recursos (Resource Balancing):


La pol√≠tica del planificador deber√≠a intentar mantener ocupados todos los recursos del sistema, no solo la CPU.
Esto implica favorecer procesos que utilicen poco los recursos que en un momento determinado est√°n sobreutilizados, y buscar una mezcla equilibrada de procesos limitados por CPU (CPU-bound) y procesos limitados por E/S (I/O-bound) para evitar que algunos recursos queden inactivos.
Predictibilidad:


Se desea que un trabajo dado se ejecute en aproximadamente el mismo tiempo y con el mismo coste, a pesar de la carga del sistema.
Grandes variaciones en el tiempo de respuesta o el tiempo de estancia son perjudiciales desde el punto de vista del usuario y pueden indicar inestabilidad o la necesidad de ajustar el sistema.
Es un criterio importante en algunos sistemas de tiempo real, especialmente aquellos que involucran multimedia, para evitar la degradaci√≥n de la calidad.
En resumen, la planificaci√≥n de la CPU es un arte de compromiso. Los dise√±adores de sistemas operativos deben sopesar estos criterios conflictivos y decidir qu√© objetivos son m√°s importantes para el uso particular del sistema. Por ejemplo, un sistema de procesamiento por lotes priorizar√° el throughput y la utilizaci√≥n de la CPU, mientras que un sistema interactivo se centrar√° en un bajo tiempo de respuesta. Los sistemas de tiempo real, por su parte, dar√°n m√°xima importancia al cumplimiento de los plazos.


4. Pol√≠ticas de planificaci√≥n
Las pol√≠ticas de planificaci√≥n son las reglas y criterios que el sistema operativo utiliza para decidir qu√© proceso (o hilo) se ejecutar√° a continuaci√≥n en la CPU. Estas pol√≠ticas se definen a trav√©s de dos elementos clave: el modo de decisi√≥n y la funci√≥n de selecci√≥n.
üîπ Modos de Decisi√≥n:
El modo de decisi√≥n especifica los momentos exactos en que la funci√≥n de selecci√≥n se activa para elegir un nuevo proceso. Existen dos categor√≠as principales:
Non-preemptive (Sin expulsi√≥n / No apropiativo):


En este modo, una vez que un proceso entra en el estado de "Ejecuci√≥n", contin√∫a ejecut√°ndose ininterrumpidamente hasta que ocurre uno de dos eventos:
El proceso termina su ejecuci√≥n.
El proceso se bloquea voluntariamente (por ejemplo, para esperar una operaci√≥n de E/S o para solicitar alg√∫n servicio del sistema operativo).
Esto significa que, incluso si un proceso se ejecuta durante horas, no ser√° suspendido de manera forzosa por el sistema operativo. Las decisiones de planificaci√≥n no se toman durante las interrupciones de reloj peri√≥dicas en este modo. Si no hay un reloj de hardware disponible, la planificaci√≥n no apropiativa es la √∫nica opci√≥n.
Este modo puede penalizar a los procesos cortos y a los procesos limitados por E/S. Por ejemplo, en un sistema uniprocesador, si un proceso "CPU-bound" (que usa mucho la CPU) se ejecuta durante mucho tiempo, los procesos "I/O-bound" (que requieren mucha E/S) pueden quedar esperando, lo que lleva a un uso ineficiente del procesador y los dispositivos de E/S.
Un ejemplo de algoritmo no apropiativo es "Primero en llegar, primero en servirse" (FCFS) y "Primero el proceso m√°s corto" (SJF).
Preemptive (Con expulsi√≥n / Apropiativo):


En este modo, el sistema operativo tiene la capacidad de interrumpir y desalojar a un proceso que se est√° ejecutando en un momento dado, pas√°ndolo al estado de "Listo".
La decisi√≥n de expulsar un proceso puede tomarse en varias situaciones:
Cuando llega un nuevo proceso al sistema, y este tiene una prioridad m√°s alta o el sistema opera con rodajas de tiempo.
Cuando se produce una interrupci√≥n de E/S que hace que un proceso que estaba bloqueado pase al estado "Listo" con una prioridad m√°s alta.
Peri√≥dicamente, bas√°ndose en interrupciones de reloj (tambi√©n conocido como "time slicing" o "rodaja de tiempo"). Si un proceso ha excedido la cantidad m√°xima de tiempo asignada para su ejecuci√≥n, es interrumpido y otro proceso puede ser seleccionado.
Cuando un proceso voluntariamente abandona el procesador sin bloquearse, por ejemplo, mediante una llamada al sistema sched_yield() en Linux, para permitir que otros procesos se ejecuten.
La expulsi√≥n es fundamental para los sistemas de tiempo compartido y de tiempo real, ya que permite mantener la interactividad y cumplir con plazos cr√≠ticos. Permite que el sistema sea m√°s equitativo y evita la inanici√≥n de procesos de baja prioridad, especialmente si se combina con mecanismos de envejecimiento o prioridad din√°mica.
Algoritmos como "Turno rotatorio" (Round Robin) y "Menor tiempo restante" (Shortest Remaining Time, SRT) son ejemplos de planificaci√≥n expulsiva.
üîπ Funci√≥n de Selecci√≥n:
La funci√≥n de selecci√≥n es la parte de la pol√≠tica de planificaci√≥n que determina cu√°l proceso, de entre los que se encuentran en el estado "Listo", ser√° el elegido para ejecutar en la CPU. El planificador a corto plazo (dispatcher) es el encargado de elegir uno de los procesos en la cola de listos.
Esta funci√≥n puede basarse en diversos criterios:
Prioridades: Se elige el proceso con la prioridad m√°s alta. En muchos sistemas, a cada proceso se le asigna una prioridad, y el planificador siempre elegir√° un proceso de mayor prioridad sobre uno de menor prioridad.
Requisitos sobre los recursos: Decisiones basadas en los recursos que el proceso necesita o libera.
Caracter√≠sticas de ejecuci√≥n del proceso: Esto implica medidas cuantitativas como el tiempo total de servicio requerido por el proceso (estimado por el usuario o por el sistema), el tiempo que lleva esperando o el tiempo que ya ha ejecutado.
Ejemplos de c√≥mo la funci√≥n de selecci√≥n trabaja en conjunto con diferentes pol√≠ticas de planificaci√≥n:
En la pol√≠tica "Primero en llegar, primero en servirse" (FCFS), la funci√≥n de selecci√≥n elige el proceso que ha estado esperando servicio por m√°s tiempo.
En el "Turno rotatorio" (Round Robin), una vez que un proceso es expulsado, el siguiente proceso en la cola de listos (siguiendo un orden FCFS) es seleccionado.
En "Primero el proceso m√°s corto" (SPN), se selecciona el proceso con el menor tiempo de procesamiento esperado.
En "Menor tiempo restante" (SRT), una versi√≥n expulsiva de SPN, el planificador escoge el proceso con el menor tiempo de proceso restante esperado.
En "Primero el de mayor tasa de respuesta" (HRRN), la decisi√≥n se basa en una estimaci√≥n del tiempo de estancia normalizado, buscando un compromiso entre favorecer procesos cortos y evitar la inanici√≥n de procesos largos.
Las pol√≠ticas de "Retroalimentaci√≥n" (Feedback) establecen un conjunto de colas de planificaci√≥n (multinivel) y sit√∫an los procesos en las colas bas√°ndose en su historial de ejecuci√≥n, favoreciendo a los procesos cortos y penalizando a los largos con prioridades decrecientes.
La elecci√≥n espec√≠fica de la funci√≥n de selecci√≥n y el modo de decisi√≥n depender√° de los criterios de planificaci√≥n (orientados al usuario y al sistema) que el dise√±ador del sistema operativo desee optimizar, ya que estos criterios a menudo entran en conflicto y requieren compromisos.


5. Priorizaci√≥n
La priorizaci√≥n es un concepto central en la planificaci√≥n de la CPU, ya que permite al sistema operativo tomar decisiones m√°s sofisticadas sobre qu√© proceso ejecutar a continuaci√≥n, m√°s all√° de un simple orden de llegada. A continuaci√≥n, desarrollaremos este concepto.
La priorizaci√≥n es una t√©cnica fundamental en la planificaci√≥n de la CPU que permite al sistema operativo favorecer la ejecuci√≥n de ciertos procesos sobre otros. Esto se logra asignando un nivel de importancia a cada proceso, lo que gu√≠a al planificador en su selecci√≥n.
Uso de m√∫ltiples colas seg√∫n prioridad
En muchos sistemas operativos, para implementar la planificaci√≥n por prioridades, se utiliza un conjunto de colas de procesos listos, organizadas en orden descendente de prioridad. Por ejemplo, podr√≠a haber colas CL0, CL1, ..., CLn, donde los procesos en CL0 tienen la prioridad m√°s alta, CL1 la siguiente m√°s alta, y as√≠ sucesivamente.
Cuando el planificador necesita seleccionar un proceso para ejecutar, comienza examinando la cola de mayor prioridad (CL0). Si esta cola contiene uno o m√°s procesos, se selecciona uno de ellos (a menudo utilizando una pol√≠tica de planificaci√≥n como FCFS dentro de esa cola). Si CL0 est√° vac√≠a, el planificador procede a examinar CL1, y as√≠ sucesivamente, hasta encontrar una cola no vac√≠a.
Este esquema de m√∫ltiples colas tambi√©n puede ser utilizado en planificaci√≥n retroalimentada (feedback scheduling), donde los procesos migran entre colas de diferentes prioridades bas√°ndose en su comportamiento. Por ejemplo, un proceso puede comenzar en una cola de alta prioridad (CL0) y, despu√©s de ser expulsado varias veces o de consumir su rodaja de tiempo, puede ser movido a una cola de menor prioridad (CL1, CL2, etc.). Esto favorece a los procesos cortos y penaliza a los largos.
Algunos sistemas operativos, como UNIX SVR4, tambi√©n utilizan esta estructura, asociando una cola de activaci√≥n con cada nivel de prioridad y ejecutando los procesos dentro de cada cola de manera circular (round-robin). De forma similar, Linux agrupa hilos en clases de prioridad (tiempo real FIFO, tiempo real Round-Robin, y tiempo compartido), con prioridades m√°s altas para las clases de tiempo real.
Prioridades est√°ticas: fijas
Las prioridades est√°ticas son aquellas que se asignan a un proceso en el momento de su creaci√≥n y permanecen fijas o inalterables durante toda la vida √∫til del proceso. En un esquema de planificaci√≥n con prioridades est√°ticas, el planificador siempre elegir√° el proceso con la prioridad m√°s alta de entre todos los procesos listos para ejecutar.
Ejemplos de sistemas o clases de procesos que utilizan prioridades est√°ticas incluyen:
Las tareas de tiempo real en Linux tienen prioridades est√°ticas, lo que significa que no hay c√°lculos de prioridad din√°mica para ellas.
En Windows, los hilos en la clase de prioridad de tiempo real tienen una prioridad fija que nunca cambia.
El algoritmo de tasa mon√≥tona (Rate Monotonic Scheduling, RMS), un algoritmo de planificaci√≥n de tiempo real, asigna prioridades fijas a las tareas bas√°ndose en su frecuencia de ocurrencia (periodo m√°s breve = mayor prioridad).
Este enfoque es m√°s sencillo de implementar y predecible, lo cual es vital en sistemas de tiempo real donde el cumplimiento de plazos es cr√≠tico.
Prioridades din√°micas: se ajustan durante la ejecuci√≥n
Las prioridades din√°micas son aquellas que pueden cambiar (ajustarse) a lo largo del tiempo de ejecuci√≥n de un proceso en funci√≥n de diversos factores, como su comportamiento, el tiempo que ha esperado o el tiempo que ha utilizado la CPU.
Estos ajustes din√°micos buscan mejorar la equidad, la capacidad de respuesta o la eficiencia general del sistema. Algunas estrategias para modificar din√°micamente las prioridades incluyen:
Envejecimiento (Aging): Aumentar la prioridad de un proceso que ha esperado en la cola durante un per√≠odo prolongado. Esto ayuda a evitar la inanici√≥n de procesos de baja prioridad.
Historial de ejecuci√≥n:
Favorecer procesos limitados por E/S: Si un proceso gasta la mayor parte de su tiempo esperando por E/S (I/O-bound), se le puede dar una prioridad m√°s alta para que pueda realizar sus operaciones de E/S r√°pidamente y mantener los dispositivos ocupados, mejorando la utilizaci√≥n del sistema. Por ejemplo, Linux favorece a las tareas limitadas por E/S sobre las limitadas por CPU d√°ndoles mayor prioridad.
Penalizar procesos CPU-bound: Si un proceso consume mucho tiempo de CPU (CPU-bound) y agota su rodaja de tiempo, su prioridad puede ser reducida.
Windows baja la prioridad de un hilo si ha usado completamente su cuanto de tiempo y la sube si se interrumpe para esperar por un evento de E/S. Tambi√©n impulsa la prioridad de un hilo cuando se completa una operaci√≥n de E/S o si est√° esperando un sem√°foro/mutex, especialmente para procesos en primer plano.
Planificaci√≥n retroalimentada: Los procesos se mueven entre diferentes colas de prioridad (o clases de prioridad) en funci√≥n de su tiempo de ejecuci√≥n o de cu√°ntas veces han sido expulsados. Por ejemplo, en UNIX SVR4, la prioridad de los procesos de tiempo compartido es variable y se ajusta seg√∫n el uso del cuanto de tiempo y los eventos de bloqueo.
Peligro de inanici√≥n (starvation) para procesos con baja prioridad
La inanici√≥n (starvation) es una situaci√≥n indeseable en la que un proceso que est√° listo para ejecutarse es denegado de forma continua el acceso al procesador porque otros procesos tienen siempre preferencia. Es decir, aunque el proceso es capaz de avanzar, nunca es seleccionado por el planificador.
Este problema es una amenaza inherente a los esquemas de planificaci√≥n basados en prioridades fijas, especialmente si hay un flujo constante de procesos de mayor prioridad. Un proceso de baja prioridad podr√≠a quedar esperando indefinidamente si siempre hay procesos de mayor prioridad listos para ejecutar.
Adem√°s de la planificaci√≥n de CPU, la inanici√≥n tambi√©n puede ocurrir en otros contextos:
En algoritmos de exclusi√≥n mutua, si la selecci√≥n de un proceso en espera es arbitraria, algunos procesos podr√≠an ser denegados indefinidamente el acceso a una secci√≥n cr√≠tica.
En la planificaci√≥n de disco, si se favorecen las peticiones m√°s recientes (LIFO) o las m√°s cercanas, las peticiones en pistas m√°s lejanas pueden sufrir inanici√≥n.
Para mitigar el riesgo de inanici√≥n, se implementan varias estrategias, muchas de las cuales involucran la priorizaci√≥n din√°mica:
Envejecimiento: Aumentar gradualmente la prioridad de un proceso a medida que pasa m√°s tiempo esperando en la cola. Esto asegura que, eventualmente, incluso un proceso de baja prioridad alcanzar√° una prioridad lo suficientemente alta como para ser ejecutado.
Planificaci√≥n retroalimentada con ajuste de prioridades: Promover procesos a colas de mayor prioridad despu√©s de un cierto tiempo de espera en su cola actual.
Algunos planificadores de E/S, como el planificador de E/S basado en plazos de Linux, est√°n dise√±ados para superar el problema de la inanici√≥n mediante el uso de m√∫ltiples colas y tiempos de expiraci√≥n para las peticiones.
En resumen, la priorizaci√≥n es una herramienta poderosa para optimizar el rendimiento del sistema y la respuesta al usuario, pero debe ser gestionada cuidadosamente para evitar que los procesos de baja prioridad queden en un estado de inanici√≥n prolongada.




6. Algoritmos de planificaci√≥n
Estos algoritmos son el coraz√≥n de la gesti√≥n de procesos en los sistemas operativos y cada uno tiene sus propias caracter√≠sticas, ventajas y desventajas.
‚ñ∂ FCFS (First-Come, First-Served - Primero en Llegar, Primero en Ser Atendido)
El algoritmo FCFS es, con diferencia, el algoritmo de planificaci√≥n de CPU m√°s sencillo. Funciona seg√∫n el principio de que el proceso que solicita la CPU primero es el primero en ser asignado a la CPU.
No apropiativo: Una caracter√≠stica fundamental de FCFS es que es no apropiativo (nonpreemptive). Esto significa que, una vez que la CPU ha sido asignada a un proceso, ese proceso mantiene la CPU hasta que la libera voluntariamente, ya sea porque termina su ejecuci√≥n o porque solicita una operaci√≥n de E/S. No hay interrupciones forzadas por el planificador una vez que un proceso est√° en ejecuci√≥n.
Selecci√≥n por orden de llegada: La implementaci√≥n de la pol√≠tica FCFS es muy sencilla y se gestiona f√°cilmente con una cola FIFO (First-In, First-Out). Cuando un proceso entra en la cola de listos, su Bloque de Control de Proceso (PCB) se enlaza a la cola. Cuando la CPU queda libre, se asigna al proceso que est√° al principio de la cola.
Problemas: A pesar de su simplicidad, FCFS presenta varias deficiencias:
Favorece procesos CPU-bound: FCFS tiende a favorecer a los procesos limitados por el procesador (CPU-bound) sobre los procesos limitados por E/S (I/O-bound). Esto puede llevar al llamado "efecto convoy" (convoy effect). Si un proceso CPU-bound obtiene y retiene la CPU, todos los dem√°s procesos (especialmente los I/O-bound que realizan r√°fagas cortas de CPU) terminan sus operaciones de E/S y se mueven a la cola de listos, esperando la CPU. Mientras esperan, los dispositivos de E/S quedan inactivos. Cuando el proceso CPU-bound finalmente termina su r√°faga de CPU y se mueve a un dispositivo de E/S, todos los procesos I/O-bound, que tienen r√°fagas de CPU cortas, se ejecutan r√°pidamente y regresan a las colas de E/S. En este punto, la CPU puede quedar inactiva.
Baja eficiencia de I/O: El "efecto convoy" resulta en una menor utilizaci√≥n de la CPU y de los dispositivos de E/S. FCFS puede conducir a un uso ineficiente del procesador y de los dispositivos de E/S. Esto es particularmente problem√°tico en sistemas de tiempo compartido, donde es crucial que cada usuario reciba una porci√≥n de la CPU a intervalos regulares. Permitir que un proceso retenga la CPU por un per√≠odo extendido ser√≠a desastroso.
‚ñ∂ Round Robin (RR - Turno Rotatorio)
El algoritmo Round Robin (RR) est√° dise√±ado especialmente para sistemas de tiempo compartido. Es una evoluci√≥n de FCFS que incorpora la apropiaci√≥n (preemption).
Apropiativo: RR es un algoritmo de planificaci√≥n apropiativo (preemptive). Utiliza la preemption basada en un reloj para alternar entre procesos. Una interrupci√≥n de reloj se genera a intervalos peri√≥dicos. Cuando ocurre la interrupci√≥n, el proceso que se est√° ejecutando se coloca en la cola de listos, y el siguiente trabajo listo es seleccionado.
Quantum de tiempo para cada proceso: Una peque√±a unidad de tiempo, llamada quantum de tiempo (time quantum o time slice), se define. Esta duraci√≥n suele ser de entre 10 y 100 milisegundos. La cola de listos se trata como una cola circular. El planificador de CPU recorre la cola, asignando la CPU a cada proceso por un intervalo de tiempo de hasta un quantum. Si un proceso utiliza todo su quantum, la CPU es apropiada para d√°rsela a otro proceso. Si el proceso se bloquea o termina antes de que el quantum haya transcurrido, la CPU se conmuta cuando el proceso se bloquea.
Sensibilidad al tama√±o del quantum: El rendimiento del algoritmo RR depende en gran medida del tama√±o del quantum de tiempo.
Si el quantum es muy grande, la pol√≠tica RR se vuelve id√©ntica a la pol√≠tica FCFS. Una regla general es que el 80% de las r√°fagas de CPU deber√≠an ser m√°s cortas que el quantum.
Si el quantum es demasiado peque√±o (por ejemplo, 1 milisegundo), RR se aproxima al "compartir procesador" (processor sharing), creando la ilusi√≥n de que cada uno de los n procesos tiene su propio procesador funcionando a 1/n de la velocidad del procesador real. Sin embargo, un quantum muy peque√±o resulta en demasiadas conmutaciones de procesos, lo que aumenta la sobrecarga (overhead) de procesamiento debido al manejo de la interrupci√≥n de reloj y a las funciones de planificaci√≥n y despacho. Esto reduce la eficiencia de la CPU.
Un quantum con un valor entre 20 y 50 milisegundos suele ser una soluci√≥n razonable para equilibrar la eficiencia y la capacidad de respuesta.
Soluci√≥n a los problemas: RR Virtual (prioriza I/O-bound): RR tiene la desventaja de tratar de forma desigual a los procesos limitados por CPU y a los limitados por E/S. Los procesos limitados por CPU tienden a usar el quantum completo y regresan inmediatamente a la cola, mientras que los limitados por E/S usan r√°fagas cortas y luego se bloquean. Esto puede resultar en un mal rendimiento para los procesos I/O-bound y un uso ineficiente de los dispositivos de E/S. Para abordar esta injusticia, se ha sugerido una mejora llamada Virtual Round Robin (VRR). En VRR, cuando un proceso se bloquea por una operaci√≥n de E/S, se mueve a una cola auxiliar FCFS. Cuando se toma una decisi√≥n de despacho, los procesos en la cola auxiliar reciben preferencia sobre los de la cola principal de listos. Cuando un proceso de la cola auxiliar es despachado, se le permite ejecutar por un tiempo igual al quantum b√°sico menos el tiempo total que ya ha corrido desde la √∫ltima vez que fue seleccionado de la cola principal. Esto mejora la equidad y el rendimiento para los procesos I/O-bound.
‚ñ∂ SPN / SJF (Shortest Process Next / Shortest Job First - Primero el Proceso M√°s Corto)
El algoritmo SJF (Shortest-Job-First), o tambi√©n conocido como SPN (Shortest Process Next), es otro enfoque para la planificaci√≥n de la CPU. Se asocia a cada proceso la duraci√≥n de su pr√≥xima r√°faga de CPU.
No apropiativo: SPN/SJF es una pol√≠tica de planificaci√≥n no apropiativa. Una vez que un proceso es seleccionado, se ejecuta hasta que termina o se bloquea.
Selecciona el proceso con menor CPU burst estimado: Cuando la CPU est√° disponible, se asigna al proceso que tiene la r√°faga de CPU m√°s peque√±a (el menor tiempo de procesamiento esperado). Si las r√°fagas de CPU de dos procesos son iguales, se utiliza FCFS para desempatar. Este algoritmo es √≥ptimo para proporcionar el tiempo de espera promedio m√°s corto, especialmente cuando todos los trabajos est√°n disponibles al mismo tiempo.
Necesita predicci√≥n del tiempo de r√°faga (media o promedio exponencial): El principal problema de SPN/SJF es la necesidad de conocer, o al menos estimar, el tiempo de procesamiento requerido por cada proceso. Para trabajos por lotes, el sistema podr√≠a exigir que el programador proporcione esta estimaci√≥n. Para procesos interactivos, el sistema operativo puede mantener un promedio del tiempo de ejecuci√≥n de cada "r√°faga" de CPU. Una forma com√∫n de predecir la longitud de la siguiente r√°faga de CPU es mediante la media exponencial (exponential average). Esta t√©cnica asigna un peso decreciente a las predicciones m√°s antiguas, reaccionando m√°s r√°pidamente a los cambios en el comportamiento del proceso.
Posible inanici√≥n de procesos largos: Un riesgo importante con SPN/SJF es la inanici√≥n (starvation) para los procesos m√°s largos, especialmente si hay un flujo constante de procesos m√°s cortos. Un proceso largo podr√≠a no tener la oportunidad de ejecutarse si siempre hay procesos m√°s cortos listos que llegan constantemente. Aunque reduce la predilecci√≥n por trabajos largos, no es ideal para entornos de tiempo compartido debido a la falta de apropiaci√≥n.
‚ñ∂ SRT (Shortest Remaining Time - Menor Tiempo Restante)
El algoritmo SRT es una versi√≥n apropiativa de SPN/SJF.
Apropiativo. Variante de SJF: En SRT, el planificador siempre elige el proceso que tiene el menor tiempo de procesamiento restante esperado.
Reemplaza al proceso si otro con menor r√°faga restante aparece: Cuando un nuevo proceso se une a la cola de listos, su tiempo restante de ejecuci√≥n se compara con el tiempo restante del proceso que se est√° ejecutando actualmente. Si el nuevo proceso necesita menos tiempo para terminar, el proceso actual es expulsado (preempted) y el nuevo proceso se inicia. Esto permite que los trabajos cortos nuevos obtengan un buen servicio. Al igual que SJF, SRT tambi√©n necesita una estimaci√≥n del tiempo de procesamiento y tiene el riesgo de inanici√≥n para procesos largos.
Menor overhead que RR: SRT no tiene el sesgo a favor de los procesos largos que se encuentra en FCFS. A diferencia de Round Robin, no genera interrupciones adicionales peri√≥dicas para la apropiaci√≥n, lo que reduce la sobrecarga. Sin embargo, requiere almacenar los tiempos de servicio transcurridos, lo que s√≠ genera cierta sobrecarga. Se espera que mejore los tiempos de estancia de SPN, ya que un trabajo corto obtiene preferencia sobre un trabajo m√°s largo en ejecuci√≥n.
‚ñ∂ Retroalimentaci√≥n Multinivel (Multilevel Feedback Scheduling - MFS)
Los algoritmos de planificaci√≥n de colas multinivel se utilizan en situaciones donde los procesos pueden clasificarse en diferentes grupos, como procesos interactivos (primer plano) y procesos por lotes (segundo plano). El algoritmo de colas de retroalimentaci√≥n multinivel es una versi√≥n m√°s compleja que permite a los procesos moverse entre colas.
Varias colas de prioridades: La idea es separar los procesos seg√∫n las caracter√≠sticas de sus r√°fagas de CPU. Se utiliza un conjunto de m√∫ltiples colas de prioridades (por ejemplo, CL0, CL1, CLn). Cada cola puede tener su propio algoritmo de planificaci√≥n (por ejemplo, Round Robin para colas de alta prioridad y FCFS para la cola de menor prioridad). El planificador primero ejecuta todos los procesos en la cola de mayor prioridad. Solo cuando esa cola est√° vac√≠a, pasa a la siguiente.
Proceso baja de nivel si usa todo el quantum: Si un proceso utiliza demasiado tiempo de CPU (es decir, agota su quantum), ser√° movido a una cola de menor prioridad. Por ejemplo, un proceso puede comenzar en la cola 0, recibir un quantum y, si no termina, pasar a la cola 1. Si agota otro quantum, pasa a la cola 2, y as√≠ sucesivamente. Esto se conoce como democi√≥n.
Favorece procesos I/O-bound: Este esquema tiende a dejar los procesos I/O-bound e interactivos en las colas de mayor prioridad. Un proceso I/O-bound, al tener r√°fagas de CPU cortas, es probable que se bloquee por E/S antes de agotar su quantum, lo que le permite permanecer en una cola de alta prioridad. Los procesos CPU-bound, que agotan sus quanta, bajan autom√°ticamente de prioridad.
Algoritmo adaptable y justo si se implementa bien: Los algoritmos de retroalimentaci√≥n multinivel son flexibles y pueden ser justos si se implementan correctamente, ya que se ajustan al comportamiento de los procesos durante la ejecuci√≥n.
Variantes: Hay varias variaciones de este esquema:
Tama√±o de quantum variable (ej: Q = 2^(i-1)): Para evitar que los procesos largos tarden demasiado en completarse, se pueden variar los tiempos de apropiaci√≥n seg√∫n la cola. Por ejemplo, a un proceso en la cola CLi se le permite ejecutar por 2^i unidades de tiempo antes de ser expulsado. Esto se ilustra en el ejemplo de los fuentes con q = 2^i.
Promoci√≥n despu√©s de cierto tiempo para evitar inanici√≥n: El problema del esquema simple es que los procesos m√°s largos pueden sufrir de inanici√≥n (starvation) si llegan continuamente nuevos trabajos. Para evitar esto, se puede aumentar la prioridad de un proceso (promoverlo) si ha esperado demasiado tiempo en una cola de baja prioridad. Esta forma de envejecimiento (aging) previene la inanici√≥n.
Estos algoritmos de planificaci√≥n representan las bases sobre las cuales se construyen planificadores m√°s complejos en sistemas operativos modernos, a menudo combinando elementos de varios de ellos para lograr un equilibrio entre los distintos criterios de rendimiento.




7. Estimaci√≥n de r√°fagas de CPU
Para algoritmos de planificaci√≥n como "Primero el Proceso M√°s Corto" (SPN/SJF) y "Menor Tiempo Restante" (SRT), es esencial conocer o estimar la duraci√≥n de la pr√≥xima r√°faga de CPU de un proceso. Dado que es imposible saber con exactitud el futuro tiempo de procesamiento de un proceso interactivo, el sistema operativo recurre a t√©cnicas de predicci√≥n basadas en el comportamiento pasado. Dos m√©todos comunes para realizar esta estimaci√≥n son el promedio simple y el promedio ponderado (tambi√©n conocido como promedio exponencial o envejecimiento).
Promedio simple: S[n+1] = (1/n) ‚àë T[i]
El promedio simple es una manera b√°sica de estimar la duraci√≥n de la pr√≥xima r√°faga de CPU.
F√≥rmula: La f√≥rmula para el promedio simple es: $S_{n+1} = \frac{1}{n} \sum_{i=1}^{n} T_i$ Donde:
$T_i$ es el tiempo de ejecuci√≥n del procesador para la $i$-√©sima instancia de este proceso (o el tiempo de r√°faga del procesador para trabajos interactivos).
$S_{n+1}$ es el valor predicho para la siguiente instancia.
$n$ es el n√∫mero de instancias anteriores medidas.
Funcionamiento: Esta f√≥rmula suma los tiempos de todas las r√°fagas de CPU anteriores ($T_i$) y los divide por el n√∫mero total de r√°fagas (n) para obtener un promedio.
Caracter√≠stica clave: En este m√©todo, cada t√©rmino de la suma tiene el mismo peso; es decir, cada r√°faga de CPU pasada contribuye de igual manera a la predicci√≥n de la siguiente r√°faga.
Limitaci√≥n: El problema con el promedio simple es que trata todas las instancias pasadas con igual importancia. Sin embargo, a menudo se desea que las instancias m√°s recientes tengan un mayor peso, ya que es m√°s probable que reflejen el comportamiento futuro del proceso.
Para evitar recalcular la suma completa cada vez, la Ecuaci√≥n (9.1) puede reescribirse como: $S_{n+1} = \frac{1}{n} T_n + \frac{n-1}{n} S_n$.
Promedio ponderado (o promedio exponencial): S[n+1] = Œ±T[n] + (1-Œ±)S[n]
El promedio ponderado, o promedio exponencial (exponential average), es una t√©cnica com√∫n y m√°s sofisticada para predecir un valor futuro bas√°ndose en una serie temporal de valores pasados. Esta t√©cnica asigna un peso decreciente a las observaciones m√°s antiguas, dando m√°s importancia a las r√°fagas recientes.
F√≥rmula: La f√≥rmula para el promedio exponencial es: $S_{n+1} = \alpha T_n + (1 - \alpha) S_n$ Donde:
$T_n$ es la duraci√≥n de la $n$-√©sima (m√°s reciente) r√°faga de CPU.
$S_n$ (o $\tau_n$ en algunas notaciones) es el valor predicho para la $n$-√©sima r√°faga de CPU (es decir, la estimaci√≥n anterior para la r√°faga que acaba de ocurrir).
$S_{n+1}$ (o $\tau_{n+1}$) es el valor predicho para la siguiente r√°faga de CPU.
$\alpha$ es un factor de ponderaci√≥n constante (constant weighting factor), con un valor que se encuentra entre 0 y 1 (es decir, $0 \le \alpha \le 1$, o $0 < \alpha < 1$ en algunas fuentes).
Da m√°s peso a r√°fagas recientes: El par√°metro $\alpha$ controla el peso relativo de la historia reciente ($T_n$) y la historia pasada ($S_n$) en la predicci√≥n.
Si $\alpha = 0$, entonces $S_{n+1} = S_n$. En este caso, la historia reciente no tiene ning√∫n efecto, y se asume que las condiciones actuales son transitorias.
Si $\alpha = 1$, entonces $S_{n+1} = T_n$. Aqu√≠, solo importa la r√°faga de CPU m√°s reciente, y se asume que la historia pasada es irrelevante.
Un valor m√°s com√∫n es $\alpha = 1/2$, lo que significa que la historia reciente y la historia pasada tienen un peso equitativo en la predicci√≥n. En este caso, la t√©cnica es muy sencilla de implementar: se suma el nuevo valor a la estimaci√≥n actual y se divide entre 2 (lo que puede hacerse con un simple desplazamiento de bits a la derecha).
Si $\alpha > 0.5$, se le da m√°s peso a las observaciones m√°s recientes. Por ejemplo, para $\alpha = 0.8$, pr√°cticamente todo el peso se da a las cuatro observaciones m√°s recientes, mientras que para $\alpha = 0.2$, el promedio se extiende sobre las ocho observaciones m√°s recientes.
Funcionamiento a largo plazo: El promedio exponencial considera todos los valores pasados, pero los menos recientes tienen cada vez menos peso. Esto se puede ver al expandir la f√≥rmula: $S_{n+1} = \alpha T_n + (1 - \alpha)\alpha T_{n-1} + \dots + (1 - \alpha)^i \alpha T_{n-i} + \dots + (1 - \alpha)^n S_1$ Dado que $\alpha$ y $(1-\alpha)$ son menores que 1, cada t√©rmino sucesivo en la ecuaci√≥n anterior es m√°s peque√±o, lo que significa que las observaciones m√°s antiguas tienen un impacto decreciente en la predicci√≥n.
Ventajas y desventajas:
La ventaja de usar un valor de $\alpha$ cercano a 1 es que el promedio reflejar√° r√°pidamente un cambio r√°pido en la cantidad observada.
La desventaja es que, si hay un aumento repentino y breve en el valor observado que luego vuelve a un valor promedio, el uso de un $\alpha$ alto puede provocar cambios bruscos o "jerky changes" en el promedio.
Inanici√≥n (Starvation): Esta t√©cnica de estimaci√≥n es tan relevante que a veces se le conoce como "envejecimiento" (aging), ya que se aplica en muchas situaciones donde se deben hacer predicciones basadas en valores anteriores, como en la planificaci√≥n.
Valor inicial: El valor inicial $S_0$ (o $\tau_0$) se puede definir como una constante o como un promedio general del sistema.
La correcta estimaci√≥n de las r√°fagas de CPU permite a los algoritmos de planificaci√≥n tomar decisiones m√°s informadas, lo que puede resultar en una mejor utilizaci√≥n de la CPU y tiempos de respuesta m√°s bajos para los usuarios.


8. Comparaci√≥n y elecci√≥n de algoritmos
La comparaci√≥n y elecci√≥n de algoritmos de planificaci√≥n es una de las decisiones de dise√±o m√°s cr√≠ticas y complejas en el desarrollo de un sistema operativo. No existe un algoritmo de planificaci√≥n √∫nico que sea "el mejor" en todas las situaciones. La eficacia de cualquier algoritmo dado depende de m√∫ltiples factores y requiere compromisos entre objetivos a menudo conflictivos.
La selecci√≥n de un algoritmo de planificaci√≥n de CPU es un proceso de ingenier√≠a que implica sopesar ventajas y desventajas en funci√≥n de las caracter√≠sticas y los objetivos espec√≠ficos del sistema. Esta decisi√≥n se ve influenciada principalmente por:
Depende de la Carga del Sistema
La naturaleza de la carga de trabajo que el sistema operativo debe manejar es un factor determinante en la elecci√≥n del algoritmo. Los procesos pueden clasificarse, por ejemplo, como CPU-bound (limitados por el procesador, es decir, que pasan la mayor parte del tiempo realizando c√°lculos) o I/O-bound (limitados por E/S, que pasan la mayor parte del tiempo esperando operaciones de entrada/salida). Un algoritmo que funciona bien para un tipo de proceso puede ser ineficiente para otro.
Sistemas de procesamiento por lotes: En estos sistemas, no hay usuarios esperando una respuesta inmediata. Los objetivos principales son maximizar el rendimiento (throughput) (cantidad de trabajos completados por unidad de tiempo) y la utilizaci√≥n de la CPU. Algoritmos no expulsivos o expulsivos con cuantums largos, como FCFS o SJF (si se pueden estimar los tiempos de r√°faga), pueden ser aceptables, ya que reducen la sobrecarga de conmutaci√≥n de procesos. Sin embargo, FCFS puede generar el "efecto convoy" y desfavorecer a los procesos I/O-bound, reduciendo la eficiencia general [FCFS section in previous turns].
Sistemas interactivos (tiempo compartido): Aqu√≠, la capacidad de respuesta (response time) es el requisito cr√≠tico. Los usuarios esperan una interacci√≥n fluida y r√°pida. Algoritmos expulsivos con cuantums de tiempo adecuados, como Round Robin, son fundamentales para proporcionar a cada usuario una peque√±a porci√≥n del tiempo de la CPU y dar la apariencia de ejecuci√≥n simult√°nea. Los algoritmos que favorecen a los procesos I/O-bound (como algunas variantes de Round Robin o prioridades din√°micas) mejoran la interactividad.
Sistemas de tiempo real: En estos sistemas, el cumplimiento de los plazos de ejecuci√≥n (deadlines) es la prioridad m√°xima, y a menudo, la predictibilidad. Fallar un plazo puede tener consecuencias graves. Se utilizan algoritmos est√°ticos o din√°micos de planificaci√≥n de tiempo real, como Rate Monotonic Scheduling (RMS) o Earliest Deadline First (EDF). Estos algoritmos garantizan que las tareas cr√≠ticas se ejecuten a tiempo, incluso si esto significa expulsar procesos de menor prioridad. La planificaci√≥n de tiempo real es un √°rea activa de investigaci√≥n en inform√°tica.
Sistemas multiprocesador: Con m√°s de un procesador, la planificaci√≥n se complica. La asignaci√≥n de procesos a procesadores y el uso de multiprogramaci√≥n en cada procesador son aspectos clave. En multiprocesadores con pocos procesadores, la disciplina de planificaci√≥n espec√≠fica es menos importante, y FCFS o FCFS con prioridades est√°ticas pueden ser suficientes. Sin embargo, en sistemas con muchos procesadores, la eficiencia individual de cada procesador cede ante el rendimiento general de las aplicaciones. Para la planificaci√≥n de hilos en multiprocesadores, se consideran enfoques como la compartici√≥n de carga, la asignaci√≥n de procesadores dedicados o la planificaci√≥n por pandilla.
Soporte de Hardware
Los algoritmos de planificaci√≥n y, m√°s ampliamente, las t√©cnicas de gesti√≥n de recursos, requieren un soporte de hardware espec√≠fico. La eficacia de un algoritmo depende de la infraestructura de hardware del sistema.
Multiprogramaci√≥n: El hardware que soporta interrupciones de E/S y Acceso Directo a Memoria (DMA) es crucial para la multiprogramaci√≥n, ya que permite al procesador continuar con otro trabajo mientras un dispositivo de E/S opera, siendo interrumpido solo al finalizar la operaci√≥n de E/S.
Gesti√≥n de memoria virtual: T√©cnicas como la paginaci√≥n o la segmentaci√≥n (y la memoria virtual en general) son inabordables sin hardware de traducci√≥n de direcciones (como la MMU) y otras funciones b√°sicas. El tama√±o de p√°gina es una decisi√≥n de dise√±o que interac√∫a con el hardware.
CPUs avanzadas: Las CPUs modernas con canalizaciones (pipelines) o dise√±os superescalares ejecutan instrucciones en paralelo o desordenadamente, lo que impone una complejidad considerable al sistema operativo y afecta la planificaci√≥n.
Memoria cach√©: Las jerarqu√≠as de memoria, incluyendo las caches, son fundamentales para el rendimiento. Un cambio de contexto de proceso puede invalidar la cach√©, lo que hace que la conmutaci√≥n sea costosa. Los detalles del hardware relacionados con la cach√© y la localidad de la memoria influyen en el dise√±o de los administradores de memoria virtual.
Relojes (Temporizadores): Son esenciales para la operaci√≥n de cualquier sistema multiprogramado, manteniendo la hora del d√≠a y evitando que un proceso monopolice la CPU, entre otras cosas. Las interrupciones del reloj son clave para la planificaci√≥n expulsiva.
Hardware Abstraction Layer (HAL): En sistemas como Windows, el HAL a√≠sla el sistema operativo de las diferencias espec√≠ficas del hardware, facilitando la portabilidad del kernel y los controladores. Esto significa que, aunque el hardware subyacente puede variar, el sistema operativo puede usar una interfaz consistente.
Peso Relativo de los Criterios (tiempo de respuesta, eficiencia, etc.)
Los criterios de planificaci√≥n son los objetivos que el sistema operativo intenta optimizar. Como se ha mencionado, estos criterios suelen ser conflictivos y es imposible optimizar todos ellos simult√°neamente. La elecci√≥n del algoritmo implica establecer compromisos y asignar pesos relativos a los distintos requisitos seg√∫n el tipo y uso del sistema.
Equidad (Fairness): Asegurar que todos los procesos comparables reciban un servicio comparable y evitar la inanici√≥n (starvation), donde un proceso es postergado indefinidamente.
Tiempo de respuesta (Response Time): Crucial para la interactividad. Minimizar el tiempo que un usuario espera una respuesta a una petici√≥n.
Rendimiento (Throughput): La tasa de finalizaci√≥n de procesos por unidad de tiempo.
Utilizaci√≥n de la CPU (CPU Utilization): El porcentaje de tiempo que la CPU est√° ocupada realizando trabajo √∫til.
Plazo de ejecuci√≥n (Deadlines): El cumplimiento de tiempos l√≠mite para la finalizaci√≥n de tareas, cr√≠tico en sistemas de tiempo real.
Balanceo de recursos: Mantener todos los recursos del sistema (CPU, E/S) ocupados de manera eficiente.
Predictibilidad: Un trabajo dado debe ejecutarse en un tiempo y coste aproximadamente iguales, independientemente de la carga del sistema, importante en sistemas de tiempo real.
Un buen tiempo de respuesta puede requerir cambios frecuentes de proceso, lo que aumenta la sobrecarga del sistema y reduce el rendimiento. Por el contrario, un alto throughput puede lograrse con menos cambios de contexto, pero a costa de tiempos de respuesta m√°s largos para procesos individuales.
Algoritmo puede cambiar din√°micamente (como en mainframes)
En sistemas complejos, la elecci√≥n de un algoritmo no siempre es est√°tica; el sistema operativo puede ajustar din√°micamente las pol√≠ticas de planificaci√≥n para adaptarse a las condiciones cambiantes del sistema y a los objetivos de rendimiento.
Mainframes: Los grandes sistemas como el IBM OS/390 tienen un Gestor de Recursos del Sistema (SRM) que es capaz de modificar din√°micamente las caracter√≠sticas de rendimiento de los trabajos bas√°ndose en la monitorizaci√≥n de la utilizaci√≥n del sistema y los objetivos de rendimiento establecidos por la instalaci√≥n.
Prioridades din√°micas: Muchos algoritmos utilizan prioridades que se ajustan durante la ejecuci√≥n de los procesos [Priorizaci√≥n section in previous turns]. Por ejemplo, en Windows, la prioridad din√°mica de un hilo puede fluctuar: baja si usa mucho el quantum de tiempo y sube si se interrumpe para esperar E/S, favoreciendo a los hilos limitados por E/S e interactivos. Tambi√©n se puede promover la prioridad de procesos que han esperado mucho tiempo (envejecimiento) para evitar la inanici√≥n.
Planificaci√≥n multinivel con retroalimentaci√≥n: Estos algoritmos est√°n dise√±ados para adaptar la prioridad de los procesos en funci√≥n de su comportamiento de ejecuci√≥n. Los procesos que agotan su quantum son movidos a colas de menor prioridad, mientras que los que se bloquean por E/S permanecen en colas de mayor prioridad [Multilevel Feedback section in previous turns].
Balanceo de carga din√°mico: En sistemas multiprocesador o multicomputadora, los hilos pueden moverse de la cola de un procesador a la cola de otro para balancear la carga de forma din√°mica. Algunos enfoques incluso permiten que el n√∫mero de hilos de un proceso cambie din√°micamente durante su ejecuci√≥n, con el sistema operativo y la aplicaci√≥n colaborando en las decisiones de planificaci√≥n.
En conclusi√≥n, la elecci√≥n y, a menudo, la adaptaci√≥n de los algoritmos de planificaci√≥n de la CPU es un reflejo de la diversidad de los sistemas computacionales, sus cargas de trabajo y sus objetivos de rendimiento. Los dise√±adores de sistemas operativos deben considerar cuidadosamente estos factores para lograr un equilibrio √≥ptimo entre la eficiencia del sistema y la satisfacci√≥n del usuario.



9. Planificaci√≥n Compartida Justa (Fair Share Scheduling ‚Äì FSS)
Este algoritmo es una evoluci√≥n interesante de la planificaci√≥n tradicional, dise√±ada para sistemas multiusuario y entornos complejos.
La Planificaci√≥n Compartida Justa (FSS) es una clase de algoritmos de planificaci√≥n que va m√°s all√° de tratar a la colecci√≥n de procesos listos como un simple conjunto homog√©neo. En lugar de eso, reconoce y gestiona la estructura de los procesos organizados en aplicaciones o grupos de usuarios. La filosof√≠a detr√°s de este tipo de planificador es asignar una "participaci√≥n justa" de los recursos del sistema a cada usuario o grupo, en lugar de centrarse √∫nicamente en el rendimiento individual de un proceso.
Orientada a multiusuario / multigrupo
A diferencia de los planificadores tradicionales que operan sobre una √∫nica "bolsa" de procesos listos, FSS est√° especialmente dise√±ada para sistemas multiusuario o multigrupo. En un sistema donde las aplicaciones o trabajos de un usuario pueden consistir en m√∫ltiples procesos o hilos, la preocupaci√≥n del usuario no es c√≥mo se desempe√±a un proceso individual, sino c√≥mo se desempe√±a el conjunto de sus procesos que constituyen una aplicaci√≥n. Este enfoque se extiende incluso a grupos de usuarios, por ejemplo, todos los usuarios de un departamento pueden ser considerados miembros del mismo grupo. As√≠, las decisiones de planificaci√≥n buscan dar un servicio similar a cada grupo.
Asigna cuotas de CPU por grupo de usuarios
El sistema FSS divide a la comunidad de usuarios en un conjunto de grupos y asigna una fracci√≥n del recurso de procesador a cada grupo. Esto significa que, si hay, por ejemplo, cuatro grupos, a cada uno se le podr√≠a asignar el 25% del uso del procesador. En efecto, cada grupo de contribuci√≥n justa recibe un sistema virtual que funciona proporcionalmente m√°s lento que un sistema completo. El objetivo es que si un usuario A tiene el doble de "peso" o "prima" que un usuario B, a largo plazo, el usuario A deber√≠a poder realizar el doble de trabajo que el usuario B. El planificador monitorea el uso para dar menos recursos a quienes se han excedido de su cuota y m√°s a quienes han recibido menos.
F√≥rmula de prioridad
La planificaci√≥n en FSS se realiza en base a una prioridad que tiene en cuenta varios factores: la prioridad base del proceso, su uso reciente del procesador y el uso reciente del procesador por parte del grupo al que pertenece el proceso. En este esquema, un valor num√©rico de prioridad m√°s alto representa una prioridad real m√°s baja.
La f√≥rmula de prioridad para un proceso $j$ en el grupo $k$ es la siguiente:
$P_j(i) = Base_j + CPU_j(i)/2 + GCPU_k(i)/(4 \times W_k)$
Donde:
$P_j(i)$: Prioridad del proceso $j$ al comienzo del intervalo de tiempo $i$. Valores m√°s peque√±os equivalen a prioridades m√°s altas.
$Base_j$: Prioridad base del proceso $j$. Esta prioridad inicial ayuda a dividir los procesos en bandas fijas de niveles de prioridad.
$CPU_j(i)$: Medida de la utilizaci√≥n del procesador por el proceso $j$ a lo largo del intervalo $i$.
$GCPU_k(i)$: Medida de la utilizaci√≥n del procesador por el grupo $k$ a lo largo del intervalo $i$.
$W_k$: Ponderaci√≥n (peso o prima) asignada al grupo $k$, con la restricci√≥n de que $0 \le W_k \le 1$ y la suma de todas las $W_k$ para todos los grupos debe ser 1. Un mayor peso asignado al grupo significa que su utilizaci√≥n afectar√° menos a la prioridad de sus procesos.
Promedios de uso se calculan exponencialmente
Las medidas de utilizaci√≥n del procesador para procesos individuales ($CPU_j(i)$) y para grupos ($GCPU_k(i)$) se actualizan en cada intervalo de planificaci√≥n. La fuente menciona espec√≠ficamente que estas medidas se calculan con un factor de decaimiento:
$CPU_j(i) = CPU_j(i-1)/2$ $GCPU_k(i) = GCPU_k(i-1)/2$
Estas f√≥rmulas indican que la utilizaci√≥n hist√≥rica (previa) decae exponencialmente con un factor de 0.5 en cada intervalo. Es decir, la mitad de la utilizaci√≥n previa se "olvida" en cada paso. Este tipo de c√°lculo es coherente con el concepto de promedio exponencial (o envejecimiento) que da m√°s peso a las r√°fagas recientes, lo que se busca en las estimaciones de r√°fagas de CPU, donde $S_{n+1} = \alpha T_n + (1 - \alpha)S_n$. En el contexto de FSS, este decaimiento asegura que el uso pasado no penalice indefinidamente la prioridad futura de un proceso o grupo. La f√≥rmula de tu consulta CPUj[i] = 0.5 * Uj[i-1] + 0.5 * CPUj[i-1] es una forma com√∫n de promedio exponencial donde $\alpha = 0.5$ y $Uj[i-1]$ ser√≠a el uso actual medido, y $CPUj[i-1]$ la estimaci√≥n anterior. La f√≥rmula del sistema FSS mostrada en la fuente es espec√≠ficamente c√≥mo la utilizaci√≥n acumulada previa se reduce antes de que se considere el nuevo uso para la prioridad.
A mayor uso del grupo o proceso ‚Üí menor prioridad futura
Observando la f√≥rmula de prioridad, podemos ver que tanto el t√©rmino $CPU_j(i)/2$ (uso del proceso) como el t√©rmino $GCPU_k(i)/(4 \times W_k)$ (uso del grupo) se suman a la prioridad base. Dado que un valor num√©rico m√°s alto en $P_j(i)$ representa una prioridad real m√°s baja, esto significa que:
Cuanto m√°s tiempo ha utilizado un proceso el procesador, su $CPU_j(i)$ aumentar√° (o se mantendr√° alto, si el uso es continuo), lo que disminuir√° su prioridad efectiva futura.
Cuanto m√°s tiempo ha utilizado el procesador el grupo al que pertenece un proceso, su $GCPU_k(i)$ aumentar√°, lo que tambi√©n disminuir√° la prioridad efectiva futura de los procesos dentro de ese grupo.
Este mecanismo tiene como objetivo asegurar la justicia: si un proceso o un grupo ha consumido su "contribuci√≥n justa" de tiempo de CPU, su prioridad se reducir√° temporalmente para permitir que otros procesos o grupos que han recibido menos tiempo de CPU tengan una oportunidad de ejecutarse.
Equilibra carga entre usuarios, no solo procesos
El FSS fue dise√±ado para abordar la limitaci√≥n de los planificadores tradicionales que tratan los procesos como entidades individuales. Con la planificaci√≥n FSS, el sistema operativo equilibra la carga de trabajo y el tiempo de CPU entre los diferentes usuarios o grupos de usuarios, en lugar de solo entre procesos individuales. Por ejemplo, si un usuario tiene muchos procesos activos (y por lo tanto consume m√°s tiempo de CPU en total) mientras otro usuario tiene solo un proceso, el FSS intentar√≠a asegurarse de que ambos usuarios reciban su cuota prometida del procesador. Si muchos usuarios de un departamento inician sesi√≥n en el sistema, la degradaci√≥n del tiempo de respuesta afectar√≠a principalmente a los miembros de ese departamento, en lugar de afectar a todos los usuarios del sistema. Un ejemplo ilustra esto: si el proceso A est√° en un grupo y los procesos B y C est√°n en otro, y cada grupo tiene una ponderaci√≥n de 0.5, el planificador se asegurar√° de que el 50% del procesador se asigne al grupo de A y el otro 50% al grupo de B y C, incluso si eso significa alternar la ejecuci√≥n de A, B y C para cumplir esa proporci√≥n.
En resumen, la Planificaci√≥n Compartida Justa es un algoritmo sofisticado que busca la equidad en sistemas multiusuario al asignar recursos a nivel de grupo o usuario, adaptando din√°micamente las prioridades en funci√≥n del uso hist√≥rico y la ponderaci√≥n asignada a cada grupo.

üìö Resumen Visual (sintetizado)
Algoritmo
Preemptivo
Favorece
R√°faga necesaria
Inanici√≥n posible
FCFS
No
CPU-bound
No
S√≠
RR
S√≠
I/O-bound
No
Menor riesgo
SPN/SJF
No
I/O-bound
S√≠
S√≠
SRT
S√≠
I/O-bound
S√≠
S√≠
MFS
S√≠
I/O-bound
No
S√≠ (sin promoci√≥n)
FSS
S√≠
Equidad por usuario
S√≠
Menor si bien configurado




