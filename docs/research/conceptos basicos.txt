¿Qué hacen los Sistemas Operativos?
El sistema operativo (SO) es una capa de software fundamental que se ejecuta directamente sobre el hardware de una computadora y sirve como base para todas las demás aplicaciones de software. Su propósito principal es proporcionar un modelo de computadora mejor, más simple y pulcro a los programas de usuario, y administrar todos los recursos del sistema. Actúa como intermediario entre los programas de aplicación, las herramientas y los usuarios, y el hardware de la computadora.
Podemos entender lo que hace un sistema operativo desde varias perspectivas:
1. Objetivos del Sistema Operativo
Los sistemas operativos se construyen para alcanzar varios objetivos clave, a veces conflictivos:
Facilidad de uso (Conveniencia): El SO hace que una computadora sea más sencilla de usar para los usuarios y programadores, ocultando las complejidades del hardware subyacente.
Eficiencia: Permite que los recursos del sistema de computación se utilicen de una manera eficiente.
Capacidad para evolucionar: El SO debe estar diseñado de tal forma que permita el desarrollo, prueba e introducción efectiva de nuevas funciones en el sistema sin interferir con el servicio existente. Esto implica un diseño modular con interfaces claras.
Generalidad: Es deseable manejar todos los dispositivos de manera uniforme, tanto en la forma en que los procesos ven los dispositivos de E/S como en la forma en que el SO los gestiona.
2. Funciones y Servicios Generales del Sistema Operativo
El sistema operativo proporciona una variedad de servicios esenciales en varias áreas:
Desarrollo de programas: Ofrece utilidades y servicios como editores y depuradores para asistir a los programadores en la creación de software.
Ejecución de programas: Realiza las tareas necesarias para que un programa se ejecute, incluyendo cargar instrucciones y datos en la memoria principal, inicializar dispositivos de E/S y ficheros, y preparar otros recursos.
Acceso a dispositivos de E/S: Proporciona una interfaz uniforme que esconde los detalles específicos de cada dispositivo, permitiendo a los programadores acceder a ellos mediante operaciones sencillas como lecturas y escrituras.
Acceso controlado a los ficheros: Gestiona el acceso a los ficheros, entendiendo la naturaleza del dispositivo de almacenamiento (disco, cinta) y la estructura de los datos. En sistemas multiusuario, también implementa mecanismos de protección para controlar el acceso no autorizado.
Acceso al sistema: En entornos compartidos o públicos, el SO controla el acceso general al sistema y a recursos específicos, protegiéndolos del uso no autorizado y resolviendo conflictos de recursos.
Detección y respuesta a errores: El sistema operativo debe proporcionar una respuesta que elimine la condición de error, minimizando el impacto en las aplicaciones en ejecución. Esto puede ir desde finalizar un programa hasta reintentar una operación o simplemente informar del error.
Contabilidad: Recopila estadísticas de uso de recursos y monitorea parámetros de rendimiento (como el tiempo de respuesta). Esta información es valiosa para anticipar futuras necesidades de mejoras, optimizar el sistema y, en sistemas multiusuario, para facturar a los usuarios.
3. Perspectivas sobre el Sistema Operativo
El sistema operativo puede ser visto desde diferentes puntos de vista, dependiendo del rol de quien interactúa con él:
a) Perspectiva del Usuario/Programador de Aplicaciones (SO como Máquina Extendida)
Desde esta perspectiva, el sistema operativo actúa como una máquina extendida o virtual. La arquitectura a nivel de lenguaje máquina de la mayoría de las computadoras es primitiva y compleja de programar, especialmente para la entrada/salida. El SO oculta esta complejidad inherente del hardware ("feo") y presenta a los programas y programadores abstracciones "hermosas, elegantes, simples y consistentes" con las que trabajar.
Las abstracciones clave que el SO proporciona desde esta visión incluyen:
Procesos: Una abstracción de un programa en ejecución. Cada proceso tiene su propio espacio de direcciones, un conjunto de recursos asociados (registros, archivos abiertos, etc.) y un estado. El SO permite que múltiples procesos compartan un procesador.
Espacios de direcciones: Una abstracción de la memoria principal, donde cada proceso tiene su propio rango de ubicaciones de memoria. Esto aísla a los programas entre sí y del SO.
Archivos: Una abstracción de los dispositivos de almacenamiento de E/S. El sistema de archivos oculta las peculiaridades de los discos y otros dispositivos, ofreciendo un modelo limpio e independiente del dispositivo para el almacenamiento de datos a largo plazo.
Entrada/salida (E/S): El SO administra los dispositivos de E/S y proporciona una interfaz sencilla e independiente de los dispositivos para interactuar con ellos.
Protección: Mecanismos para salvaguardar la información y los recursos, evitando que un usuario interfiera con otro o con el propio sistema operativo.
El Shell: Una interfaz de usuario (de línea de comandos o gráfica) que permite a los usuarios interactuar con el sistema operativo y ejecutar comandos, aunque no es parte del SO en sí, utiliza sus llamadas al sistema.
El SO facilita la programación y la gestión de la complejidad, permitiendo a los desarrolladores enfocarse en la lógica de la aplicación en lugar de en los intrincados detalles del hardware.
b) Perspectiva del Diseñador del Sistema/Administrador de Recursos (SO como Gestor de Recursos)
Desde esta perspectiva, el SO es un administrador de recursos. Una computadora es un conjunto de recursos (procesadores, memoria principal, dispositivos de E/S, almacenamiento) que el SO se encarga de gestionar.
Las responsabilidades principales del SO como gestor de recursos son:
Asignación de recursos: Llevar un registro de qué programa está utilizando qué recursos y otorgar las peticiones de recursos.
Contabilización: Medir y registrar el uso de los recursos del sistema.
Mediación de conflictos: Resolver peticiones en conflicto de diferentes programas y usuarios para los mismos recursos.
Protección: Asegurar que los recursos estén protegidos y que los usuarios no interfieran unos con otros, especialmente en entornos multiusuario.
El SO es un mecanismo de control inusual en dos aspectos: sus funciones son un conjunto de programas ejecutados por el procesador, y frecuentemente cede el control, dependiendo del procesador para retomarlo. Por ejemplo, en un entorno multiprogramado, el SO debe gestionar los procesos y recursos, decidir el patrón de entrelazado para la ejecución y asignar recursos para maximizar la utilización del procesador y los dispositivos de E/S, mientras se proporcionan tiempos de respuesta razonables.
4. Estructura y Conceptos Fundamentales
Para lograr estas funciones y objetivos, los sistemas operativos se estructuran de diversas formas, como sistemas monolíticos, de capas, microkernels, cliente-servidor, máquinas virtuales o exokernels. Independientemente de la estructura, conceptos como los procesos (e hilos), la gestión de memoria (espacios de direcciones, paginación, segmentación), la gestión de E/S, el sistema de archivos y la seguridad son fundamentales en todos los sistemas operativos modernos.
En resumen, el sistema operativo se encarga de convertir un hardware complejo en una plataforma accesible y útil para los programas de usuario, al tiempo que gestiona de manera eficiente y segura los recursos disponibles para el beneficio de todos los usuarios y aplicaciones.
Organización del sistema de cómputo
La organización de un sistema de cómputo se puede entender como una estructura en capas o jerárquica, compuesta fundamentalmente por cuatro componentes principales: el hardware, el sistema operativo (SO), los programas de aplicación y los usuarios. Estos componentes interactúan entre sí para permitir que el sistema funcione y resuelva los problemas de cómputo de los usuarios.
A continuación, se detalla cada componente:
1. Hardware del Computador
El hardware es la base física del sistema y proporciona los recursos de cómputo fundamentales. A un nivel superior, un computador consta de procesador (CPU), memoria y componentes de entrada/salida (E/S).
Procesador (CPU): Es el "cerebro" del computador. Controla el funcionamiento del computador y realiza sus funciones de procesamiento de datos. Obtiene instrucciones de la memoria y las ejecuta en un ciclo básico de búsqueda, decodificación y ejecución. Los sistemas modernos pueden tener chips multinúcleo con múltiples CPUs independientes para mejorar el rendimiento. El sistema operativo debe estar al tanto de todos los registros del procesador para guardar y restaurar su estado cuando se multiplexa la CPU.
Memoria: Incluye la memoria principal (RAM) y los dispositivos de almacenamiento. La memoria principal es un recurso importante que el SO debe administrar cuidadosamente. Existe una jerarquía de memoria con diferentes velocidades y costos (caché, memoria principal, almacenamiento en disco, almacenamiento removible). Un módulo de memoria consta de un conjunto de posiciones con direcciones numeradas secuencialmente, que pueden contener instrucciones o datos.
Dispositivos de Entrada/Salida (E/S): Son los mecanismos que permiten al computador interactuar con el mundo exterior. Incluyen componentes como discos, impresoras, teclado, ratón, pantallas y interfaces de red. Los dispositivos de E/S suelen constar de un controlador de dispositivo (un chip que controla físicamente el dispositivo) y el dispositivo en sí. El SO debe gestionar estos dispositivos, que varían ampliamente en función y velocidad. Las técnicas de E/S han evolucionado, reduciendo la intervención directa del procesador.
2. Sistema Operativo (Software en Modo Núcleo)
El sistema operativo (SO) es un programa fundamental que se ejecuta directamente sobre el hardware. Actúa como una capa de software intermedia entre los programas de aplicación, las herramientas y los usuarios, y el hardware de la computadora.
El SO cumple dos funciones básicas pero no relacionadas:
Máquina Extendida o Virtual: Oculta la complejidad y las peculiaridades del hardware subyacente. Presenta a los programas y programadores abstracciones "agradables, elegantes, simples y consistentes" con las que trabajar, como procesos, espacios de direcciones y archivos, haciendo que la programación sea mucho más sencilla.
Administrador de Recursos: Gestiona todos los recursos del sistema (procesadores, memoria, dispositivos de E/S) para una asignación ordenada y controlada entre los diversos programas que compiten por ellos. Lleva un registro de qué programa utiliza qué recursos, otorga las peticiones de recursos, contabiliza su uso y media en las solicitudes conflictivas de diferentes programas y usuarios. El SO multiplexa los recursos en tiempo (por ejemplo, compartiendo la CPU) y en espacio (por ejemplo, dividiendo la memoria).
El SO es una pieza crítica de software que se ejecuta en modo kernel (o modo supervisor), lo que le otorga acceso total al hardware y a los datos del sistema, y lo protege de intentos de modificación por parte de programas de usuario. Los objetivos de diseño de un SO suelen ser facilidad de uso (conveniencia), eficiencia y capacidad para evolucionar. El núcleo (kernel) es la parte del SO que reside en la memoria principal y contiene las funciones más frecuentemente utilizadas, gestionando la planificación de hilos, el intercambio de procesos, el manejo de interrupciones, entre otros.
3. Programas de Aplicación (Software en Modo Usuario)
Los programas de aplicación son el software que define cómo se utilizan los recursos del hardware para resolver los problemas de cómputo de los usuarios. Estos programas son desarrollados por programadores de aplicaciones y pueden ser desde procesadores de texto, hojas de cálculo, compiladores y navegadores web, hasta juegos complejos. Se ejecutan en modo usuario, lo que significa que tienen acceso limitado a los datos del sistema y dependen del sistema operativo para interactuar con el hardware y otros recursos.
4. Usuarios
Los usuarios son las personas que interactúan con el sistema de cómputo. Desde la perspectiva del usuario final, el sistema se ve principalmente en términos de las aplicaciones que utiliza, sin preocuparse por los detalles internos del hardware. La interfaz de usuario, ya sea un shell (basado en texto) o una Interfaz Gráfica de Usuario (GUI), es el programa con el que los usuarios suelen interactuar. Aunque el shell o la GUI utilizan el sistema operativo para realizar su trabajo, no son parte del SO en sí.
En conjunto, estos cuatro componentes forman un sistema interconectado y jerárquico. El hardware proporciona los cimientos, el sistema operativo gestiona y abstrae el hardware para los programas, los programas de aplicación resuelven las tareas específicas, y los usuarios interactúan con las aplicaciones para lograr sus fines.
Arquitectura del sistema de cómputo
Comprendo. En nuestra conversación anterior, hemos establecido que la organización de un sistema de cómputo se compone de hardware, sistema operativo (SO), programas de aplicación y usuarios. Ahora, profundicemos en la arquitectura del sistema de cómputo, centrándonos en los modelos subyacentes del hardware y cómo el sistema operativo se adapta a ellos, desde el modelo tradicional hasta las configuraciones más modernas de multiprocesamiento y clusters.
1. Modelo de Computadora Tradicional (Arquitectura von Neumann implícita)
Aunque las fuentes no utilizan explícitamente el término "arquitectura von Neumann", describen el modelo fundamental que la caracteriza: una máquina secuencial con un solo procesador que ejecuta instrucciones de forma lineal.
En este modelo, una computadora consta de:
Un procesador (CPU): El "cerebro" que controla el funcionamiento y realiza el procesamiento de datos. Su ciclo básico es buscar, decodificar y ejecutar una instrucción a la vez.
Memoria principal (RAM): Donde el procesador obtiene las instrucciones y los datos.
Componentes de Entrada/Salida (E/S): Para interactuar con el mundo exterior.
Estos componentes se interconectan, típicamente a través de un bus del sistema, para permitir la ejecución de programas. Históricamente, este fue el modelo dominante, y los primeros sistemas operativos se encargaban de gestionar un solo programa cargado manualmente a la vez.
2. Multiprocesamiento
El concepto de multiprocesamiento surge cuando un sistema de cómputo contiene más de un procesador. Esta evolución es impulsada por la búsqueda constante de mayor poder de cómputo y, en algunos casos, para mejorar la fiabilidad. La distinción clave aquí es el procesamiento paralelo real, a diferencia del pseudoparalelismo que se logra con la multiprogramación en un solo procesador (donde los procesos se entrelazan en el tiempo).
Los sistemas multiprocesador pueden clasificarse de varias maneras:
Procesamiento fuertemente acoplado: Un conjunto de procesadores que comparten la memoria principal y están bajo el control integrado de un único sistema operativo. Aquí se incluyen los multiprocesadores simétricos (SMP) y asimétricos.
Débilmente acoplado o multiprocesador distribuido (clusters): Una colección de sistemas relativamente autónomos, donde cada procesador tiene su propia memoria principal y canales de E/S.
a) Multiprocesamiento Simétrico (SMP)
El multiprocesamiento simétrico (SMP) es una forma de organización de sistemas multiprocesador en la que múltiples procesadores comparten la misma memoria principal y dispositivos de E/S, interconectados por un bus de comunicación u otro esquema de conexión interna. Una característica clave es que todos los procesadores pueden realizar las mismas funciones ("simétrico").
Arquitectura Hardware SMP:
Múltiples procesadores, cada uno con su unidad de control, unidad aritmético-lógica y registros.
Acceso a una memoria principal compartida a través de un mecanismo de interconexión (comúnmente un bus compartido).
Los procesadores pueden comunicarse entre sí a través de la memoria (mediante mensajes o información de estado en espacios de memoria compartidos).
Consideraciones de Diseño del Sistema Operativo para SMP: El sistema operativo de un SMP debe gestionar los procesadores y otros recursos de manera que el usuario perciba el sistema como un uniprocesador multiprogramado. Esto implica:
Procesos o hilos simultáneos concurrentes: Las rutinas del núcleo deben ser reentrantes, permitiendo que varios procesadores ejecuten el mismo código del núcleo simultáneamente. Las tablas y estructuras del núcleo deben gestionarse cuidadosamente para evitar interbloqueos u operaciones inválidas.
Planificación: Puede ser realizada por cualquier procesador, por lo que se deben evitar conflictos. Si se utiliza multihilo a nivel de núcleo, es posible planificar múltiples hilos del mismo proceso simultáneamente en diferentes procesadores.
Fiabilidad y tolerancia a fallos: El sistema operativo no debe degradarse si un procesador falla. Debe reestructurar las tablas de gestión adecuadamente.
Multiplexación de recursos: El SO multiplexa los recursos en tiempo (ej. compartiendo la CPU) y en espacio (ej. dividiendo la memoria).
Ventajas de SMP:
Rendimiento: Si el trabajo puede paralelizarse, un SMP supera a un uniprocesador del mismo tipo. Con multiproceso, más de un proceso puede ejecutarse simultáneamente, cada uno en un procesador diferente.
Crecimiento incremental: El rendimiento se puede mejorar añadiendo más procesadores.
Fiabilidad: La falla de un procesador no detiene todo el sistema; el trabajo puede redistribuirse.
Transparencia al usuario: La existencia de múltiples procesadores es transparente al usuario, ya que el SO se encarga de la planificación y sincronización.
Ejemplos de SO con soporte SMP: Casi todos los sistemas operativos modernos, incluyendo Windows, Windows XP, Mac OS X y Linux, proporcionan soporte para SMP.
Relación con Multihilamiento: Aunque complementarias, las técnicas de multihilamiento y SMP son independientes. El multihilamiento es útil incluso en uniprocesadores para estructurar aplicaciones, y un SMP beneficia a procesos sin hilos al permitir la ejecución paralela.
b) Multiprocesamiento Asimétrico (Maestro/Esclavo)
En una arquitectura maestro/esclavo, el núcleo del sistema operativo siempre se ejecuta en un procesador determinado (el "maestro"). Los demás procesadores ("esclavos") solo pueden ejecutar programas de usuario y, quizás, algunas utilidades del sistema operativo. El maestro es responsable de toda la planificación de procesos e hilos. Si un esclavo necesita servicios (ej. una llamada de E/S), debe enviar una petición al maestro y esperar.
Desventajas:
Punto único de fallo: Un fallo en el procesador maestro derriba todo el sistema.
Cuello de botella: El maestro puede convertirse en un cuello de botella en sistemas con muchas CPUs, ya que es el único responsable de la planificación y gestión de procesos.
Este enfoque es más simple de implementar que el SMP porque requiere menos mejoras a un sistema operativo multiprogramado uniprocesador, y la resolución de conflictos se simplifica al tener un procesador que controla todos los recursos.
c) Chips Multinúcleo (Multicore)
Con la evolución de la tecnología, los fabricantes de microprocesadores han comenzado a integrar múltiples procesadores completos, o núcleos (cores), dentro de un solo chip. Estos chips multinúcleo son, en esencia, pequeños multiprocesadores o CMPs (Multiprocesadores a nivel de chip).
Desde la perspectiva del software, los CMPs no son muy distintos de los multiprocesadores basados en bus, y por lo tanto, para aprovechar un chip multinúcleo se requiere un sistema operativo multiprocesador. El sistema operativo los ve como CPUs separadas, lo que exige una planificación cuidadosa para evitar ineficiencias, como planificar múltiples hilos en el mismo núcleo mientras otro permanece inactivo. La combinación de máquinas virtuales y CPUs multinúcleo permite configurar una sola CPU (de escritorio) como una multicomputadora de muchos nodos virtualmente.
3. Clusters
Un cluster es un grupo de computadoras completas e interconectadas que trabajan juntas como un recurso de computación unificado, pudiendo crear la ilusión de ser una única máquina. El término "computadora completa" (o nodo) significa que cada sistema puede operar por sí mismo, aparte del cluster.
Beneficios y Objetivos:
Escalabilidad absoluta: Posibilidad de crear un cluster que supere la potencia de las máquinas más grandes, con decenas o centenas de nodos.
Escalabilidad incremental: Permite añadir nuevos sistemas al cluster en pequeños incrementos, expandiendo la capacidad según las necesidades.
Alta disponibilidad: Proporcionan una alta probabilidad de que todos los recursos estén en servicio, con mecanismos de recuperación ante fallos (failover).
Rendimiento: Logran alto rendimiento para aplicaciones de servidor.
Configuraciones de Cluster:
Sin disco compartido (shared-nothing): Los nodos se interconectan solo a través de un enlace de alta velocidad (LAN dedicada) para mensajes y coordinación. Cada nodo tiene sus propios discos. Windows Server Cluster utiliza este modelo.
Con disco compartido: Además del enlace entre nodos, hay un subsistema de discos que está directamente enlazado con múltiples computadoras del cluster, a menudo utilizando sistemas RAID para redundancia.
Consideraciones de Diseño del Sistema Operativo para Clusters: Para aprovechar un cluster, los sistemas operativos necesitan mejoras para gestionar:
Gestión de Fallos: Los clusters con alta disponibilidad aseguran que los recursos estén en servicio, y si un nodo falla, otro puede tomar el control de la aplicación (failover). Esto requiere copiar información constantemente entre sistemas.
Equilibrado de Carga (Load Balancing): Distribuir uniformemente la carga entre las computadoras disponibles, escalando automáticamente cuando se añaden nuevos nodos. El middleware puede facilitar la migración de servicios.
Computación Paralela: Permitir que una única aplicación se ejecute en paralelo en el cluster mediante:
Compilación paralela: El compilador identifica partes de la aplicación para ejecución paralela.
Aplicaciones paralelas: El programador escribe la aplicación para el cluster, utilizando paso de mensajes entre nodos.
Computación paramétrica: Un algoritmo se ejecuta muchas veces con diferentes parámetros o condiciones iniciales.
Imagen Única del Sistema (Single-System Image - SSI): El middleware del cluster es una capa de software que proporciona una SSI al usuario, dando la ilusión de un sistema único. Los servicios y funciones deseables para una SSI incluyen:
Un único espacio de memoria (memoria compartida distribuida).
Un único sistema de control de trabajos (planificador global).
Una única interfaz de usuario.
Un único espacio de E/S (acceso remoto a periféricos o discos).
Un único espacio de procesos (identificación uniforme, comunicación entre procesos remotos).
Puntos de control (para recuperación ante fallos).
Migración de procesos (para balanceo de carga).
Clusters frente a SMP:
Característica
SMP
Cluster
Gestión
Más fácil de gestionar y configurar
Más complejo de configurar
Espacio físico
Menos espacio físico y energía
Mayor espacio físico y energía
Escalabilidad
Limitada
Superior (incremental y absoluta)
Disponibilidad
Bueno, pero un fallo de CPU puede afectar al bus.
Superior (alta redundancia de componentes)
Modelo base
Cercano al monoprocesador
Colección de computadoras completas

Ejemplos de SO con soporte de Cluster: Windows Server Cluster, Sun Clusters, y clusters Beowulf (implementados mayormente en computadoras Linux).
4. Multicomputadoras (Contexto Distribuido)
Las multicomputadoras son sistemas con acoplamiento fuerte de CPUs que no comparten memoria. Cada CPU tiene su propia memoria privada. A diferencia de los multiprocesadores que usan memoria compartida, las multicomputadoras se comunican enviando mensajes entre sí. A menudo se les conoce también como "clusters de computadoras" o "COWS (Clusters of Workstations)".
Se sitúan en un punto intermedio entre los multiprocesadores (memoria compartida) y los sistemas distribuidos (acoplamiento más débil a nivel geográfico). Aunque son más fáciles de construir que los multiprocesadores, son más difíciles de programar debido al paso explícito de mensajes.
Estructura del Sistema Operativo
Profundicemos en la estructura del Sistema Operativo (SO), analizando en detalle los conceptos de multiprogramación, multitarea, planificación y uso de memoria virtual. Estos elementos son pilares fundamentales que permiten a los sistemas operativos modernos gestionar los recursos del computador y proporcionar un entorno eficiente y fácil de usar para los usuarios y programas de aplicación.
1. Multiprogramación
La multiprogramación es un concepto fundamental que surgió para mejorar la eficiencia de los sistemas de cómputo. En sus inicios, las primeras máquinas eran muy costosas y el tiempo que se perdía en la planificación y configuración de trabajos era inaceptable.
Definición y Objetivo: La multiprogramación implica tener múltiples programas residentes en la memoria principal al mismo tiempo, lo que permite que el procesador (CPU) conmute rápidamente entre ellos. El objetivo principal es maximizar la utilización del procesador y los dispositivos de E/S. Si un programa necesita esperar por una operación de entrada/salida (E/S), el procesador puede asignarse a otro programa que esté listo para ejecutarse, evitando así periodos de inactividad de la CPU. Esto crea una apariencia de paralelismo (pseudoparalelismo) incluso en sistemas con un solo procesador.
Requisitos y Desafíos: Los sistemas operativos multiprogramados son más sofisticados que los monoprogramados. Para que funcionen eficazmente, requieren:
Gestión de memoria: Debe haber suficiente memoria para contener al sistema operativo y a varios programas de usuario simultáneamente. El SO debe subdividir la memoria de usuario y gestionarla.
Algoritmo de planificación: Si varios trabajos están listos para ejecutarse, el procesador debe decidir cuál ejecutar, lo que requiere un algoritmo de planificación.
Protección: Los programas deben protegerse entre sí y del sistema operativo para evitar interferencias, como la modificación accidental de datos.
Gestión de recursos: El sistema operativo debe administrar los conflictos por recursos compartidos como impresoras o dispositivos de almacenamiento.
2. Multitarea
El término multitarea es, en esencia, sinónimo de multiprogramación y se refiere a un modo de operación que permite la ejecución intercalada de dos o más tareas de cómputo. Sin embargo, a menudo se usa para enfatizar una experiencia de usuario más interactiva y responsiva.
Contexto Moderno: La necesidad de multitarea se ha acentuado con el incremento de la velocidad y capacidad de memoria de los microprocesadores, y el soporte de memoria virtual, lo que ha llevado a aplicaciones más complejas e interrelacionadas.
Aplicaciones: Por ejemplo, un usuario puede desear usar simultáneamente un procesador de texto, un programa de dibujo y una hoja de cálculo. Sin multitarea, las tareas tendrían que completarse secuencialmente.
Computación Cliente/Servidor: La multitarea también es crucial en el paradigma cliente/servidor, donde un cliente y uno o más servidores colaboran en una aplicación. El sistema operativo debe soportar el hardware de comunicación, protocolos y arquitecturas de transferencia de datos para manejar esta interacción.
Ejemplo: Windows (a partir de Windows 2000) es un sistema operativo multitarea diseñado para computadoras personales, estaciones de trabajo y servidores.
3. Planificación (Scheduling)
La planificación es la función principal de cualquier sistema operativo moderno que gestiona los procesos. Es el mecanismo que el sistema operativo utiliza para asignar el tiempo de ejecución del procesador entre los diversos procesos o hilos que compiten por él.
Objetivos: Los objetivos de la planificación incluyen:
Equidad: Conceder a todos los procesos que compiten un acceso equitativo al recurso.
Eficiencia: Utilizar el tiempo del procesador de forma eficiente, manteniéndolo ocupado.
Tiempo de respuesta: Minimizar el tiempo que tarda el sistema en responder a una solicitud del usuario (importante en sistemas interactivos).
Rendimiento: Maximizar la cantidad de trabajo completado por unidad de tiempo.
Ausencia de inanición: Asegurar que ningún proceso se quede sin ejecutar indefinidamente.
Aplicación de políticas: Asegurar que se cumplan las políticas establecidas, por ejemplo, dando prioridad a ciertos trabajos.
Equilibrio: Mantener ocupadas todas las partes del sistema.
Tipos de Planificación: El sistema operativo realiza tres tipos de decisiones de planificación para el procesador:
Planificación a largo plazo (Admisión): Decide cuándo se admiten nuevos procesos al sistema. Es responsable de controlar el grado de multiprogramación, es decir, el número de procesos que pueden estar activos en el sistema.
Planificación a medio plazo: Es parte de la función de intercambio (swapping) y determina cuándo un programa (o parte de él) se trae a la memoria principal para que pueda ejecutarse. Se utiliza para gestionar el grado de multiprogramación y la memoria.
Planificación a corto plazo (Dispatcher): Decide cuál de los procesos listos para ejecutar se asigna al procesador. Se ejecuta con mucha frecuencia. Una estrategia común es el round-robin (turno rotatorio), donde cada proceso obtiene un intervalo de tiempo. Otra es asignar niveles de prioridad a los procesos.
Planificación de E/S: Decide qué petición de E/S pendiente será atendida por un dispositivo de E/S disponible.
Planificación en Multiprocesadores (SMP): En sistemas multiprocesador simétricos (SMP), la planificación se vuelve más compleja. El núcleo puede ejecutarse en cualquier procesador, y cada procesador realiza su propia planificación de los procesos o hilos disponibles. Se requiere que las rutinas del núcleo sean reentrantes y que se gestionen las estructuras de datos para evitar conflictos. La planificación puede ser a nivel de proceso o a nivel de hilo.
Planificación por afinidad: Intenta que un hilo se ejecute en la misma CPU en que lo hizo la última vez para aprovechar las cachés locales.
Planificación por pandilla (Gang scheduling): Planificación simultánea de hilos relacionados de un mismo proceso en diferentes procesadores para aplicaciones paralelas de grano medio o fino.
Multihilo y Planificación: En sistemas operativos modernos, la planificación se realiza a nivel de hilo (thread). Un proceso puede contener múltiples hilos, cada uno con su propio estado de ejecución, contexto y pila. Los hilos son las entidades que el sistema operativo planifica y activa.
4. Uso de Memoria Virtual
La memoria virtual es una de las técnicas más sofisticadas y esenciales en la gestión de memoria de los sistemas operativos contemporáneos.
Definición y Propósito: La memoria virtual permite a los programas direccionar la memoria desde un punto de vista lógico, sin preocuparse por la cantidad de memoria principal física disponible. Crea la ilusión de un espacio de direcciones grande e ilimitado para cada proceso, que puede ser mayor que la memoria física real. Este espacio de direcciones virtual se respalda en el disco, y solo las porciones necesarias se traen a la memoria principal (real) cuando se necesitan.
Beneficios Clave:
Ejecución de programas grandes: Los programas pueden ser mucho más grandes que la memoria principal física, liberando al programador de las restricciones de tamaño y de la necesidad de técnicas manuales como los "overlays".
Mayor multiprogramación: Al cargar solo partes de los procesos en la memoria, hay espacio para más procesos, lo que incrementa la probabilidad de que la CPU tenga siempre trabajo listo.
Aislamiento de procesos: Cada proceso tiene su propio espacio de direcciones virtual, evitando que interfieran entre sí.
Compartición de memoria: Permite compartir de forma controlada porciones de memoria entre procesos (por ejemplo, bibliotecas compartidas).
Mecanismos Fundamentales:
Paginación: La técnica más común para la memoria virtual. Divide el espacio de direcciones virtual de un proceso en bloques de tamaño fijo llamados páginas, y la memoria física en bloques del mismo tamaño llamados marcos de página.
Tabla de páginas: Una estructura de datos mantenida por el sistema operativo para cada proceso, que mapea las páginas virtuales a los marcos de página físicos.
Unidad de Gestión de Memoria (MMU): Un componente de hardware que realiza la traducción dinámica de direcciones virtuales a físicas durante la ejecución.
Fallo de página (Page Fault): Una interrupción que ocurre cuando una página referenciada no se encuentra en la memoria principal. El sistema operativo interviene para cargar la página desde el disco.
Segmentación: Divide un programa en segmentos lógicos de tamaño variable. Ofrece beneficios para la organización modular, el manejo de estructuras de datos que pueden crecer dinámicamente y la protección.
Paginación y Segmentación Combinadas: Algunos sistemas, como MULTICS y el Intel Pentium, combinan ambas técnicas para aprovechar sus respectivas ventajas. Por ejemplo, en MULTICS, cada segmento se considera una memoria virtual y se pagina.
Desafíos y Políticas del SO:
Thrashing (Trasiego): Una condición de bajo rendimiento donde el sistema gasta más tiempo intercambiando páginas entre disco y memoria que ejecutando instrucciones. El SO debe implementar algoritmos efectivos para evitarlo.
Política de reemplazo: Cuando la memoria está llena y se necesita traer una nueva página, el sistema operativo debe decidir qué página residente reemplazar.
Gestión del conjunto residente: Determinar cuánta memoria principal asignar a un proceso en particular (asignación fija o variable).
Localidad de referencia: El principio de que los programas tienden a acceder a un subconjunto limitado de su espacio de direcciones en periodos cortos, lo que hace que la memoria virtual funcione eficientemente.
Estructura General del Sistema Operativo
Estos conceptos no operan de forma aislada, sino que se integran en la estructura general del sistema operativo, que debe lograr objetivos a menudo conflictivos como la conveniencia, la eficiencia y la capacidad de evolución.
Abstracciones Fundamentales: Los procesos (y más recientemente los hilos), los espacios de direcciones y los archivos son las abstracciones clave que los sistemas operativos proporcionan para simplificar la interacción con el hardware subyacente.
El Proceso como Unidad Central: El proceso es la unidad fundamental para la gestión de recursos y la planificación en los SO modernos. Un proceso incluye el programa ejecutable, los datos asociados, el contexto de ejecución y los recursos del sistema.
Hilos (Threads): El concepto de proceso se ha refinado con los hilos, que separan la propiedad de recursos (proceso) de la unidad de ejecución (hilo). Múltiples hilos pueden existir dentro de un solo proceso, compartiendo su espacio de direcciones y recursos, pero siendo planificados de forma independiente. Esto mejora la eficiencia y la modularidad de las aplicaciones.
Estructuras de Control: El SO mantiene tablas y estructuras de datos complejas (como los bloques de control de proceso o BCP, y tablas de memoria, E/S y ficheros) para supervisar y controlar el estado de los procesos y la asignación de recursos.
Arquitecturas del SO: La implementación de estas funciones varía según la arquitectura del sistema operativo:
Sistemas monolíticos: Gran parte del sistema operativo se ejecuta en modo núcleo como un solo programa, compartiendo el mismo espacio de direcciones.
Sistemas de capas: Organizan las funciones del sistema operativo jerárquicamente, con interacciones solo entre capas adyacentes.
Micronúcleos (Microkernels): Asignan solo las funciones esenciales (gestión de espacios de direcciones, comunicación entre procesos y planificación básica) al núcleo. Otros servicios del sistema operativo se ejecutan en modo usuario como "servidores". Esto promueve la modularidad, flexibilidad y soporte a sistemas distribuidos.
Máquinas virtuales: Proporcionan una copia exacta del hardware subyacente, permitiendo que múltiples sistemas operativos invitados se ejecuten simultáneamente en un único hardware físico bajo un hipervisor.
Clusters: Grupos de computadoras completas interconectadas que trabajan como un recurso unificado para proporcionar escalabilidad, alta disponibilidad y rendimiento.
En definitiva, la estructura del sistema operativo se construye en torno a estas abstracciones y mecanismos, permitiendo una gestión eficiente y segura del hardware, al tiempo que proporciona un entorno de computación potente y flexible para las aplicaciones y los usuarios.
Operaciones del Sistema Operativo
Las operaciones del sistema operativo (SO) son funciones fundamentales que permiten al computador funcionar de manera eficiente, segura y conveniente, actuando como intermediario entre el hardware y los programas de usuario. Estas operaciones son esenciales para la multiprogramación y la multitarea, ya que permiten el control y la asignación de recursos. Para comprenderlas en profundidad, es crucial analizar las interrupciones, los modos de operación (usuario/supervisor) y el control de errores.
1. Modos de Operación (Modo Usuario/Supervisor o Núcleo)
Los sistemas operativos modernos, como Windows, Linux y UNIX, operan en al menos dos modos de ejecución para garantizar la protección y la seguridad del sistema.
Modo Núcleo (o Modo Sistema/Supervisor):


El sistema operativo es la pieza fundamental del software y se ejecuta en este modo.
Tiene acceso completo a todo el hardware y puede ejecutar cualquier instrucción que la máquina sea capaz de realizar.
Ciertas instrucciones a nivel de máquina, denominadas privilegiadas, solo pueden ser ejecutadas por el monitor o el núcleo. Estas incluyen instrucciones de E/S, acceso a registros de control, modificación de la palabra de estado del programa (PSW) e instrucciones relacionadas con la gestión de memoria.
También tiene acceso a regiones de memoria protegidas.
El núcleo contiene las porciones de software utilizadas más frecuentemente y generalmente se mantiene permanentemente en memoria principal.
Los componentes en modo núcleo de Windows incluyen el sistema ejecutivo, el núcleo (kernel), los controladores de dispositivo y los sistemas gráficos. En Linux, el kernel también controla el acceso de la CPU a los dispositivos y a la unidad de administración de memoria.
Modo Usuario:


El resto del software, incluyendo los programas de aplicación y las utilidades, se ejecuta en este modo.
En este modo, solo se permite un subconjunto de las instrucciones de máquina. Las instrucciones que afectan el control de la máquina o que se encargan de la E/S están prohibidas.
El acceso a ciertas áreas de memoria está restringido.
Si un programa de usuario intenta realizar una operación privilegiada o acceder a un área de memoria restringida, se produce un error que transfiere el control al monitor o núcleo.
Aunque el shell o la interfaz gráfica de usuario (GUI) son los programas con los que los usuarios suelen interactuar, no forman parte del sistema operativo en sí, sino que lo utilizan para llevar a cabo su trabajo.
La distinción entre modos es crucial para proteger el sistema operativo y las tablas clave del sistema (como los bloques de control de proceso) de la interferencia de los programas de usuario. Un cambio de modo puede ocurrir sin que cambie el estado del proceso en ejecución, lo que implica una sobrecarga menor.
2. Interrupciones
Las interrupciones son un mecanismo fundamental que permite a otros módulos (memoria y E/S) interrumpir el secuenciamiento normal del procesador. Su propósito principal es mejorar la utilización del procesador y permitir que el sistema operativo responda a eventos asíncronos o errores.
Existen dos categorías principales:
a) Interrupciones de Hardware (Interrupciones "ordinarias")
Son eventos externos e independientes del proceso que se está ejecutando.
Clases de interrupciones de hardware:


De programa: Generadas por una condición que resulta de la ejecución de una instrucción, como un desbordamiento aritmético, división por cero, intento de ejecutar una instrucción ilegal, o referencias fuera del espacio de memoria permitido. Estas a menudo se consideran traps por otros autores o en un contexto más específico.
Por temporizador: Generadas por un temporizador del procesador, permiten al sistema operativo realizar funciones de forma regular, como la planificación de procesos.
De E/S: Generadas por un controlador de E/S para señalar la conclusión normal de una operación (ej., la impresora ha terminado de imprimir) o para indicar condiciones de error. Permiten al procesador dedicarse a otras tareas mientras el dispositivo de E/S opera.
Procesamiento de interrupciones de hardware:


El dispositivo genera una señal de interrupción al procesador.
El procesador termina la ejecución de la instrucción actual.
El procesador comprueba si hay una petición de interrupción pendiente y envía una señal de reconocimiento al dispositivo.
El hardware guarda el contexto del programa actual (contador de programa, PSW y otros registros) en la pila.
El procesador carga el contador de programa con la dirección del manejador de interrupción (una rutina del sistema operativo) que responderá a esa interrupción específica.
El procesador cambia a modo núcleo para que el manejador pueda ejecutar instrucciones privilegiadas.
El manejador de interrupción determina la naturaleza de la interrupción, procesa el evento y realiza las acciones requeridas.
Finalmente, se restaura el contexto del programa interrumpido (o de otro proceso de mayor prioridad) desde la pila, permitiendo que la ejecución se reanude.
Manejo de múltiples interrupciones:


Se pueden definir prioridades para las interrupciones, permitiendo que una interrupción de mayor prioridad interrumpa la ejecución de un manejador de una interrupción de menor prioridad.
Las CPUs modernas con canalización (pipelining) o superescalares pueden presentar el problema de interrupciones imprecisas, donde el estado del programa al momento de la interrupción no está claramente definido, lo que complica la labor del sistema operativo para restaurar el estado correctamente. Las interrupciones precisas son deseables para simplificar el diseño del SO.
Acceso Directo a Memoria (DMA):


La E/S dirigida por interrupciones aún requiere que cada palabra de datos pase por el procesador.
DMA es un chip especial que puede controlar el flujo de bits entre la memoria y un dispositivo controlador sin la intervención constante de la CPU. La CPU configura el chip DMA y lo deja hacer su trabajo, generando una interrupción solo cuando se ha transferido un bloque completo de datos. Esto reduce drásticamente la carga de la CPU en operaciones de E/S grandes.
b) Interrupciones de Software (Traps y Llamadas al Sistema)
Son eventos que ocurren sincrónicamente en relación con la ejecución del programa.
Traps (Excepciones/Fallos):


Son interrupciones generadas por una condición de error o excepción dentro del proceso que está ejecutando. Ejemplos incluyen división por cero, intento de acceso a memoria prohibida (fallo de segmentación), o un fallo de memoria (page fault) cuando una dirección virtual referenciada no está en memoria principal.
Cuando ocurre un trap, el control se transfiere inicialmente al manejador de trap en modo núcleo, similar a una interrupción de hardware.
En el caso de un fallo de memoria, el sistema operativo interviene para cargar el bloque (página o segmento) necesario desde la memoria secundaria (disco) a la principal, bloqueando el proceso que lo causó y permitiendo que otro se ejecute mientras tanto.
Llamadas al Sistema (System Calls):


Son solicitudes explícitas que un programa de usuario hace al sistema operativo para que realice un servicio.
Un programa de usuario ejecuta una instrucción especial (una "trap" o instrucción de cambio de modo) que transfiere el control al núcleo.
Las llamadas al sistema son la interfaz principal entre los programas de usuario y el sistema operativo para acceder a sus abstracciones (ej., crear/eliminar/leer/escribir archivos, crear/terminar procesos, operaciones de E/S) y servicios.
La ejecución de una llamada al sistema a menudo implica que el proceso que la realiza pase a un estado bloqueado si el servicio solicitado requiere esperar por un recurso (ej., E/S).
3. Control de Errores
El sistema operativo tiene una responsabilidad crítica en la detección y respuesta a errores para garantizar la estabilidad, la disponibilidad y la integridad del sistema.
Objetivos del control de errores:


Eliminar la condición de error.
Minimizar el impacto en las aplicaciones en ejecución.
Mantener la disponibilidad, confidencialidad, integridad y autenticidad de los datos y recursos.
Tipos de errores y respuestas del SO:


Errores de hardware: Fallos de memoria o dispositivos. El SO puede intentar reintentar operaciones, reestructurar tablas de gestión si un procesador falla (en SMP), o informar del error.
Errores de software/aplicación: División por cero, intento de acceso a memoria prohibida, instrucción ilegal, incapacidad del SO para conceder una solicitud. La respuesta puede ser finalizar el programa, reintentar la operación o informar del error. Los traps son el mecanismo hardware/software para detectar muchos de estos.
Fallas de página (Page Faults): Cuando un proceso intenta acceder a una página de memoria virtual que no está presente en la memoria física. El SO suspende el proceso, carga la página desde el disco y luego reanuda el proceso.
Interbloqueos (Deadlocks): Bloqueo permanente de un conjunto de procesos que compiten por recursos. El SO debe implementar estrategias de prevención, detección y predicción para evitar o resolver interbloqueos. Esto a menudo implica la gestión de recursos compartidos (como impresoras o archivos bloqueados).
Amenazas de seguridad: Interrupciones (ataques a la disponibilidad), intercepciones (ataques a la confidencialidad), modificación no autorizada o fabricación (ataques a la integridad y autenticidad). Esto incluye software malicioso como virus y gusanos. Los mecanismos de protección incluyen la protección de memoria (aislamiento de procesos), control de acceso a archivos y recursos, y autenticación de usuarios.
Errores de E/S: El software independiente del dispositivo en el SO maneja muchos errores específicos del dispositivo y proporciona un marco general para el reporte de errores. Los controladores de disco pueden reasignar sectores defectuosos o incluso restablecerse si fallan.
Operación de fallo suave (Soft Failures): En sistemas de tiempo real, la habilidad del sistema de fallar de tal manera que se preserve tanta capacidad y datos como sea posible, intentando corregir el problema o minimizar sus efectos mientras continúa en ejecución.
En resumen, las operaciones del sistema operativo giran en torno a la gestión inteligente de eventos y recursos. Mediante la diferenciación de modos de ejecución, el manejo sofisticado de interrupciones (hardware y software) y una robusta arquitectura de control de errores y protección, el SO transforma un hardware complejo en una plataforma de computación funcional y segura.
Administración de Procesos
La administración de procesos es una de las tareas fundamentales y más complejas de cualquier sistema operativo (SO) moderno. Su objetivo principal es la gestión de la ejecución concurrente de programas, asignando recursos y coordinando las actividades de los procesos para maximizar la utilización del procesador y proporcionar tiempos de respuesta razonables.
Un proceso es la unidad fundamental de ejecución en un sistema operativo, a menudo definido como "un programa en ejecución". Representa una actividad con un programa, una entrada, una salida y un estado. Cada proceso posee un espacio de direcciones virtuales que contiene el código del programa, sus datos y la pila. También se le asocia un contexto de ejecución (o estado del proceso), que incluye los valores de los registros del procesador (como el contador de programa y los registros de datos), la palabra de estado del programa (PSW), su prioridad, información de E/S, y otros datos que el SO necesita para supervisar y controlar el proceso. Toda esta información se guarda en una estructura de datos llamada Bloque de Control de Proceso (BCP).
Los sistemas operativos que implementan la multiprogramación o la multitarea mantienen múltiples programas en memoria principal simultáneamente, permitiendo que la CPU conmute entre ellos para mejorar la eficiencia. Esto da la apariencia de que varios programas se ejecutan en paralelo, incluso en un solo procesador.
A continuación, se detallan las operaciones clave en la administración de procesos:
1. Creación de Procesos
Los sistemas operativos necesitan mecanismos para crear procesos según sea necesario. Hay cuatro eventos principales que pueden iniciar la creación de un proceso:
Arranque del sistema: Al iniciar el sistema operativo, se crean varios procesos, incluyendo procesos en primer plano (interactivos) y procesos en segundo plano (para tareas específicas del sistema, como la gestión del correo electrónico entrante).
Llamada al sistema para creación de procesos: Un proceso existente (llamado proceso padre) ejecuta una llamada al sistema para crear uno o más procesos hijos. Esta es la forma principal en UNIX/Linux (usando fork).
Petición de usuario: Un usuario solicita la creación de un proceso (ej. iniciando una aplicación).
Inicio de un trabajo por lotes: En sistemas de procesamiento por lotes, el SO crea un proceso para ejecutar el siguiente trabajo de la cola de entrada.
Los pasos generales que el sistema operativo realiza al crear un proceso son:
Asignar un identificador de proceso único.
Reservar espacio para el proceso: Esto incluye el espacio de direcciones virtual para el programa, datos y la pila de usuario, así como para el Bloque de Control de Proceso (BCP).
Inicializar el BCP: Se establecen el estado inicial (ej. Nuevo o Listo), la prioridad y otra información de control.
Establecer enlaces apropiados: Si el sistema mantiene colas de planificador, el nuevo proceso se añade a la cola de Listo o Listo/Suspendido.
Crear o expandir otras estructuras de datos: Por ejemplo, registros de auditoría.
Un proceso hijo es una copia casi exacta del padre al momento de la creación, con su propio espacio de direcciones y recursos (aunque el código puede compartirse para optimización).
2. Destrucción de Procesos
La terminación de un proceso ocurre cuando su ejecución finaliza, ya sea normalmente o debido a un error. Las razones típicas para la terminación de un proceso incluyen:
Finalización normal: El proceso indica que ha completado su tarea (ej. instrucción HALT, salida de aplicación).
Error fatal: Una condición de error irrecuperable (ej. división por cero, acceso a memoria inválida).
Abandono voluntario: El proceso se termina a sí mismo.
Terminación por otro proceso: Un proceso padre puede terminar a un proceso hijo.
Terminación por el SO: El sistema operativo puede terminar procesos por razones como interbloqueo, falta de recursos o abusos de privilegios.
Cuando un proceso termina, se mueve al estado Saliente. El sistema operativo preserva temporalmente la información asociada para que programas de soporte puedan extraer datos (ej. para contabilidad o análisis de rendimiento). Finalmente, el proceso se borra del sistema. Si un proceso termina, todos sus hilos también lo hacen.
3. Suspensión de Procesos
La suspensión de un proceso implica que un proceso no está disponible para su ejecución inmediata, independientemente de si está esperando un evento o no. El proceso es "expulsado" de la memoria principal y transferido a un almacenamiento secundario (disco), liberando espacio en RAM.
Las características de un proceso suspendido incluyen:
No está inmediatamente disponible para su ejecución.
Puede estar esperando un evento o no; si el evento ocurre, no se ejecuta inmediatamente.
Fue puesto en estado suspendido por un agente (el proceso, el padre o el SO) con el propósito de prevenir su ejecución.
No puede ser reactivado hasta que el agente lo indique explícitamente.
Razones para la suspensión:
Swapping (intercambio): Si no hay procesos listos para ejecutar y la memoria principal está llena, el SO puede suspender procesos bloqueados para liberar espacio y cargar otros procesos.
Petición del padre: Un proceso padre puede suspender un hijo para examinarlo, modificarlo o coordinar sus actividades.
Propósito del SO: Para la monitorización, auditoría, detección de problemas (como interbloqueos) o por fallo de dispositivos.
Temporización: Un proceso periódico puede suspenderse mientras espera su próximo intervalo de ejecución.
El modelo de procesos de cinco estados incluye Listo/Suspendido y Bloqueado/Suspendido para reflejar si un proceso está en disco esperando un evento o listo para cargar en memoria. La suspensión es un mecanismo crucial para la gestión de memoria y el grado de multiprogramación.
4. Sincronización de Procesos
La concurrencia (procesos ejecutándose de forma intercalada o simultánea) introduce problemas de temporización y sincronización. La sincronización permite que múltiples procesos coordinen sus actividades para intercambiar información o acceder a recursos compartidos de forma ordenada.
Los principales problemas de la concurrencia incluyen:
Condiciones de carrera (Race Conditions): Suceden cuando múltiples procesos leen y escriben datos compartidos, y el resultado final depende del orden no predecible de su ejecución.
Sección crítica (Critical Section): Una porción de código que requiere acceso a recursos compartidos y que no puede ser ejecutada por más de un proceso a la vez.
Exclusión mutua (Mutual Exclusion): El requisito fundamental de que, si un proceso está utilizando un recurso compartido (es decir, en su sección crítica), ningún otro proceso pueda acceder al mismo recurso crítico al mismo tiempo.
Para asegurar la exclusión mutua y la sincronización, se utilizan diversos mecanismos:
a) Soporte Hardware
El hardware proporciona instrucciones especiales que permiten realizar dos acciones atómicamente (indivisiblemente) sobre una única posición de memoria, como leer y escribir, o leer y comprobar. Esto es vital para evitar condiciones de carrera. Un ejemplo es la instrucción TSL (Test and Set Lock), que copia el valor de un "candado" a un registro y luego fija el candado a 1, permitiendo que un proceso entre en su sección crítica si el candado estaba a 0.
b) Semáforos
Inventados por Dijkstra, los semáforos son variables especiales que se utilizan para la señalización entre procesos. Tienen dos operaciones atómicas:
semWait(s) (o p(s), down(s)): Decrementa el valor del semáforo. Si el valor resultante es negativo (o cero, dependiendo de la implementación), el proceso que llamó se bloquea.
semSignal(s) (o v(s), up(s)): Incrementa el valor del semáforo. Si hay procesos esperando en la cola del semáforo, uno de ellos es desbloqueado.
Existen semáforos binarios (que solo toman valores 0 o 1, también llamados mutexes) y semáforos de conteo (que pueden tomar cualquier valor no negativo). Los semáforos se utilizan para implementar exclusión mutua y sincronización.
c) Monitores
Un monitor es una construcción de lenguaje de programación que encapsula datos compartidos y los procedimientos que los manipulan, garantizando que solo un proceso a la vez pueda ejecutar un procedimiento dentro del monitor. Utiliza variables de condición con operaciones wait y signal para la sincronización condicional.
d) Paso de Mensajes (Message Passing)
El paso de mensajes es un método que proporciona tanto sincronización como comunicación entre procesos, siendo especialmente útil en sistemas distribuidos y multiprocesadores sin memoria compartida.
Primitivas: Se basa en las operaciones send(destino, mensaje) y receive(origen, mensaje).
Sincronización: Las llamadas pueden ser bloqueantes (síncronas), donde el remitente o receptor se suspende hasta que la operación se completa, o no bloqueantes (asíncronas), donde el control regresa de inmediato y la notificación se produce después (ej. por interrupción). Las llamadas bloqueantes son más simples de programar, pero las no bloqueantes ofrecen más paralelismo.
Direccionamiento: Puede ser directo (especificando el ID del proceso) o indirecto (utilizando una estructura de datos intermedia como un buzón o puerto). Los buzones permiten almacenar mensajes en cola, desacoplando el remitente y el receptor.
5. Comunicación entre Procesos (IPC)
La IPC permite a los procesos intercambiar información y coordinar sus actividades. El paso de mensajes es un mecanismo fundamental para la IPC, especialmente cuando los procesos no comparten memoria.
Otros mecanismos, no detallados como IPC explícita pero implícitamente usados en la coordinación, incluyen:
Memoria Compartida: Procesos que comparten el mismo espacio de direcciones (como los hilos) o mapean páginas en sus espacios para acceder a datos comunes. Los hilos en un mismo proceso comparten memoria y archivos, pudiendo comunicarse sin invocar al núcleo.
Llamadas a Procedimiento Remoto (RPC): En sistemas distribuidos, una RPC permite a un proceso invocar un procedimiento en otro proceso como si fuera una llamada local, ocultando la complejidad de la comunicación de red.
6. Interbloqueo (Deadlock)
Un interbloqueo es el bloqueo permanente de un conjunto de procesos, donde cada proceso en el conjunto está esperando un evento (comúnmente la liberación de un recurso) que solo puede ser causado por otro proceso del mismo conjunto. Esto significa que ningún proceso puede avanzar.
Para que ocurra un interbloqueo de recursos, deben cumplirse cuatro condiciones necesarias simultáneamente:
Exclusión mutua: Solo un proceso puede utilizar un recurso en un momento dado.
Retención y espera (Hold and Wait): Un proceso puede mantener recursos asignados mientras espera la asignación de otros recursos.
Sin expropiación (No Preemption): Los recursos asignados no pueden ser quitados por la fuerza a un proceso que los posee; deben ser liberados explícitamente.
Espera circular (Circular Wait): Debe existir una cadena cerrada de dos o más procesos, donde cada proceso espera un recurso que está siendo retenido por el siguiente proceso en la cadena.
Una herramienta útil para modelar la asignación de recursos y detectar interbloqueos es el grafo de asignación de recursos.
Las estrategias para manejar los interbloqueos son:
a) Prevención del Interbloqueo (Deadlock Prevention)
Consiste en diseñar el sistema para asegurar que al menos una de las cuatro condiciones necesarias para el interbloqueo nunca se cumpla.
Negar la exclusión mutua: No siempre es posible, ya que muchos recursos son inherentemente no compartibles.
Negar la retención y espera: Se requiere que un proceso solicite todos sus recursos al mismo tiempo, bloqueándose hasta que todos puedan ser concedidos. Esto puede ser ineficiente.
Negar la no expropiación: Permitir que el sistema operativo expropie recursos de un proceso. Esto es complejo y puede ser costoso.
Negar la espera circular: Imponer un ordenamiento lineal de los recursos y exigir que los procesos soliciten los recursos en orden creciente.
b) Predicción del Interbloqueo (Deadlock Avoidance)
Implica analizar cada nueva petición de un recurso para determinar si su concesión podría conducir a un interbloqueo, concediéndola solo si no es posible.
Algoritmo del Banquero: Un algoritmo clásico que utiliza la información sobre los requisitos máximos de recursos de cada proceso para determinar si un estado es "seguro" (existe una secuencia de ejecución que permite a todos los procesos terminar sin interbloqueo) o "inseguro". Si una solicitud lleva a un estado inseguro, se deniega temporalmente.
c) Detección del Interbloqueo y Recuperación (Deadlock Detection and Recovery)
El sistema operativo permite que los interbloqueos ocurran, pero periódicamente ejecuta un algoritmo para detectarlos y luego toma acciones correctivas.
Detección: Se construye un grafo de asignación de recursos y se buscan ciclos.
Recuperación: Las estrategias incluyen:
Abortar todos los procesos involucrados en el interbloqueo: La solución más común.
Retroceder procesos a un punto de control y reiniciarlos: Requiere mecanismos de checkpointing y rollback.
Abortar procesos uno por uno hasta que el interbloqueo se rompa.
Problemas relacionados con el interbloqueo:
Inanición (Starvation): Un proceso listo para ejecutarse es denegado continuamente el acceso al procesador o a un recurso, en deferencia a otros procesos, sin que haya un interbloqueo formal. Se puede evitar con políticas de asignación "primero en llegar, primero en ser atendido".
Bloqueo activo (Livelock): Una situación en la que dos o más procesos cambian continuamente su estado en respuesta a cambios en otros procesos, sin realizar ningún trabajo útil. A diferencia del interbloqueo, ningún proceso está técnicamente "bloqueado", pero tampoco progresa.
En un sistema distribuido, el manejo del interbloqueo y la exclusión mutua es más complejo porque ningún nodo tiene un conocimiento preciso del estado global del sistema, y la comunicación entre procesos implica retardos impredecibles. Pueden ocurrir fenómenos como el interbloqueo fantasma, donde se detecta un interbloqueo que no es real debido a información desactualizada.
La administración de procesos es, por tanto, un área central del diseño de sistemas operativos, abarcando desde la gestión del ciclo de vida de una aplicación hasta la coordinación compleja de sus interacciones y la resolución de conflictos por recursos.
Administración de Memoria
Claro, profundicemos en el concepto de Administración de Memoria, que es una de las tareas más complejas y cruciales para cualquier sistema operativo moderno.
Administración de Memoria: Una Introducción
La administración de memoria es la función del sistema operativo que se encarga de gestionar la jerarquía de memoria de la computadora de manera eficiente. Su propósito es administrar la memoria de manera efectiva, llevando un registro de qué partes de la memoria están en uso, asignando memoria a los procesos cuando la necesitan y liberándola cuando terminan. En un sistema multiprogramado, la memoria principal debe subdividirse para acomodar múltiples procesos. Esta tarea dinámica es responsabilidad del sistema operativo y se conoce como gestión de memoria.
Una gestión de memoria efectiva es vital en un sistema multiprogramado, ya que si solo unos pocos procesos residen en memoria, el procesador podría estar ocioso esperando operaciones de E/S. Por lo tanto, es necesario asignar memoria para asegurar que haya una cantidad suficiente de procesos listos que puedan consumir el tiempo de procesador disponible.
Los objetivos de la administración de memoria son:
Reubicación: Permitir que los procesos se carguen y muevan en la memoria principal de forma flexible.
Protección: Aislar los procesos para que no interfieran con la memoria de otros, incluyendo datos e instrucciones, y proteger el sistema operativo.
Compartición: Permitir el acceso controlado a áreas de memoria compartidas entre procesos.
Organización Lógica: Permitir que los programas y datos se organicen de forma modular, con diferentes grados de protección y la posibilidad de compartir módulos.
Organización Física: Gestionar el movimiento de información entre los distintos niveles de la jerarquía de memoria (principal y secundaria).
Ejecución de programas más grandes: Permite ejecutar programas más grandes que la memoria física real.
Multiprogamación más efectiva: Aumenta el grado de multiprogramación.
1. Jerarquía de Memoria
Las computadoras modernas utilizan una jerarquía de memoria que organiza los diferentes tipos de almacenamiento según su velocidad, costo y capacidad. Esta jerarquía explota el principio de la proximidad de referencia, que establece que las referencias a memoria tienden a agruparse en periodos cortos.
Los niveles típicos de esta jerarquía, de más rápido y caro a más lento y económico, son:
Registros del Procesador: La memoria más rápida, pequeña y costosa, interna al procesador.
Memoria Caché (Cache): Una memoria pequeña y muy rápida que actúa como un búfer entre el procesador y la memoria principal, guardando copias de partes de la memoria principal que se utilizan con mayor frecuencia. No es visible para el programador.
Memoria Principal (RAM): El sistema de memoria interna fundamental del computador. Proporciona acceso rápido a un costo relativamente alto y es volátil (no almacena permanentemente). Cada posición tiene una dirección única.
Memoria Secundaria (Almacenamiento Secundario): Más lenta y barata que la memoria principal, y usualmente no volátil (proporciona almacenamiento permanente). Se utiliza para el almacenamiento a largo plazo de programas y datos, y para extender la memoria principal mediante la memoria virtual. Ejemplos incluyen discos magnéticos, CD-ROM, DVD-RW y cintas magnéticas.
Almacenamiento Fuera de Línea: Dispositivos como cintas magnéticas y CD-RW para almacenamiento que no está constantemente conectado al sistema.
La clave del éxito de esta jerarquía es la disminución de la frecuencia de acceso a los niveles inferiores.
2. Memoria Caché
La memoria caché es un componente esencial en la jerarquía de memoria diseñado para superar la discrepancia de velocidad entre el procesador y la memoria principal.
Propósito: Proporcionar un tiempo de acceso a memoria cercano al de las memorias más rápidas disponibles, mientras se ofrece un tamaño de memoria grande al precio de las memorias semiconductoras menos costosas. La caché contiene una copia de una parte de la memoria principal.
Funcionamiento: Cuando el procesador intenta leer un byte de memoria, primero se comprueba si el byte está en la caché.
Si está (un "acierto"), el byte se entrega directamente al procesador.
Si no está (un "fallo"), un bloque de memoria principal (de un tamaño fijo, K palabras) se lee y se introduce en la caché, y luego se entrega el byte solicitado al procesador.
Diseño: Los elementos fundamentales de diseño de la caché incluyen su tamaño, el tamaño del bloque (unidad de datos intercambiada entre caché y memoria principal), la función de correspondencia, el algoritmo de reemplazo y la política de escritura.
Cache de Disco: El mismo principio de la caché se puede aplicar a la memoria del disco. Una caché de disco es un búfer en memoria principal para almacenar sectores del disco, mejorando el rendimiento de E/S al reducir el número de accesos al disco. Windows incluye un Gestor de caché que proporciona este servicio.
3. Memoria Principal y Memoria Secundaria
Estos son los dos niveles principales de memoria que el sistema operativo administra directamente para los programas.
Memoria Principal (Main Memory / RAM):


Es la memoria directamente direccionable por el procesador.
Proporciona acceso rápido, pero a un coste relativamente alto.
Es volátil: no retiene su contenido cuando se desconecta la energía.
Contiene el sistema operativo (núcleo) y los programas y datos que están actualmente en uso.
Se organiza como un espacio de almacenamiento lineal de bytes o palabras.
Su gestión es clave para la multiprogramación, ya que determina cuántos procesos pueden residir simultáneamente.
Memoria Secundaria (Secondary Memory / Almacenamiento Auxiliar):


Almacenamiento externo al computador, al que el procesador no puede acceder directamente; la información debe copiarse primero a la memoria principal.
Es más lenta y más barata que la memoria principal.
Usualmente no es volátil: proporciona almacenamiento permanente a largo plazo para programas y datos.
El disco es el dispositivo de memoria secundaria más común.
También se utiliza para extender la memoria principal mediante la memoria virtual.
La tarea de mover información entre estos dos niveles es la esencia de la gestión de memoria.
4. Paginación
La paginación es una de las técnicas más sofisticadas y comunes para la gestión de memoria y la implementación de la memoria virtual.
Concepto: Divide la memoria principal en porciones de tamaño fijo relativamente pequeñas llamadas marcos de página (frames). Cada proceso también se divide en porciones pequeñas del mismo tamaño fijo llamadas páginas.
Objetivo: Abordar la ineficiencia del particionamiento (fragmentación interna y externa) y permitir que los procesos se carguen en memoria principal sin necesidad de que sus partes estén contiguas. No existe fragmentación externa, y la fragmentación interna se limita a una fracción de la última página.
Direccionamiento y Traducción:
Un programa hace referencia a la memoria utilizando una dirección lógica (o virtual), que se compone de un número de página virtual y un desplazamiento (offset) dentro de esa página.
El hardware del procesador (específicamente la Unidad de Gestión de Memoria - MMU) realiza la traducción dinámica de estas direcciones lógicas a direcciones físicas (número de marco de página + desplazamiento) durante la ejecución.
Para cada proceso, el sistema operativo mantiene una tabla de páginas, que mapea cada página virtual a un marco de página físico en la memoria principal.
Una entrada en la tabla de páginas típicamente incluye el número de marco de página, un bit de "presente/ausente" (P) que indica si la página está en memoria principal, un bit de "modificado" (M) o "sucio" que indica si la página ha sido alterada, y bits de protección.
Fallos de Página (Page Faults): Si una referencia a una dirección lógica encuentra que la página correspondiente no está en memoria principal (bit P = 0), se produce una interrupción conocida como fallo de página. El sistema operativo suspende el proceso y carga la página necesaria desde el disco.
Aceleración: Para acelerar la traducción de direcciones, las computadoras utilizan un Buffer de Traducción Adelantada (TLB - Translation Lookaside Buffer), una caché especial de alta velocidad para las entradas de la tabla de páginas usadas recientemente.
Tablas de páginas multinivel: Para espacios de direcciones muy grandes, las tablas de páginas pueden volverse enormes. Se utilizan estructuras multinivel (ej., directorio de páginas que apunta a tablas de páginas, como en Linux o Pentium) para reducir el espacio ocupado por las tablas de páginas y evitar mantenerlas todas en memoria simultáneamente.
Tablas de páginas invertidas: Una alternativa donde hay una entrada por cada marco de página real en lugar de una por cada página virtual. Su tamaño es fijo y proporcional a la memoria real.
5. Segmentación
La segmentación es otra técnica de gestión de memoria que organiza la memoria de forma lógica, siendo visible para el programador.
Concepto: Divide el programa y sus datos en un número de segmentos. Los segmentos no requieren ser de la misma longitud y pueden variar dinámicamente.
Direccionamiento: Una dirección lógica se compone de un número de segmento y un desplazamiento dentro de ese segmento.
Tabla de Segmentos: Cada proceso tiene una tabla de segmentos que almacena la dirección física inicial y la longitud de cada segmento. Esta tabla también contiene un bit de "presente" y un bit de "modificado" para la gestión de memoria virtual.
Ventajas:
Organización Lógica: Se corresponde mejor con la estructura modular de los programas (código, datos, pila pueden ser segmentos separados).
Manejo de estructuras de datos que crecen: Facilita la gestión de estructuras de datos que pueden cambiar de tamaño dinámicamente.
Protección y Compartición: Permite aplicar diferentes grados de protección a diferentes módulos (lectura, escritura, ejecución) y facilita la compartición de código o datos entre procesos.
Vinculación Simplificada: Simplifica la vinculación de procedimientos compilados por separado.
Desventajas: Al ser los segmentos de longitud variable, puede producirse fragmentación externa, similar al particionamiento dinámico.
6. Memoria Virtual
La memoria virtual es una técnica casi universal en la gestión de memoria de los sistemas contemporáneos.
Concepto: Permite que los programas direccionen la memoria desde un punto de vista lógico, sin importar la cantidad de memoria principal física disponible. Crea la ilusión de un espacio de direcciones grande e ilimitado para cada proceso, que puede ser mucho mayor que la memoria física real.
Funcionamiento: El espacio de direcciones virtual de un proceso se almacena en disco (memoria secundaria), y solo las porciones necesarias (páginas o segmentos) se traen a la memoria principal cuando se necesitan (paginación bajo demanda).
Beneficios:
Ejecución de programas grandes: Permite ejecutar programas más grandes que la memoria física.
Mayor grado de multiprogramación: Al cargar solo partes de los procesos en memoria, se pueden mantener más procesos concurrentemente, aumentando la utilización de la CPU.
Abstracción de memoria física: El programador no necesita preocuparse por la cantidad de memoria física disponible ni por la reubicación del programa.
Protección y Aislamiento: Cada proceso tiene su propio espacio de direcciones virtual, lo que aísla los procesos entre sí.
Implementación: La memoria virtual se implementa principalmente utilizando paginación o una combinación de paginación y segmentación.
Control de Carga: El sistema operativo utiliza el control de carga para determinar el número de procesos que van a residir en la memoria principal, gestionando así el grado de multiprogramación para optimizar el rendimiento y evitar el thrashing (una condición donde el sistema dedica demasiado tiempo al intercambio de páginas y poco a la ejecución de trabajo útil).
Políticas del SO: El diseño del software del SO para memoria virtual implica elegir políticas de recuperación (cuándo traer una página, ej., paginación bajo demanda o adelantada), ubicación (dónde colocarla en memoria real, irrelevante en paginación pura o combinada), reemplazo (qué página desalojar cuando la memoria está llena) y gestión del conjunto residente (cuántos marcos de página asignar a un proceso).
Combinación de Paginación y Segmentación: Algunos sistemas, como MULTICS y el Intel Pentium, combinan ambas técnicas para obtener sus ventajas. El espacio de direcciones del usuario se divide en segmentos, y cada segmento, a su vez, se divide en páginas de tamaño fijo. Esto proporciona una memoria virtual bidimensional, combinando la organización lógica de la segmentación con la gestión eficiente de la memoria física de la paginación.
La administración de memoria es, en esencia, un compromiso constante entre velocidad, costo y tamaño, y el sistema operativo debe gestionar esta complejidad para ofrecer un entorno de cómputo robusto y de alto rendimiento.
Administración del Almacenamiento
La Administración del Almacenamiento es una función esencial del sistema operativo (SO) que abarca la gestión eficiente y segura de los datos a largo plazo en los dispositivos de almacenamiento secundario de un computador. A diferencia de la memoria principal volátil, el almacenamiento secundario proporciona persistencia, permitiendo que la información sobreviva incluso después de apagar el sistema. Esta administración es fundamental para el funcionamiento de cualquier aplicación informática.
Los conceptos clave de la administración del almacenamiento incluyen:
1. Archivos
Un archivo es la abstracción fundamental para almacenar información en el disco. Desde la perspectiva del programador, el sistema operativo oculta las peculiaridades de los discos y otros dispositivos de E/S, presentando un modelo abstracto limpio y agradable de archivos independientes del dispositivo.
Características y Propósito:


Persistencia: La información almacenada en los archivos no se ve afectada por la creación y terminación de los procesos; un archivo desaparece solo cuando su propietario lo elimina explícitamente.
Identificación: Los archivos tienen nombres simbólicos para que los usuarios puedan referenciarlos.
Compartibilidad: Pueden ser compartidos entre múltiples procesos, a menudo con control de acceso y permisos asociados.
Estructura: Un archivo puede tener una estructura interna (ej. registros de longitud fija) o ser un flujo de bytes sin estructura.
Protección: El SO es responsable de proteger los archivos y sus contenidos.
Tipos de Archivos:


Regulares: Contienen información del usuario (programas ejecutables, texto, bases de datos).
Directorios: Se utilizan para mantener la estructura del sistema de archivos.
Especiales:
De Carácter: Modelan dispositivos de E/S serial (terminales, impresoras, módems).
De Bloque: Modelan dispositivos que consisten en una colección de bloques direccionables al azar, como los discos.
Atributos de Archivos: Los archivos tienen atributos como propietario, tiempo de creación/modificación, permisos de acceso y tamaño. En sistemas como UNIX, estos se almacenan en el nodo-i.


Operaciones Comunes de Archivos:


Crear (Create): Define un nuevo archivo sin datos.
Borrar (Delete): Elimina un archivo.
Abrir (Open): Declara un archivo existente como "abierto" para un proceso, llevando sus atributos y direcciones de disco a memoria para un acceso rápido.
Cerrar (Close): Un proceso cierra un archivo, liberando recursos internos y forzando la escritura del último bloque modificado en disco.
Leer (Read): Lee datos del archivo.
Escribir (Write): Escribe datos en el archivo, actualizándolo o extendiéndolo.
Añadir (Append): Una forma restringida de escritura que solo agrega datos al final del archivo.
Buscar (Seek): Para archivos de acceso aleatorio, reposiciona el apuntador del archivo en una posición específica.
Obtener atributos (Get attributes): Permite leer los atributos de un archivo.
Organización Lógica y Acceso:


Pila: Los datos se almacenan en el orden de llegada, con búsqueda exhaustiva.
Secuencial: Registros de tamaño fijo en un orden específico, acceso secuencial.
Secuencial Indexado: Registros en secuencia con un campo clave y un índice para acceso aleatorio.
Indexado: Utiliza un índice para cada registro (ej. nodo-i).
De Acceso Directo o Hash: Utiliza una función hash para ubicar registros, ideal para acceso muy rápido y registros de tamaño fijo.
Acceso Aleatorio: Posibilidad de leer bytes o registros fuera de orden, esencial para bases de datos.
Archivos vs. Bases de Datos: Una base de datos es una colección de datos relacionados, diseñada para múltiples aplicaciones, y puede contener uno o más tipos de archivos. Generalmente, hay un sistema de gestión de base de datos separado del SO que utiliza sus programas de gestión de archivos.


2. Directorios
Los directorios (o carpetas) son estructuras que los sistemas de archivos utilizan para organizar y agrupar archivos. También son, en muchos sistemas, archivos ellos mismos.
Organización:


Un Solo Nivel: Sencillo, pero inadecuado para muchos archivos o usuarios.
Jerárquico (Tipo Árbol): La forma más común y flexible. Permite a los usuarios organizar archivos en subdirectorios.
Directorio Raíz: El punto de partida de la jerarquía (ej. / en UNIX/Linux).
Subdirectorios: Directorios contenidos dentro de otros directorios.
Directorio de Trabajo (Current Working Directory): Un directorio asociado a cada proceso, donde se buscan los nombres de ruta que no empiezan con una barra diagonal.
Nombres de Rutas:


Absolutas: Especifican la ruta completa desde el directorio raíz.
Relativas: Especifican la ruta desde el directorio de trabajo.
Vínculos (Enlaces):


Vínculo Duro (Hard Link): Permite que un archivo aparezca bajo dos o más nombres, a menudo en distintos directorios. Los cambios son visibles al instante para todos los que comparten el archivo.
Vínculo Simbólico (Soft Link / Symbolic Link): Un nombre que apunta a un pequeño archivo que, a su vez, nombra a otro archivo. Pueden traspasar los límites de los discos y nombrar archivos en computadoras remotas, pero son menos eficientes.
Operaciones de Directorio:


Buscar: Encontrar la entrada de un archivo.
Crear directorio (mkdir): Crea un directorio vacío (excepto por . y ..).
Eliminar directorio (rmdir): Elimina un directorio vacío.
Abrir directorio (opendir): Abre un directorio para leer sus contenidos.
Cerrar directorio (closedir): Cierra un directorio abierto.
Leer directorio (readdir): Devuelve la siguiente entrada en un directorio abierto.
Cambiar nombre (rename): Permite cambiar el nombre de un directorio.
Vincular (link): Crea un vínculo duro a un archivo existente.
Implementación de Directorios: Las entradas de directorio pueden ser de tamaño fijo o variable. Los atributos pueden almacenarse en la propia entrada de directorio o en estructuras separadas como los nodos-i.


3. Administración del Espacio Libre
La gestión del espacio libre es crucial para llevar un registro de los bloques de disco disponibles para su asignación a nuevos archivos.
Métodos:
Mapas de Bits (Bitmaps): La memoria se divide en unidades de asignación, y cada unidad tiene un bit correspondiente (0 libre, 1 ocupado). Es eficiente en espacio, a menos que el disco esté casi lleno.
Listas Ligadas de Bloques Libres (Linked Lists of Free Blocks): Cada bloque en la lista contiene números de otros bloques libres. Es más complejo para el sistema operativo manejar.
Listas de Porciones Libres Encadenadas: En vez de bloques individuales, se lleva la cuenta de series de bloques libres.
Cuotas de Disco: En sistemas multiusuario, el administrador puede asignar a cada usuario una cantidad máxima de archivos y bloques para evitar el uso excesivo de espacio.
Problemas: La fragmentación del espacio libre es un desafío. La contabilidad del espacio libre puede requerir I/O al disco, lo que puede ser costoso.
4. Asignación de Archivos
La asignación de archivos se refiere a cómo el sistema operativo decide dónde y cómo almacenar los bloques de un archivo en el disco.
Pre-asignación vs. Asignación Dinámica:


Pre-asignación: Requiere declarar el tamaño máximo del archivo al crearlo. Puede llevar a desperdicio de espacio si se sobrestima el tamaño.
Asignación Dinámica: Asigna espacio al archivo en porciones a medida que se necesita. Es más flexible, pero puede aumentar la complejidad de la gestión de memoria.
Métodos de Asignación:


Asignación Contigua: Almacena cada archivo como una serie contigua de bloques de disco.
Ventajas: Simple de implementar, buen rendimiento para acceso secuencial y aleatorio.
Desventajas: Fragmentación externa, dificultad para encontrar bloques contiguos de suficiente longitud, necesidad de especificar el tamaño en la creación, y posible necesidad de compactación del disco. Se utiliza en CD-ROMs.
Asignación Encadenada (Linked Allocation): Cada archivo se almacena como una lista enlazada de bloques de disco. El primer bloque de cada bloque se utiliza como puntero al siguiente.
Ventajas: No hay fragmentación externa, el bloque completo está disponible para datos si los punteros están fuera del bloque.
Desventajas: Acceso aleatorio muy lento (hay que seguir la cadena), el espacio de datos en un bloque no es una potencia de dos si el puntero está dentro del bloque.
Asignación Encadenada con Tabla en Memoria (FAT - File Allocation Table): Se colocan los punteros de la cadena en una tabla en la memoria principal (FAT). La entrada de directorio solo necesita el número del bloque inicial.
Ventajas: El bloque completo está disponible para datos, acceso aleatorio mucho más rápido (la cadena se sigue en memoria).
Desventajas: La FAT puede ser muy grande (proporcional al tamaño del disco), ocupando mucha memoria.
Asignación Indexada (Indexed Allocation / Nodos-i): Se asocia a cada archivo una estructura de datos (como un nodo-i en UNIX/Linux) que lista los atributos y las direcciones de disco de los bloques del archivo.
Ventajas: Permite encontrar todos los bloques del archivo dado el nodo-i. El nodo-i solo necesita estar en memoria cuando el archivo está abierto, lo que lo hace más eficiente en memoria que la FAT para discos grandes. No hay fragmentación externa.
Implementación en UNIX/Linux: El nodo-i incluye direcciones directas a los primeros bloques y puede utilizar niveles de indirección (indirecto simple, doble, triple) para archivos muy grandes, permitiendo un tamaño máximo de archivo enorme. El sistema de archivos de Linux utiliza asignación dinámica a nivel de bloque y un método indexado para llevar la traza de cada archivo.
5. Scheduling de Disco
El scheduling de disco es una función del sistema operativo que gestiona múltiples peticiones de E/S pendientes para el disco, con el objetivo de minimizar el tiempo de búsqueda (seek time) y, con ello, mejorar el rendimiento general del sistema. Dada la gran diferencia de velocidad entre los procesadores/memoria principal y los discos, la planificación del disco es de vital importancia.
Contexto: En un entorno multiprogramado, el sistema operativo mantiene una cola de peticiones de lectura y escritura para cada dispositivo de E/S. El tiempo de acceso al disco se compone de: tiempo de búsqueda, retraso rotacional y tiempo de transferencia de datos. El tiempo de búsqueda suele ser el factor dominante.


Algoritmos de Planificación de Disco:


Primero en Llegar, Primero en Ser Atendido (FCFS - First-Come, First-Served): Las peticiones se atienden en el orden en que llegan. Es simple, pero puede resultar en un movimiento ineficiente del brazo del disco si las peticiones están muy dispersas, llevando a un rendimiento deficiente.
Cilindro Más Cercano a Continuación (SSTF - Shortest Seek Time First): El planificador selecciona la petición que requiere el menor movimiento del brazo del disco desde su posición actual. Mejora el tiempo de búsqueda, pero puede causar inanición (starvation) a peticiones ubicadas en los extremos del disco.
Algoritmo del Elevador (SCAN / C-SCAN): El brazo del disco se mueve en una dirección, atendiendo todas las peticiones en su camino, y luego invierte la dirección. C-SCAN (Circular SCAN) solo escanea en una dirección y luego regresa rápidamente al otro extremo del disco sin atender peticiones durante el regreso. Estos algoritmos evitan la inanición.
Planificación de E/S Basada en Plazos (Deadline Scheduler): Considera los plazos de tiempo de las peticiones para asegurar que se cumplan, útil en sistemas de tiempo real. Linux utiliza una versión revisada que incluye dos listas adicionales y mantiene las operaciones de lectura o escritura en orden basándose en sus tiempos límite para evitar inanición.
Planificador de E/S Previsor (Anticipatory Scheduler): Diseñado para mejorar el rendimiento percibido en entornos con muchas peticiones de lectura síncronas. Retiene la siguiente petición de disco por un corto tiempo, anticipando que una aplicación generará otra petición de lectura para un sector cercano.
Cache de Disco (Disk Cache):


Un búfer en la memoria principal que almacena sectores de disco referenciados recientemente.
Propósito: Mejorar el rendimiento de E/S, reduciendo el número de accesos al disco al aprovechar la proximidad de referencias temporales y espaciales.
Funcionamiento: Si un bloque solicitado está en la caché ("acierto"), se entrega desde la RAM. Si no ("fallo"), el bloque se lee del disco y se coloca en la caché. También se utiliza para escrituras, retardando la escritura a disco.
Gestor de Caché (Cache Manager): En sistemas como Windows, gestiona la caché de disco. Puede realizar lectura adelantada (read-ahead).
Caché de Páginas (Page Cache): Linux utiliza una caché de páginas para los bloques de disco, aplicando un algoritmo de reemplazo de páginas cuando la caché está llena.
RAID (Redundant Array of Independent Disks): Un esquema estándar para el diseño de bases de datos en múltiples discos para mejorar el rendimiento y la fiabilidad mediante redundancia.


En síntesis, la administración del almacenamiento es un componente complejo del sistema operativo que garantiza la persistencia, organización, acceso controlado y rendimiento óptimo de los datos, sirviendo como la "memoria a largo plazo" de la computadora y como interfaz crucial para usuarios y aplicaciones.
Subsistema de Entrada/Salida (I/O)
La Administración del Almacenamiento es un componente esencial del sistema operativo (SO) que gestiona los dispositivos de almacenamiento secundario de un computador, asegurando la persistencia y el acceso eficiente a los datos a largo plazo.
El Subsistema de Entrada/Salida (E/S) es una parte crítica del núcleo del sistema operativo, responsable de ocultar las peculiaridades de los dispositivos de hardware específicos al usuario y al resto del sistema. Su objetivo principal es proporcionar la interfaz más sencilla posible al resto del sistema y optimizar la E/S para la máxima concurrencia, ya que los dispositivos de E/S suelen ser los componentes más lentos del computador.
El subsistema de E/S consta de varios componentes clave:
1. Buffering (Uso de Buffers de E/S)
El buffering implica utilizar áreas de almacenamiento temporales (buffers) en la memoria principal para suavizar el flujo de datos entre la CPU y los dispositivos de E/S. Esta técnica es fundamental para mejorar el rendimiento general del sistema y la utilización de la CPU, ya que las velocidades internas del computador suelen ser mucho más rápidas que las de los dispositivos de E/S.
Propósito y Utilidad: Los buffers se utilizan para evitar sobrecargas e ineficiencias causadas por la disparidad de velocidades entre la CPU y los dispositivos. Realizan transferencias de entrada antes de que se hagan las peticiones correspondientes y las de salida un tiempo después, desacoplando la transferencia real de E/S del espacio de direcciones del proceso de aplicación. Esto permite al SO más flexibilidad en la gestión de memoria. El uso de buffers mejora el rendimiento, permitiendo que un proceso de usuario procese un bloque de datos mientras se lee el siguiente.


Tipos de Dispositivos para Buffering: Es importante distinguir entre:


Dispositivos Orientados a Bloques: Almacenan información en bloques de tamaño fijo y las transferencias se realizan bloque a bloque. Los discos y las cintas son ejemplos.
Dispositivos Orientados a Flujo de Caracteres: Transfieren datos como un flujo de bytes sin estructura de bloques. Los terminales, impresoras, puertos de comunicación y el ratón son ejemplos.
Esquemas de Buffering:


Buffer Único: El sistema operativo asigna un buffer en la memoria principal para la operación de E/S. Para dispositivos orientados a bloques, se puede realizar una lectura adelantada (read-ahead), donde se lee el siguiente bloque anticipadamente, asumiendo acceso secuencial. Para dispositivos orientados a flujo de caracteres, puede operar línea a línea (suspendiendo el proceso hasta que llegue una línea completa) o byte a byte (modelo productor-consumidor).
Buffer Doble (Intercambio de Buffers): Se asignan dos buffers al sistema. Un proceso transfiere datos a (o desde) un buffer mientras el SO vacía (o llena) el otro. Esto puede mantener el dispositivo trabajando a toda velocidad si el tiempo de procesamiento es menor o igual al tiempo de transferencia. Para E/S línea a línea, el proceso de usuario no necesita suspenderse si los buffers dobles están por delante.
Buffer Circular: Se utilizan más de dos buffers organizados en un conjunto circular. Esto es útil si el proceso realiza ráfagas rápidas de E/S y el buffer doble resulta insuficiente. Opera bajo el modelo productor-consumidor.
Desafíos: La gestión de buffers introduce complejidad en el SO, ya que debe rastrear la asignación de buffers y su impacto en el intercambio de procesos.


2. Caching (Uso de Caché de Disco)
La caché de disco es un buffer, generalmente almacenado en la memoria principal, que actúa como una caché de bloques de disco entre el almacenamiento en disco y el resto de la memoria principal. Es un componente de gestión de memoria del subsistema de E/S.
Propósito y Funcionamiento: Mejora el rendimiento de la E/S basada en archivos, haciendo que los datos de disco recientemente referenciados residan en memoria principal para un acceso rápido y retardando las escrituras en disco al mantener las actualizaciones en memoria por un corto tiempo antes de enviarlas en lotes más eficientes. Cuando se solicita un bloque de disco, primero se verifica si está en la caché de disco (un "acierto"). Si lo está, se entrega rápidamente. Si no (un "fallo"), se lee del disco y se coloca en la caché.
Gestor de Caché: En sistemas como Windows, el gestor de caché maneja la caché para todo el subsistema de E/S, proporcionando un servicio de caché en memoria principal para los componentes del sistema de archivos y de red. Puede ajustar dinámicamente el tamaño de la caché. Linux utiliza una caché de páginas unificada entre los drivers de disco y el sistema de archivos.
Políticas de Escritura: Pueden implicar retrasar las escrituras en disco, lo que también mejora el rendimiento.
3. Spooling
El spooling (de Simultaneous Peripheral Operation On Line, operación periférica simultánea en línea) es una técnica que utiliza la memoria secundaria como un buffer de almacenamiento para reducir los retrasos de procesamiento al transferir datos entre equipos periféricos y los procesadores de un computador.
Contexto Histórico y Propósito: Surgió en la tercera generación de sistemas operativos para mejorar la eficiencia. Permitía leer trabajos desde tarjetas y colocarlos en disco tan pronto como llegaban, de modo que el SO podía cargar un nuevo trabajo del disco en una partición de memoria vacía en cuanto el trabajo en ejecución terminaba. Esto eliminó la necesidad de máquinas 1401 y el transporte de cintas, agilizando las operaciones de entrada/salida y haciendo más eficiente el uso de la CPU.
Funcionamiento: Básicamente, el spooling es un sistema de buffering que gestiona la entrada y salida de trabajos para dispositivos como impresoras. Los trabajos se copian a un área en disco (el "spool") desde donde el dispositivo puede recogerlos a su propio ritmo.
4. Drivers (Controladores de Dispositivo)
Los drivers de dispositivo son módulos de software especializados que actúan como traductores, ocultando las peculiaridades del hardware específico de los dispositivos de E/S. Son una parte fundamental del subsistema de E/S.
Función y Rol:
Abstracción: Permiten que el resto del sistema operativo (y los programas de usuario) interactúen con los dispositivos de E/S de una manera estándar y uniforme, sin conocer los detalles específicos de cada dispositivo.
Traducción de Comandos: Su entrada consiste en comandos de alto nivel (ej. "recuperar bloque 123") y su salida son instrucciones de bajo nivel, específicas del hardware, utilizadas por el controlador de hardware del dispositivo.
Interfaz Estándar: Exportan una de las interfaces estándar para la comunicación con el hardware.
Modo Kernel: Los drivers de dispositivo se ejecutan en modo kernel (o supervisor) para acceder a los recursos protegidos del sistema.
Controladores Específicos: Solo el driver de dispositivo conoce las particularidades del dispositivo al que está asignado. En sistemas como Linux, los drivers se clasifican en controladores de dispositivos de caracteres, de bloques y de red.
Estructura en Windows: En Windows, los drivers de dispositivo son bibliotecas de vínculos dinámicos que se cargan por el ejecutivo de NTOS y se utilizan como un mecanismo de extensibilidad general para el modo kernel. El Administrador de E/S organiza una pila de dispositivos para cada instancia de dispositivo, donde los objetos de dispositivo se vinculan a objetos de driver específicos. Los drivers de filtro pueden insertarse en esta pila para preprocesar o postprocesar operaciones de E/S, o para implementar funcionalidad nueva como RAID o filtros antivirus. Los drivers pueden dividirse en una parte superior (interfaz con el resto del kernel) y una inferior (interacción con el hardware).
Estructura en Symbian OS: Define LDD (Logical Device Driver) para la interfaz de programa y PDD (Physical Device Driver) para la interfaz con el dispositivo físico, que maneja el buffering y el control de flujo.
Plug and Play: En sistemas como Windows, el administrador de Plug and Play trabaja estrechamente con el administrador de E/S para descubrir dispositivos conectados, asignar recursos de hardware y cargar los drivers apropiados dinámicamente.
5. Gestión de Dispositivos (Device Management)
La gestión de dispositivos se refiere a la forma en que el sistema operativo controla y coordina los diversos dispositivos de E/S conectados al sistema.
Diversidad de Dispositivos: Los dispositivos de E/S varían ampliamente en función y velocidad (ej., un ratón, un disco duro, un reproductor de CD-ROM). Pueden ser legibles para el usuario (impresoras, terminales) o legibles por máquina (discos, unidades USB, sensores). También difieren en complejidad de control, unidad de transferencia (bytes o bloques) y si son dedicados o compartibles.
Abstracción y Estructura Jerárquica:
El SO es el responsable de gestionar y controlar las operaciones y los dispositivos de E/S.
Se busca proporcionar una interfaz estándar y uniforme para los dispositivos, lo que se logra mediante la abstracción, encapsulación y capas de software.
La función de E/S del SO se organiza jerárquicamente en capas:
Control de E/S / Controladores de Dispositivo / Manejadores de Interrupción: El nivel más bajo, que interactúa directamente con el hardware y sus controladores.
E/S Lógica: Trata a los dispositivos como recursos lógicos y maneja funciones generales de E/S para los procesos de usuario (ej. open, close, read, write).
E/S de Dispositivo: Convierte las operaciones solicitadas en secuencias de instrucciones de E/S específicas del hardware.
Planificación y Control: Gestiona la cola y la planificación de operaciones de E/S, maneja las interrupciones y recopila el estado de E/S.
Organización Física: Se encarga de convertir referencias lógicas a archivos en direcciones físicas del almacenamiento secundario (pistas, sectores), y gestiona la asignación de espacio y el buffering.
Software de E/S Independiente del Dispositivo: Realiza funciones comunes a muchos o todos los dispositivos, como el buffering y el reporte de errores, independientemente del hardware específico.
Software de E/S en Espacio de Usuario: Incluye bibliotecas de E/S y el uso de colas que se ejecutan en espacio de usuario.
Técnicas de Comunicación de E/S:
E/S Programada: La CPU envía un comando de E/S y luego espera activamente (sondear continuamente) a que el dispositivo termine antes de continuar. Es ineficiente.
E/S Dirigida por Interrupciones: La CPU inicia una operación de E/S y luego continúa con otro trabajo. El módulo de E/S interrumpe a la CPU cuando está listo para intercambiar datos. Esto mejora el rendimiento general.
Acceso Directo a Memoria (DMA): Un módulo especial (chip DMA) controla el flujo de datos directamente entre la memoria principal y un controlador de E/S sin la intervención constante de la CPU. La CPU delega la transferencia al DMA, que genera una interrupción solo después de que se ha transferido un bloque completo. Es la forma dominante de transferencia para grandes volúmenes de datos. Puede operar en modo de palabra (robo de ciclo) o en modo de ráfaga.
Control de Errores: El SO detecta y responde a errores en dispositivos, reintentando operaciones o informando del fallo. El software de E/S independiente del dispositivo puede manejar errores específicos del dispositivo.
Dispositivos Especiales (UNIX/Linux): Los dispositivos de E/S se integran en el sistema de archivos como archivos especiales (en /dev), lo que permite usar las mismas llamadas al sistema (read, write) que para archivos ordinarios. Se distinguen entre archivos especiales de bloque (para discos y dispositivos direccionables aleatoriamente) y archivos especiales de carácter (para impresoras, módems, que manejan flujos de caracteres).
Sockets: En Linux, los sockets se implementan como archivos especiales y proporcionan una interfaz para la comunicación de red. El modelo de sockets también sirve de base para la E/S de otros dispositivos, adaptándose a diferentes entornos de red.
En resumen, el subsistema de E/S es una capa compleja pero fundamental del sistema operativo que gestiona la diversidad del hardware de E/S, mejora el rendimiento mediante técnicas como el buffering y la caché, y proporciona una interfaz abstracta y segura para que los programas y usuarios interactúen con los dispositivos.


Protección y Seguridad
La Protección y la Seguridad son dos conceptos interrelacionados y fundamentales en el diseño y funcionamiento de los sistemas operativos (SO) modernos. A medida que los sistemas informáticos se han vuelto más sofisticados, la necesidad de proteger su integridad ha crecido exponencialmente. Originalmente, la protección se concibió para sistemas operativos multiprogramados donde usuarios no confiables compartían un espacio de nombres lógico (como directorios de archivos) o físico (como la memoria). Hoy en día, con la proliferación de redes e Internet, todos los sistemas informáticos, desde servidores hasta dispositivos móviles, deben preocuparse por la protección y la seguridad.
Aunque a menudo se usan indistintamente, es útil diferenciar entre protección y seguridad:
La protección se refiere a los mecanismos específicos del sistema operativo que controlan el acceso de programas, procesos o usuarios a los recursos definidos por un sistema informático. Estos mecanismos deben permitir especificar los controles a imponer y hacerlos cumplir. La protección también mejora la fiabilidad al detectar errores latentes en las interfaces entre subsistemas.
La seguridad es una medida de confianza en que la integridad de un sistema y sus datos se preservará. Es un problema más amplio que abarca aspectos técnicos, administrativos, legales y políticos. La seguridad protege la integridad de la información (datos y código) y los recursos físicos del sistema contra el acceso no autorizado, la destrucción o alteración maliciosa y la introducción accidental de inconsistencias.
Los objetivos de la seguridad informática, conocidos como la tríada CIA, son:
Confidencialidad: Asegura que la información privada o confidencial no sea divulgada a individuos no autorizados.
Integridad: Asegura que la información y los programas solo se modifiquen de manera especificada y autorizada.
Disponibilidad: Asegura que los sistemas funcionen puntualmente y que el servicio no sea negado a usuarios autorizados.
A estos se suma la Autenticidad, que verifica que los usuarios son quienes dicen ser y que cada entrada proviene de una fuente confiable.
1. Control de Acceso
El control de acceso es el mecanismo que implementa una política de seguridad, especificando quién o qué (por ejemplo, un proceso) puede tener acceso a cada recurso del sistema y el tipo de acceso permitido. Media entre un usuario (o proceso) y los recursos del sistema como aplicaciones, SO, archivos y bases de datos.
Objetivos y Principios del Control de Acceso:
Principio de Menor Privilegio (Principle of Least Authority - POLA): Dicta que los programas, usuarios e incluso sistemas deben tener solo los privilegios suficientes para realizar sus tareas, y no más. Seguir este principio simplifica las decisiones de diseño y mantiene la consistencia del sistema.
Protección de Recursos: Asegurar que solo los procesos con la debida autorización del SO puedan operar sobre archivos, memoria, CPU y otros recursos.
Distingue entre uso autorizado y no autorizado.
Funcionalidad del Acceso: Los sistemas de protección modernos se preocupan no solo por la identidad del recurso, sino también por la naturaleza funcional de ese acceso, incluso incluyendo funciones definidas por el usuario.
Niveles y Grados de Protección:
Sin protección alguna: Apropiado para procedimientos que son sensibles de ejecutar en instantes diferentes.
Aislamiento: Cada proceso opera por separado, sin compartición ni comunicación alguna. Cada uno tiene su propio espacio de direcciones, ficheros y otros objetos.
Compartición completa o sin compartición: El propietario declara si un objeto será público (cualquier proceso puede acceder) o privado (solo los procesos del propietario).
Compartición vía limitaciones de acceso: El SO verifica la permisibilidad de cada acceso de cada usuario específico sobre cada objeto, actuando como guardián.
Acceso vía capacidades dinámicas: Permite la creación dinámica de derechos de acceso a los objetos.
Uso limitado de un objeto: Limita no solo el acceso, sino también el uso que se puede hacer del objeto (ej., ver un documento, pero no imprimirlo).
Modelos de Control de Acceso: Conceptualmente, el control de acceso se puede representar mediante una matriz de acceso.
Las filas de la matriz son los sujetos (iniciadores de acciones), que son típicamente usuarios o procesos actuando en nombre de usuarios o grupos.
Las columnas son los objetos (receptores de acciones), que pueden ser archivos, programas, segmentos de memoria, procesos, terminales, impresoras, o estructuras creadas por usuarios o programas.
Cada celda de la matriz especifica los derechos de acceso (acciones) que un sujeto tiene sobre un objeto (ej., lectura, escritura, ejecución, borrado, etc.).
Dado que una matriz de acceso suele ser dispersa (muchas celdas vacías), se implementa descomponiéndola de dos maneras principales:
Listas de Control de Acceso (ACLs - Access Control Lists):


La matriz se descompone por columnas.
Cada objeto tiene asociada una lista (ordenada) que contiene todos los dominios (usuarios/grupos) que pueden acceder al objeto y la forma de hacerlo.
Pueden incluir entradas por defecto o públicas, y pueden contener usuarios individuales o grupos.
En UNIX/Linux, los archivos están protegidos por un código binario de 9 bits (rwx bits) para el propietario, el grupo y todos los demás. Estos bits especifican permisos de lectura (r), escritura (w) y ejecución (x). El UID (User ID) y GID (Group ID) del proceso determinan su dominio de protección. Cuando un proceso intenta abrir un archivo, el sistema comprueba estos bits en el nodo-i del archivo.
En Windows, los objetos tienen un descriptor de seguridad que contiene una Lista de Control de Acceso Discrecional (DACL). La DACL especifica qué usuarios y grupos pueden acceder a un objeto y para qué operaciones. Consiste en una lista de Entradas de Control de Acceso (ACEs) que asocian un SID (Security ID) a una máscara de acceso con los derechos permitidos o denegados. Las entradas de denegación se suelen colocar antes que las de permiso para evitar puertas traseras.
Las ACLs son fáciles de gestionar porque el acceso se otorga por usuario, no por proceso.
Capacidades (Capabilities):


La matriz se descompone por filas.
Una capacidad es un "ticket" que especifica objetos y operaciones autorizadas para un determinado usuario.
Cada proceso tiene una lista de capacidades (lista-C) que define los objetos a los que puede acceder y los permisos asociados.
Requieren protección contra la falsificación, a menudo siendo gestionadas por el SO en áreas de memoria no accesibles al usuario o utilizando criptografía.
Proporcionan mayor flexibilidad que los esquemas de protección basados en anillos.
En Symbian OS, las aplicaciones reciben un conjunto de capacidades al instalarse, que se compara con el acceso solicitado a los recursos.
2. Identificación de Usuarios y Grupos (Autenticación)
La autenticación es el medio para establecer la validez de una identidad declarada por un usuario. Sin una autenticación sólida, el sistema operativo no puede saber a qué archivos y otros recursos puede acceder un usuario.
Los métodos de autenticación se basan en tres principios generales:
Algo que el usuario conoce:


Contraseñas (Passwords): La forma más común de autenticación. Un usuario proporciona un identificador (ID) y una contraseña. El sistema verifica si el ID es conocido y si la contraseña coincide.
Vulnerabilidad: Las contraseñas son susceptibles a ataques de adivinación, como pruebas exhaustivas de contraseñas cortas, uso de diccionarios, o recopilación de información personal.
Esquema UNIX: Las contraseñas no se almacenan en claro; en su lugar, se usa un procedimiento que cifra la contraseña con un valor de "aderezo" (salt) de 12 bits (relacionado con el momento de asignación) y un algoritmo (como DES modificado), repitiendo el proceso 25 veces. El "aderezo" evita que contraseñas duplicadas sean visibles en el archivo de contraseñas.
Contramedidas: Proteger el archivo de contraseñas del acceso no autorizado, educar a los usuarios sobre contraseñas fuertes, y verificación proactiva de contraseñas (el sistema prueba la contraseña al momento de la selección y la rechaza si no es suficientemente fuerte).
Algo que el usuario posee:


Ejemplos incluyen tarjetas electrónicas, tarjetas inteligentes o llaves físicas. Las tarjetas inteligentes son pequeñas computadoras a prueba de falsificaciones que pueden autenticar al usuario con un sistema central.
Algo que el usuario es (Biometría):


Mide características físicas del usuario difíciles de falsificar, como huellas digitales o reconocimiento de voz.
El proceso implica una fase de inscripción (medición y digitalización de características) y una fase de identificación (verificación contra el registro almacenado).
Identificación de Usuarios y Grupos en Sistemas Específicos:
UNIX/Linux: Cada usuario tiene un UID (User ID) único (entero entre 0 y 65,535). Los archivos y procesos se marcan con el UID de su propietario. Los usuarios pueden organizarse en grupos con GIDs (Group IDs) de 16 bits. La combinación (UID, GID) define el dominio de protección de un proceso. El bit SETUID es un bit de protección especial que, al ejecutar un programa con él activado, hace que el UID efectivo del proceso sea el del propietario del archivo ejecutable, no el del usuario que lo invocó, lo que permite el acceso controlado a recursos privilegiados.
Windows XP/Vista: El modelo de seguridad se basa en cuentas de usuario, que pueden agruparse. Los usuarios se identifican por un Security ID (SID) único. Al iniciar sesión, Windows crea un security access token (testigo de acceso) para el proceso del usuario.
El testigo de acceso incluye el SID del usuario, los SIDs de los grupos a los que pertenece el usuario, y una lista de privilegios especiales. Cada proceso ejecutado en nombre del usuario recibe una copia de este testigo.
Los privilegios otorgan poderes especiales, como apagar el equipo o acceder a archivos protegidos. Se pueden habilitar individualmente por proceso.
Imitación de identidad (Impersonation): Un hilo servidor puede asumir temporalmente la identidad de un hilo cliente para acceder a objetos protegidos del cliente, simplificando la seguridad en entornos cliente/servidor.
Los descriptores de seguridad asociados a los objetos contienen la DACL (Discretionary Access Control List) y, en Windows Vista, también la SACL (System Access Control List) y un nivel de integridad. Los niveles de integridad obligatorios evitan el acceso de escritura a objetos, protegiendo contra procesos comprometidos.
3. Defensa contra Ataques
La defensa contra ataques implica proteger el sistema de amenazas externas e internas. Se requiere una estrategia de defensa en profundidad, con múltiples capas de seguridad, para que si una falla, otras aún protejan el sistema.
Tipos de Amenazas de Seguridad:
Amenazas a la confidencialidad:


Exposición de datos.
Intercepción: Elemento no autorizado accede a datos, como la escucha en canales de comunicación o la copia ilícita de archivos.
Análisis de tráfico: Observar patrones de mensajes (frecuencia, tamaño, origen/destino) incluso si el contenido está cifrado.
Amenazas a la integridad:


Alteración de datos.
Modificación: Elemento no autorizado modifica un componente (ej., cambiar valores de archivos de datos, alterar un programa).
Software Malicioso (Malware): Programas diseñados para causar daño o realizar acciones no autorizadas.
Virus: Código que se adjunta a otros programas y se replica.
Gusanos (Worms): Programas autónomos que se propagan a través de redes.
Caballos de Troya (Trojans): Contienen código malicioso oculto en software legítimo, realizando acciones no autorizadas sin el conocimiento del usuario.
Puertas Secretas (Backdoors): Puntos de entrada secretos no documentados en un programa para evadir los controles de seguridad estándar.
Bombas Lógicas (Logic Bombs): Código que se activa bajo ciertas condiciones (ej., fecha específica).
Rootkits: Herramientas que ocultan la presencia de un atacante y le permiten mantener el control de un sistema.
Spyware: Recopila información del usuario sin su consentimiento.
Ataques de desbordamiento de búfer (Buffer Overflow Attacks): Explotan errores de programación para ejecutar código malicioso.
Amenazas a la disponibilidad:


Denegación de Servicio (DoS - Denial of Service): Prevenir o imposibilitar el uso normal de los recursos o servicios del sistema (ej., sobrecargar una red).
Amenazas a la autenticación:


Fabricación: Elemento no autorizado inserta objetos extraños en el sistema (ej., mensajes falsos en una red).
Enmascaramiento (Masquerading): Un elemento intenta hacerse pasar por otro (suplantación de identidad).
Reenvío (Replay Attacks): Capturar secuencias de autenticación válidas y retransmitirlas para obtener acceso.
Intrusos (Intruders): Individuos no autorizados que penetran en los controles de acceso. Clases: Enmascarado (usuario no autorizado usa cuenta legítima), Trasgresor (usuario legítimo accede a recursos no autorizados o usa privilegios maliciosamente), Clandestino (modifica el sistema para ocultar su presencia).
Estrategias de Defensa y Contramedidas:
Prevención del Interbloqueo: Diseño del sistema para asegurar que una de las cuatro condiciones necesarias para el interbloqueo nunca se cumpla (exclusión mutua, retención y espera, sin expropiación, espera circular). Es una estrategia conservadora que limita el acceso a recursos.
Predicción del Interbloqueo: Analizar cada petición de recurso para determinar si concederla podría llevar a un interbloqueo ("algoritmo del banquero").
Detección del Interbloqueo: Permitir que los interbloqueos ocurran, pero ejecutar periódicamente un algoritmo para detectarlos y tomar acciones correctivas (ej., abortar procesos involucrados).
Detección de Intrusos (IDS - Intrusion Detection System): Se basa en la suposición de que el comportamiento de un intruso difiere del de un usuario legítimo.
Basada en la estadística de anomalías: Desarrolla perfiles de actividad normal y detecta desviaciones.
Basada en reglas: Intenta definir un conjunto de reglas para identificar comportamiento sospechoso.
Tarro de miel (Honeypot): Trampas para atraer y estudiar a los atacantes.
Antivirus: Estrategias de detección, identificación y eliminación de virus.
Firewalls: Mecanismos de seguridad de red que inspeccionan todo el tráfico entrante y saliente. Pueden ser sin estado o con estado, y algunos implementan IDS.
Sistemas Confiables (Trusted Systems): Diseñados e implementados de forma que los usuarios tienen la seguridad completa de que el sistema aplicará una política de seguridad determinada.
Seguridad Multinivel (Multilevel Security - MLS): Clasifica la información en diferentes niveles (ej., no clasificada, confidencial, secreta) y asigna a los sujetos niveles de autorización.
Modelo Bell-La Padula: Un modelo de seguridad multinivel ampliamente utilizado, basado en dos reglas:
Propiedad de seguridad simple ("No leer hacia arriba"): Un sujeto en un nivel k solo puede leer objetos en su nivel o en uno inferior.
Propiedad * ("No escribir hacia abajo"): Un sujeto en un nivel k solo puede escribir en un objeto en su nivel o en uno superior.
Monitor de Referencia: Un elemento de control en el hardware y el SO que regula el acceso de sujetos a objetos basándose en parámetros de seguridad. Debe ser completo, aislado y verificable.
Base de Cómputo Confiable (TCB - Trusted Computing Base): La porción del sistema que es responsable de hacer cumplir la política de seguridad. Reducir el tamaño de la TCB aumenta la seguridad potencial.
Protección de la Memoria: Esencial en entornos multiprogramados para evitar que un proceso interfiera con el espacio de memoria de otro. Los esquemas de memoria virtual suelen incluir mecanismos para esto.
Criptografía: Herramienta esencial para la seguridad de la información.
Cifrado/Descifrado: Transforma mensajes (texto simple) en texto cifrado para que solo los autorizados puedan leerlos.
Criptografía de clave secreta (simétrica): Usa una única clave compartida para cifrar y descifrar.
Criptografía de clave pública (asimétrica): Usa un par de claves (una pública para cifrar, una privada para descifrar).
Firmas digitales: Utilizan criptografía para verificar la autenticidad e integridad de la información.
Seguridad en Windows: Implementa mecanismos como los SIDs, descriptores de seguridad, tokens de acceso, DACLs, SACLs, y niveles de integridad para proporcionar un control de acceso detallado y protección contra varias amenazas. El monitor de referencia de seguridad es el módulo en el kernel que implementa las comprobaciones de acceso.
En resumen, la protección y la seguridad son pilares de los sistemas operativos modernos, asegurando no solo la funcionalidad sino también la resiliencia ante una creciente variedad de amenazas.

Sistemas Distribuidos
Los Sistemas Distribuidos representan una evolución significativa en la arquitectura de computación, diseñada para superar las limitaciones de los sistemas centralizados en términos de velocidad, disponibilidad y fiabilidad. Se definen por una colección de procesadores que no comparten memoria ni un reloj, sino que cada uno posee su propia memoria local y se comunican entre sí a través de diversas redes.
1. Definición de Sistema Operativo Distribuido (SO Distribuido)
Un sistema distribuido es una colección de sistemas computacionales físicamente separados, y posiblemente heterogéneos, que están interconectados por una red para proporcionar a los usuarios acceso a los diversos recursos que mantiene. Desde la perspectiva de un procesador específico, los demás procesadores y sus recursos son remotos, mientras que los suyos son locales. Los procesadores en un sistema distribuido pueden variar en tamaño y función, incluyendo desde pequeños microprocesadores y estaciones de trabajo hasta minicomputadores y grandes sistemas de propósito general.
El Sistema Operativo Distribuido (SO distribuido) es una capa de software que gestiona estos procesadores para crear una ilusión de un sistema único y unificado para el usuario. A diferencia de un sistema operativo de red (donde los usuarios son conscientes de la existencia de múltiples computadoras y deben interactuar explícitamente con ellas para compartir recursos), un SO distribuido proporciona un entorno menos autónomo y una imagen de sistema único.
Características clave de un SO distribuido:
Ausencia de memoria compartida y reloj común: La característica definitoria es que los procesadores no comparten memoria principal ni un reloj físico. En su lugar, cada uno tiene su propia memoria local y se comunican exclusivamente mediante el intercambio de mensajes a través de la red.
Aumento de capacidades: Proporciona acceso a los recursos del sistema, lo que puede aumentar la velocidad de cómputo, la funcionalidad, la disponibilidad de datos y la fiabilidad.
Gestión de la complejidad: Debe ofrecer mecanismos para la sincronización y comunicación entre procesos, así como para la gestión de problemas de interbloqueo y una variedad de fallos que no se encuentran en los sistemas centralizados.
Migración de procesos: Permite mover un proceso activo de una máquina a otra para fines como el equilibrio de carga, la mejora del rendimiento de las comunicaciones, la disponibilidad o el uso de facilidades especiales.
Soporte de abstracciones: Proporciona abstracciones de recursos como un sistema de ficheros distribuido, y la ilusión de un único espacio de memoria principal y secundaria.
Manejo de estados globales: Dada la naturaleza asíncrona y los retrasos en la comunicación, el SO distribuido debe lidiar con el hecho de que no existe un "estado global" preciso e instantáneo del sistema.
Enfoque orientado a objetos: El diseño orientado a objetos puede facilitar la creación de sistemas distribuidos, promoviendo la modularidad, la flexibilidad y la integración de componentes.
2. Redes LAN/MAN/WAN
Los sistemas distribuidos se construyen sobre redes de computadoras, que son vías de comunicación entre dos o más sistemas. Estas redes varían según los protocolos utilizados, las distancias entre nodos y los medios de transporte.
Existen dos tipos principales de redes:
Redes de Área Local (LAN - Local-Area Networks): Están compuestas por procesadores distribuidos en áreas geográficas pequeñas, como un edificio o un grupo de edificios adyacentes. Ethernet es un ejemplo común de LAN.
Redes de Área Extensa (WAN - Wide-Area Networks): Comprenden un número de procesadores autónomos distribuidos sobre un área geográfica grande, como una ciudad, un país o incluso a nivel mundial.
Las WAN surgieron a finales de los años 60, con el Arpanet siendo la primera.
Las conexiones típicas son líneas telefónicas, líneas dedicadas, enlaces de microondas y canales satelitales, que son relativamente lentos e inestables.
Los procesadores de comunicación especiales controlan estas conexiones y son responsables de definir la interfaz de comunicación y transferir información entre los sitios.
Internet es un ejemplo de WAN, una red mundial de redes que interconecta hosts en sitios geográficamente separados. Los hosts suelen estar en LANs, que a su vez se conectan a Internet a través de redes regionales interconectadas por encaminadores (routers). Los encaminadores controlan la ruta que toma cada mensaje a través de la red.
Protocolos de Red y Servicios:
Las redes de computadoras proporcionan servicios a sus usuarios, los cuales se implementan mediante protocolos, que son conjuntos de reglas que gobiernan el intercambio de datos entre entidades.
La funcionalidad de comunicación se organiza típicamente en una arquitectura de protocolos por capas, donde cada capa proporciona una parte de la funcionalidad necesaria y depende de la capa inferior.
TCP/IP es la arquitectura de protocolos estándar más común, que consiste en cinco capas: física, de acceso a red, de Internet, de transporte y de aplicación.
La Capa de Internet utiliza el Protocolo de Internet (IP) para el encaminamiento de datos a través de múltiples redes interconectadas.
La Capa de Transporte utiliza protocolos como TCP (Protocolo de Control de Transmisión) o UDP (Protocolo de Datagramas de Usuario) para la comunicación entre aplicaciones en diferentes máquinas.
Sockets: El concepto de socket es fundamental para la comunicación en red. Un socket es un punto final de comunicación que permite la comunicación entre procesos cliente y servidor, pudiendo ser orientado a conexión o no. La Interfaz de Sockets de Berkeley es un API estándar para el desarrollo de aplicaciones de red.
Middleware: En sistemas distribuidos, a menudo se añade una capa de software conocida como middleware sobre el sistema operativo. Esta capa proporciona una base uniforme para que las aplicaciones interactúen con la red y los recursos remotos, ocultando la complejidad subyacente de la distribución y heterogeneidad.
3. Transparencia del Sistema
La transparencia del sistema es un objetivo clave en el diseño de los sistemas distribuidos, buscando que el usuario perciba el sistema como una única entidad coherente, a pesar de que esté compuesto por múltiples máquinas interconectadas. Un sistema transparente oculta la distribución a los usuarios y programadores, simplificando la interacción y el desarrollo de aplicaciones.
Aspectos clave de la transparencia en sistemas distribuidos:
Transparencia de ubicación (Location Transparency): Significa que el nombre de un recurso (como un archivo) no revela su ubicación física en la red. Un archivo puede moverse de un servidor a otro sin que su nombre de ruta cambie, y el usuario no necesita saber dónde reside físicamente el archivo. Los SO distribuidos, a diferencia de los de red, ocultan al usuario dónde se ejecutan sus programas o dónde se encuentran sus archivos.
Ilusión de un sistema centralizado: El SO distribuido presenta a sus usuarios la ilusión de un sistema tradicional con un solo procesador, incluso si en realidad está compuesto por múltiples procesadores. Esto implica que las aplicaciones no necesitan preocuparse por los detalles de la distribución.
Imagen Única del Sistema (Single-System Image - SSI): Un cluster, por ejemplo, es un grupo de computadoras interconectadas que trabajan juntas como un recurso computacional unificado, pudiendo crear la ilusión de ser una única máquina. Para lograr esto, el middleware del cluster puede proporcionar:
Un único espacio de memoria (memoria compartida distribuida).
Un único sistema de control de trabajos (un planificador de trabajos global).
Una única interfaz de usuario.
Un único espacio de E/S (permitiendo el acceso remoto a cualquier periférico o disco sin conocer su ubicación física).
Transparencia de acceso: Relacionada con la transparencia de ubicación, permite que se acceda a los recursos remotos de la misma manera que a los locales. Por ejemplo, en el modelo cliente/servidor, el cliente no necesita saber si una petición es atendida local o remotamente.
Facilitación de la programación: Al ocultar la complejidad de la distribución, la transparencia del sistema facilita la programación de aplicaciones distribuidas, ya que los desarrolladores pueden centrarse en la lógica de la aplicación en lugar de en los detalles de la red.
En resumen, los sistemas distribuidos aprovechan la conectividad de las redes para ofrecer capacidades computacionales avanzadas, y los SO distribuidos trabajan para abstraer esta complejidad, presentando al usuario una vista unificada y transparente, aunque internamente gestionen una intrincada interacción de componentes hardware y software.


Sistemas de Propósitos Especiales

Los Sistemas de Propósitos Especiales son una categoría de sistemas operativos diseñados y optimizados para satisfacer requisitos específicos que van más allá de los de un sistema de cómputo de propósito general. Estos requisitos pueden incluir restricciones de tiempo críticas, altas tasas de datos para contenido multimedia, o limitaciones severas de recursos en dispositivos pequeños. Abordaremos en profundidad tres de estos tipos: Sistemas de Tiempo Real, Sistemas Multimedia y Sistemas Móviles (incluyendo sistemas de bolsillo y embebidos).
1. Sistemas de Tiempo Real
Los sistemas de tiempo real son aquellos que deben producir respuestas correctas en un tiempo determinado. La falla en la entrega de una respuesta dentro de un plazo específico es a menudo tan mala como entregar una respuesta incorrecta.
Definición y Características:


Un proceso o tarea de tiempo real ejecuta en conexión con procesos o funciones o un conjunto de eventos externos al sistema, y debe cumplir uno o más plazos para interactuar efectiva y correctamente con el entorno externo.
Los requisitos de tiempo de respuesta son críticos, ya que estos sistemas deben cumplir plazos impuestos por individuos, dispositivos o flujos de datos externos.
Determinismo: La operación se ejecuta en un plazo predecible o un intervalo de tiempo. Un sistema determinista puede interrumpir un proceso no crítico para atender uno crítico.
Reactividad: La rapidez con la que el sistema responde a los eventos externos. El determinismo y la reactividad juntos conforman el tiempo de respuesta a eventos externos.
Control del usuario: Generalmente, un SO de tiempo real permite un control de grano fino sobre la prioridad de la tarea y la asignación de recursos (ej., paginación, residencia en memoria principal, algoritmos de transferencia a disco).
Fiabilidad: La capacidad de operar correctamente ante fallos.
Operación de fallo suave (Soft Failure Operation): La habilidad del sistema para fallar de tal manera que se preserve la mayor cantidad posible de capacidad y datos.
Requieren un cambio rápido de proceso o hilo, un tamaño pequeño (funcionalidad mínima), capacidad de respuesta rápida a interrupciones externas, multitarea con herramientas de comunicación entre procesos (semáforos, señales, eventos), y minimización de intervalos en los que se deshabilitan las interrupciones.
Tipos:


Sistemas de Tiempo Real Duro (Hard Real-time Systems): Deben cumplir estrictamente todos los plazos. El fallo en el cumplimiento de un plazo puede tener consecuencias catastróficas. Ejemplos: control de vuelos en aeronaves, robots industriales.
Sistemas de Tiempo Real Suave (Soft Real-time Systems): Es aceptable que muy ocasionalmente se pueda fallar un plazo predeterminado sin causar fallos críticos del sistema, pero con una degradación de la calidad o rendimiento. Ejemplos: sistemas de audio digital, multimedia, teléfonos digitales.
Planificación (Scheduling) de Tiempo Real:


Los algoritmos pueden ser estáticos (decisiones de planificación antes de la ejecución) o dinámicos (durante la ejecución). La planificación estática requiere información perfecta de antemano.
Planificación basada en plazos (Deadline Scheduling): Prioriza las tareas basándose en el momento en que deben comenzar o finalizar.
Tiempo de activación: Momento en que la tarea está lista.
Plazo de comienzo: Momento en que la tarea debe empezar.
Plazo de conclusión: Momento en que la tarea debe estar completa.
Tiempo de proceso: Tiempo necesario para ejecutar la tarea.
Planificación por el plazo más cercano primero (Earliest Deadline First - EDF): Es un algoritmo dinámico que selecciona el proceso con el plazo más cercano. Siempre funciona para cualquier conjunto programable de procesos y puede lograr el 100% de uso de la CPU.
Planificación de tasa monótona (Rate Monotonic Scheduling - RMS): Es un algoritmo preferente estático que asigna prioridades fijas a los procesos con base en sus periodos.
Otros enfoques dinámicos incluyen aquellos basados en un plan (donde la factibilidad se determina en tiempo de ejecución antes de la aceptación de una tarea) y de mejor esfuerzo (donde no hay análisis de factibilidad y el sistema intenta cumplir los plazos, abortando si falla).
La planificación en sistemas de tiempo real va más allá de la equidad o prioridad, especificando límites temporales para el comienzo o finalización.
2. Sistemas Multimedia
Los sistemas multimedia están diseñados para manejar y reproducir contenido como películas digitales, clips de video y música. Sus características son muy distintas de los archivos de texto tradicionales y, por lo tanto, imponen nuevas demandas a los sistemas operativos.
Características y Requisitos:


Altas velocidades de datos: Provienen de la naturaleza de la información visual y acústica. El ojo y el oído procesan cantidades prodigiosas de información por segundo, y necesitan alimentarse a esa velocidad.
Reproducción en tiempo real: Los datos deben entregarse en intervalos precisos (ej., 33.3 ms para NTSC, 40 ms para PAL/SECAM), o la experiencia visual se verá entrecortada.
Sincronización: Las múltiples pistas (video, audio, subtítulos) deben permanecer sincronizadas durante la reproducción para evitar desfases.
Los archivos multimedia son muy grandes, a menudo se escriben una sola vez pero se leen muchas veces y tienden a usarse secuencialmente.
Impacto en el Diseño del Sistema Operativo:


Planificación de Procesos: Requieren planificación de tiempo real expulsiva. Los algoritmos EDF y RMS son comúnmente utilizados. La planificación de procesos en multimedia es un área activa de investigación.
Sistemas de Archivos Multimedia:
Necesitan un paradigma distinto al de los sistemas de archivos tradicionales.
Suelen usar un modelo de empuje (push model): una vez que comienza el flujo, los bits llegan del disco sin más peticiones del usuario.
Deben manejar archivos que constan de varias pistas paralelas (video, audio, subtítulos) y mantenerlas sincronizadas.
La organización puede ser compleja, requiriendo estructuras de datos para listar "subarchivos" por cada archivo multimedia o un "nodo-i de dos dimensiones".
La colocación de archivos en disco es crucial para el rendimiento. Pueden almacenarse de forma contigua (simple y de alto rendimiento para acceso secuencial, pero con fragmentación externa) o no contigua (más flexible, pero más lenta).
Programación de Discos para Multimedia: Las aplicaciones multimedia imponen demandas de alta velocidad de datos y entrega en tiempo real. A diferencia de las aplicaciones tradicionales, las peticiones de disco multimedia son predecibles. Esto permite la planificación de discos estática, donde el sistema puede anticipar las necesidades de datos de cada flujo activo.
Compresión/Descompresión: Necesaria debido al gran tamaño de los datos. Hay algoritmos de codificación (compresión) y decodificación (descompresión). La codificación puede ser lenta y costosa (se hace una vez), mientras que la decodificación debe ser rápida y económica (se hace muchas veces). Para conferencias de video, la codificación también debe ser en tiempo real.
3. Sistemas Móviles (Handheld Systems)
Los sistemas de bolsillo (handheld systems) y los teléfonos celulares se han fusionado esencialmente en los llamados teléfonos inteligentes (smartphones), los cuales ejecutan sistemas operativos completos y utilizan características de las computadoras de escritorio.
Características y Desafíos:


Tamaño y Peso Limitados: Son dispositivos pequeños (ej., 5x3 pulgadas, menos de media libra).
Recursos Limitados: Típicamente tienen poca memoria, procesadores lentos y pantallas pequeñas. Aunque esto está cambiando rápidamente.
Bajo Consumo de Energía: La administración de energía es crucial debido a las baterías limitadas.
Enfoque en Comunicación: Están diseñados con un fuerte énfasis en las capacidades de comunicación.
No tienen discos duros grandes: Aunque esto está cambiando, históricamente carecían de grandes capacidades de almacenamiento en disco. Sin embargo, usan memoria flash como "disco" y esto tiene propiedades distintas a un disco magnético.
Software de terceros: Muchos ejecutan aplicaciones desarrolladas por terceros.
Symbian OS como Caso de Estudio:


Symbian OS es un sistema operativo diseñado específicamente para plataformas de teléfonos inteligentes. Heredó su orientación a objetos y diseño de microkernel de EPOC, un sistema operativo anterior de Psion para PDAs.
Arquitectura Orientada a Objetos: La orientación a objetos está diseñada en todo el marco de trabajo del sistema operativo, asociando la funcionalidad y las llamadas al sistema con objetos del sistema, y concentrando la asignación y protección de recursos en la asignación de objetos.
Arquitectura Microkernel: Utiliza un diseño de nanokernel (un núcleo muy pequeño que ejecuta las funciones más rápidas y primitivas del kernel) y un nivel de kernel de Symbian OS para funciones más complejas (gestión de objetos complejos, hilos en modo usuario, planificación, etc.). Esto facilita la modularidad, flexibilidad y el soporte para entornos distribuidos.
Arquitectura Cliente/Servidor: Coordina el acceso a los recursos del sistema con servidores en espacio de usuario.
Multitarea y Multihilo: Symbian OS es un sistema operativo multitarea que utiliza procesos e hilos.
Nanohilos: Hilos ultraligeros con un contexto mínimo, controlados directamente por el nanokernel.
Hilos de Symbian OS: Se basan en los nanohilos, añadiendo soporte para pilas en tiempo de ejecución, manejo de excepciones y señalización.
Objetos Activos: Formas especializadas de hilos, implementadas para aligerar la carga en el entorno operativo, especialmente en aplicaciones orientadas a la comunicación. Cuando un objeto activo espera un evento, el SO no necesita comprobarlo continuamente, lo que aumenta el rendimiento.
Administración de Memoria: A diferencia de los sistemas de escritorio, Symbian OS no utiliza memoria virtual con paginación de RAM a disco magnético debido a las limitaciones de los teléfonos inteligentes. La paginación en estos dispositivos se realiza a memoria flash, que tiene propiedades distintas a los discos magnéticos. La administración de memoria se enfoca en gestionar el tamaño de la aplicación y puede usar un modelo directo (sin MMU, aunque esto es raro) o un modelo de emulador.
Subsistema de E/S y Comunicaciones:
Tiene una infraestructura de comunicación modular y conectable.
Utiliza un modelo de drivers de dispositivo de dos capas: LDD (Logical Device Driver) para la interfaz de programa y PDD (Physical Device Driver) para la interfaz con el dispositivo físico. Ambos se pueden cargar dinámicamente.
Los controladores de medios son PDDs especiales para dispositivos de almacenamiento (fijos y removibles), creando un objeto de socket cuando se inserta una tarjeta.
La comunicación se basa en protocolos organizados en capas (driver de dispositivo, implementación de protocolos, aplicación), incluyendo TCP/IP y Bluetooth.
Los sockets se implementan como archivos especiales y proporcionan una interfaz para la comunicación de red.
Seguridad y Protección: La seguridad en teléfonos inteligentes es un desafío. Symbian OS ha tomado decisiones de diseño que lo diferencian de los sistemas de escritorio y otras plataformas móviles.
Sistemas Embebidos (Embedded Systems):


Controlan dispositivos que generalmente no se consideran computadoras (hornos microondas, televisores, autos).
La propiedad principal es que el software que se ejecuta es el que los diseñadores colocan en ROM, lo que significa que nunca se ejecutará software no confiable. Esto simplifica la protección.
Algunos sistemas operativos embebidos son solo bibliotecas enlazadas con el programa de aplicación (ej., e-Cos), proporcionando llamadas al sistema para E/S y otras tareas comunes.
Sistemas de Tarjetas Inteligentes:


Son los sistemas operativos más pequeños, operando en dispositivos del tamaño de una tarjeta de crédito con un chip de CPU.
Tienen restricciones severas de poder de procesamiento y memoria.
A menudo son sistemas propietarios y pueden ser susceptibles a ataques de canal lateral (ej., análisis de energía).
La categoría de sistemas de bolsillo, embebidos y de tiempo real se superponen considerablemente, con casi todos ellos teniendo aspectos de tiempo real suave.

Ambientes de Cómputo
Los ambientes de cómputo representan las distintas formas en que los recursos de hardware y software de las computadoras se organizan e interconectan para satisfacer las necesidades de los usuarios y las aplicaciones. A lo largo de la historia de los sistemas operativos, estos ambientes han evolucionado desde configuraciones centralizadas y secuenciales hasta modelos distribuidos y altamente interconectados.
A continuación, desarrollaremos en profundidad los conceptos solicitados:
1. Computación Tradicional
La computación tradicional se refiere a los modelos de sistemas informáticos que prevalecieron en las primeras generaciones y que sentaron las bases para los sistemas modernos, aunque con características muy diferentes a las actuales.
Procesamiento Serie: En los inicios, los usuarios accedían a la computadora de forma secuencial, uno por uno, en lo que se denominaba procesamiento serie. La ejecución de programas se realizaba de manera lineal, una instrucción a la vez.
Sistemas en Lotes Sencillos (Batch Systems): Para maximizar la utilización de las primeras máquinas, que eran muy costosas, se desarrolló el concepto de sistemas operativos en lotes. Estos sistemas permitían que el procesador alternara entre la ejecución de programas de usuario y la ejecución de un "monitor" (parte residente del SO), mejorando la utilización del computador a pesar de la sobrecarga. Un sistema de procesamiento por lotes procesa trabajos de rutina sin la presencia de un usuario interactivo.
Multiprogramación: La multiprogramación fue un avance clave que permitió tener varios trabajos residentes en la memoria principal al mismo tiempo. Si un programa necesitaba esperar una operación de E/S, el procesador podía asignarse a otro programa listo para ejecutarse. Esto generaba una apariencia de paralelismo o "pseudoparalelismo" en un único procesador, aumentando la eficiencia.
Tiempo Compartido (Time-Sharing): Como variante de la multiprogramación, el tiempo compartido surgió para proporcionar un modo interactivo donde varios usuarios remotos podían ejecutar trabajos simultáneamente en una computadora grande y costosa, cada uno con su terminal en línea. El sistema operativo asignaba "rodajas de tiempo" a los procesos. CTSS (Compatible Time Sharing System) fue el primer sistema de tiempo compartido de propósito general.
Mainframes: Computadoras grandes y potentes, del tamaño de una habitación, que aún se encuentran en centros de datos. Están orientadas a procesar muchos trabajos a la vez y suelen ofrecer servicios de procesamiento por lotes, procesamiento de transacciones y tiempo compartido. Un ejemplo es el OS/360, que fue la primera línea importante en usar circuitos integrados.
Computadoras Personales (PC): Con el desarrollo de circuitos de gran escala de integración (LSI) en la década de 1980, surgieron las computadoras personales. Inicialmente, sistemas como CP/M y MS-DOS se basaban en comandos de teclado, pero la Interfaz Gráfica de Usuario (GUI) con ventanas, iconos y ratón, inventada por Doug Engelbart y adoptada por Xerox PARC, revolucionó la interacción. Al principio monoprogramadas, las PCs modernas soportan la multiprogramación.
2. Computación Cliente-Servidor
La computación cliente-servidor es un modelo especializado de sistema distribuido que ha ganado prominencia con la evolución de las PCs y las redes, alejándose de la arquitectura centralizada tradicional.
Definición: Un entorno cliente-servidor está formado por clientes (generalmente PCs o estaciones de trabajo con interfaces de usuario amigables, incluyendo GUI) y servidores (máquinas que proporcionan un conjunto de servicios compartidos). La comunicación entre ellos se realiza a través de una red (LAN, WAN o Internet).
Ventajas:
Simplificación del sistema ejecutivo: Permite construir diversas API sin conflictos.
Mejora de la fiabilidad: Cada módulo de servicio se ejecuta como un proceso separado y protegido. Un único servidor puede fallar sin comprometer el resto del sistema operativo.
Base para la computación distribuida: Los clientes no necesitan saber si una petición es atendida local o remotamente, y esta gestión puede cambiar dinámicamente según las condiciones.
Facilidad de uso y control local: Ofrece aplicaciones de fácil manejo en los sistemas de usuario, dando a estos un gran control sobre su computadora.
Centralización de recursos compartidos: Permite centralizar bases de datos corporativas y funciones de gestión de red en los servidores, facilitando la interoperabilidad.
Arquitectura y Distribución de la Lógica de Aplicación: La tarea fundamental es distribuir la lógica de la aplicación entre el cliente y el servidor, buscando optimizar el uso de recursos.
Procesamiento basado en el servidor (Cliente Ligero - Thin Client): El cliente se encarga principalmente de la interfaz gráfica de usuario, mientras que casi todo el procesamiento se realiza en el servidor. Este enfoque es a menudo una ruta de migración de aplicaciones mainframe a entornos distribuidos.
Procesamiento basado en el cliente (Cliente Pesado - Fat Client): La mayor parte del procesamiento se realiza en el cliente, con el servidor manejando principalmente la lógica de la base de datos y la validación de datos. Este modelo aprovecha la potencia de los escritorios, descargando a los servidores.
Procesamiento cooperativo: La lógica de la aplicación se distribuye óptimamente entre el cliente y el servidor para beneficiarse de ambos, aunque es más complejo de configurar.
Arquitectura de Tres Capas: Común en muchos sistemas, donde el software de aplicación se distribuye entre:
Una máquina de usuario (típicamente un cliente ligero).
Un servidor en la capa central (actúa como pasarela entre los clientes ligeros y varios servidores backend, y puede convertir protocolos o integrar resultados).
Un servidor backend (servidores de bases de datos).
Middleware: Es una capa de software que se sitúa entre las aplicaciones y los sistemas operativos/software de comunicaciones, proporcionando un conjunto de herramientas y APIs estándar. Su propósito es permitir a las aplicaciones y usuarios acceder a una variedad de servicios en el servidor de manera uniforme, sin preocuparse por las diferencias entre los servidores o su ubicación, facilitando la interoperabilidad y el desarrollo de aplicaciones distribuidas. Se basa típicamente en paso de mensajes y llamadas a procedimiento remoto (RPC).
Paso de mensajes: Los procesos se comunican enviando mensajes entre sí. Las llamadas pueden ser bloqueantes (síncronas) o no bloqueantes (asíncronas). Es la base de sistemas como MPI.
Llamadas a procedimiento remoto (RPC): Permite a un programa cliente invocar un procedimiento en un servidor remoto como si fuera una llamada local, ocultando los detalles de la comunicación de red.
Orientación a objetos: La tecnología orientada a objetos se ha adoptado en el diseño cliente-servidor, donde clientes y servidores se comunican mediante mensajes entre objetos. CORBA (Common Object Request Broker Architecture) es un sistema cliente-servidor basado en objetos en tiempo de ejecución, diseñado para sistemas heterogéneos.
3. Computación Peer-to-Peer (P2P)
Las fuentes proporcionadas no desarrollan explícitamente el concepto de "computación peer-to-peer" como una sección dedicada. Sin embargo, abordan conceptos relacionados con la computación distribuida que comparten algunas de las ideas fundamentales de P2P, como la interconexión de sistemas autónomos y la colaboración.
Sistemas Distribuidos: Son colecciones de computadoras físicamente separadas, interconectadas por una red, donde cada una tiene su propia memoria y se comunica mediante mensajes. Estos sistemas tienen un acoplamiento más débil que las multicomputadoras.
Multicomputadoras: Son CPUs con "acoplamiento fuerte que no comparten memoria. Cada una tiene su propia memoria". También se conocen como "clústeres de computadoras" o "COWS (Clusters of Workstations)". Se construyen con PCs y una red de alta velocidad.
Clusters: Un grupo de computadoras completas e interconectadas que trabajan juntas como un recurso de computación unificado y pueden crear la ilusión de ser una única máquina. Cada computadora ("nodo") puede operar de forma independiente. Los clusters ofrecen escalabilidad absoluta e incremental, así como alta disponibilidad. El middleware del cluster proporciona una "imagen única del sistema" (SSI) al usuario.
Clusters Beowulf y Linux: Son implementaciones comunes de clusters que utilizan estaciones de trabajo Linux sobre PCs interconectadas por una red Ethernet, utilizando herramientas de computación gratuitas.
Aunque estos conceptos describen la interconexión y cooperación de múltiples computadoras, no detallan las características específicas de la arquitectura P2P, como la ausencia de servidores centralizados con todos los nodos actuando como clientes y servidores simultáneamente, o la auto-organización descentralizada, lo cual es distintivo de una verdadera red P2P.
4. Computación Basada en la Web
La computación basada en la Web es una aplicación muy extendida del modelo cliente-servidor y representa un cambio hacia una computación "céntrica de Web".
Modelo Fundamental: La Web se basa en el paradigma de que cada computadora puede contener uno o más documentos (páginas Web), que incluyen texto, imágenes, sonidos, películas e hipervínculos a otras páginas. Un programa llamado navegador Web solicita una página y la muestra en pantalla.
Evolución del Modelo Cliente-Servidor: La Web es un ejemplo de cómo los clientes (PCs domésticas) y los servidores (equipos más grandes que operan remotamente) interactúan. Una PC envía una petición de una página Web al servidor, y el servidor devuelve la página.
"Utilidad para Computadora" Revisitada: La idea de una "utilería para computadora" (una máquina masiva que proporciona poder de cómputo a muchos usuarios) que se disipó con MULTICS, ha regresado en forma de servidores masivos de Internet centralizados. Los usuarios se conectan con "máquinas de usuario relativamente tontas" (clientes delgados) donde la mayor parte del trabajo se realiza en los servidores.
Aplicaciones Centradas en la Web: La gente ha pasado de aplicaciones locales a servicios en línea (ej., Gmail, Hotmail). La expectativa es que, en el futuro, las PCs solo ejecuten un navegador Web, con el procesamiento de palabras y hojas de cálculo realizándose en sitios Web.
Middleware Basado en Documentos: La World Wide Web es un middleware basado en documentos, que unifica el sistema distribuido bajo la apariencia de una colección gigante de documentos hipervinculados.
Paradigmas de Acceso: El Protocolo Web (HTTP) es un ejemplo de protocolo orientado a no conexión a nivel de aplicación, lo que simplifica la interacción al no requerir una configuración previa explícita para cada solicitud.
Abstracción y Coherencia Arquitectónica: La Web proporciona un paradigma unificador donde el ciberespacio se percibe como lleno de documentos con URLs. Aunque muchos "documentos" se generan dinámicamente por programas en el servidor, el usuario interactúa con ellos como si fueran documentos estáticos.
En conjunto, estos ambientes de cómputo reflejan la continua adaptación y evolución de los sistemas operativos para satisfacer las cambiantes demandas tecnológicas y de los usuarios.

